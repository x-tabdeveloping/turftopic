{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Getting Started","text":"<p>Turftopic is a topic modeling library which intends to simplify and streamline the usage of contextually sensitive topic models. We provide stable, minimal and scalable implementations of several types of models along with extensive documentation.</p> Build and Train Topic Models Explore, Interpret and Visualize your Models Modify and Fine-tune Topic Models Choose the Right Model for your Use-Case Explore Topics Changing over Time Use Phrases or Lemmas for Topic Models Extract Topics from a Stream of Documents Find Hierarchical Order in Topics Name Topics with Large Language Models"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Turftopic can be installed from PyPI.</p> <pre><code>pip install turftopic\n</code></pre> <p>Turftopic's models follow the scikit-learn API conventions, and as such they are quite easy to use if you are familiar with scikit-learn workflows.</p> <p>Here's an example of how you use KeyNMF, one of our models on the 20Newsgroups dataset from scikit-learn.</p> <pre><code>from turftopic import KeyNMF\nfrom sklearn.datasets import fetch_20newsgroups\n\nnewsgroups = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n)\ncorpus = newsgroups.data\nmodel = KeyNMF(20).fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Top 10 Words 0 armenians, armenian, armenia, turks, turkish, genocide, azerbaijan, soviet, turkey, azerbaijani 1 sale, price, shipping, offer, sell, prices, interested, 00, games, selling ...."},{"location":"FASTopic/","title":"FASTopic","text":"<p>FASTopic is a neural topic model based on Dual Semantic-relation Reconstruction.</p> <p>Turftopic contains an implementation repurposed for our API, but the implementation is mostly from the original FASTopic package.</p> <p> This part of the documentation is still under construction </p>"},{"location":"FASTopic/#references","title":"References","text":"<p>Wu, X., Nguyen, T., Zhang, D. C., Wang, W. Y., &amp; Luu, A. T. (2024). FASTopic: A Fast, Adaptive, Stable, and Transferable Topic Modeling Paradigm. ArXiv Preprint ArXiv:2405.17978.</p>"},{"location":"FASTopic/#api-reference","title":"API Reference","text":""},{"location":"FASTopic/#turftopic.models.fastopic.FASTopic","title":"<code>turftopic.models.fastopic.FASTopic</code>","text":"<p>             Bases: <code>ContextualModel</code></p> <p>Implementation of the FASTopic model with a Turftopic API. The implementation is based on the original FASTopic package, but is adapted for optimal use in Turftopic (you can pre-compute embeddings for instance).</p> <p>You will need to install torch to use this model.</p> <pre><code>pip install turftopic[torch]\n## OR:\npip install turftopic[pyro-ppl]\n</code></pre> <pre><code>from turftopic import FASTopic\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\nmodel = FASTopic(10).fit(corpus)\nmodel.print_topics()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of topics. If you're using priors on the weight, feel free to overshoot with this value.</p> required <code>encoder</code> <code>Union[Encoder, str]</code> <p>Model to encode documents/terms, all-MiniLM-L6-v2 is the default.</p> <code>'sentence-transformers/all-MiniLM-L6-v2'</code> <code>vectorizer</code> <code>Optional[CountVectorizer]</code> <p>Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.</p> <code>None</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state to use so that results are exactly reproducible.</p> <code>None</code> <code>DT_alpha</code> <code>float</code> <p>Sinkhorn alpha between document embeddings and topic embeddings.</p> <code>3.0</code> <code>TW_alpha</code> <code>float</code> <p>Sinkhorn alpha between topic embeddings and word embeddings.</p> <code>2.0</code> <code>theta_temp</code> <code>float</code> <p>Temperature parameter of used in softmax to compute topic probabilities in documents.</p> <code>1.0</code> <code>n_epochs</code> <code>int</code> <p>Number of epochs to train the model for.</p> <code>200</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the ADAM optimizer.</p> <code>0.002</code> <code>device</code> <code>str</code> <p>Device to run the model on. Defaults to CPU.</p> <code>'cpu'</code> Source code in <code>turftopic/models/fastopic.py</code> <pre><code>class FASTopic(ContextualModel):\n    \"\"\"\n    Implementation of the FASTopic model with a Turftopic API.\n    The implementation is based on the [original FASTopic package](https://github.com/BobXWu/FASTopic/tree/master),\n    but is adapted for optimal use in Turftopic (you can pre-compute embeddings for instance).\n\n    You will need to install torch to use this model.\n\n    ```bash\n    pip install turftopic[torch]\n    ## OR:\n    pip install turftopic[pyro-ppl]\n    ```\n\n    ```python\n    from turftopic import FASTopic\n\n    corpus: list[str] = [\"some text\", \"more text\", ...]\n\n    model = FASTopic(10).fit(corpus)\n    model.print_topics()\n    ```\n\n    Parameters\n    ----------\n    n_components: int\n        Number of topics. If you're using priors on the weight,\n        feel free to overshoot with this value.\n    encoder: str or SentenceTransformer\n        Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n    vectorizer: CountVectorizer, default None\n        Vectorizer used for term extraction.\n        Can be used to prune or filter the vocabulary.\n    random_state: int, default None\n        Random state to use so that results are exactly reproducible.\n    DT_alpha: float, default 3.0\n        Sinkhorn alpha between document embeddings and topic embeddings.\n    TW_alpha: float, default 2.0\n        Sinkhorn alpha between topic embeddings and word embeddings.\n    theta_temp: float, default 1.0\n        Temperature parameter of used in softmax to compute topic probabilities in documents.\n    n_epochs: int, default 200\n        Number of epochs to train the model for.\n    learning_rate: float, default 0.002\n        Learning rate for the ADAM optimizer.\n    device: str, default \"cpu\"\n        Device to run the model on. Defaults to CPU.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: int,\n        encoder: Union[\n            Encoder, str\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        random_state: Optional[int] = None,\n        batch_size: Optional[int] = None,\n        DT_alpha: float = 3.0,\n        TW_alpha: float = 2.0,\n        theta_temp: float = 1.0,\n        n_epochs: int = 200,\n        learning_rate: float = 0.002,\n        device: str = \"cpu\",\n    ):\n        self.n_components = n_components\n        self.encoder = encoder\n        self.random_state = random_state\n        if isinstance(encoder, str):\n            self.encoder_ = SentenceTransformer(encoder)\n        else:\n            self.encoder_ = encoder\n        if vectorizer is None:\n            self.vectorizer = default_vectorizer()\n        else:\n            self.vectorizer = vectorizer\n        self.DT_alpha = DT_alpha\n        self.TW_alpha = TW_alpha\n        self.theta_temp = theta_temp\n        self.n_epochs = n_epochs\n        self.learning_rate = learning_rate\n        self.device = device\n        self.model = fastopic(n_components, theta_temp, DT_alpha, TW_alpha)\n        self.batch_size = batch_size\n\n    def make_optimizer(self, learning_rate: float):\n        args_dict = {\n            \"params\": self.model.parameters(),\n            \"lr\": learning_rate,\n        }\n        optimizer = torch.optim.Adam(**args_dict)\n        return optimizer\n\n    def _train_model(self, embeddings, document_term_matrix, status):\n        self.model.init(document_term_matrix.shape[1], embeddings.shape[1])\n        self.model = self.model.to(self.device)\n        optimizer = self.make_optimizer(self.learning_rate)\n        self.model.train()\n        if self.batch_size is None:\n            batch_size = embeddings.shape[0]\n        else:\n            batch_size = self.batch_size\n        num_batches = int(\n            math.ceil(document_term_matrix.shape[0] / batch_size)\n        )\n        for epoch in range(self.n_epochs):\n            running_loss = 0\n            for i in range(num_batches):\n                batch_bow = np.atleast_2d(\n                    document_term_matrix[\n                        i * batch_size : (i + 1) * batch_size, :\n                    ].toarray()\n                )\n                # Skipping batches that are smaller than 2\n                if batch_bow.shape[0] &lt; 2:\n                    continue\n                batch_contextualized = np.atleast_2d(\n                    embeddings[i * batch_size : (i + 1) * batch_size, :]\n                )\n                batch_contextualized = (\n                    torch.tensor(batch_contextualized).float().to(self.device)\n                )\n                batch_bow = torch.tensor(batch_bow).float().to(self.device)\n                rst_dict = self.model(batch_bow, batch_contextualized)\n                batch_loss = rst_dict[\"loss\"]\n                running_loss += batch_loss\n                optimizer.zero_grad()\n                batch_loss.backward()\n                optimizer.step()\n            status.update(\n                f\"Fitting model. Epoch [{epoch}/{self.n_epochs}], Loss [{running_loss}]\"\n            )\n        self.components_ = self.model.get_beta().detach().numpy()\n\n    def fit_transform(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        console = Console()\n        with console.status(\"Fitting model\") as status:\n            if embeddings is None:\n                status.update(\"Encoding documents\")\n                embeddings = self.encoder_.encode(raw_documents)\n                console.log(\"Documents encoded.\")\n            self.train_doc_embeddings = embeddings\n            status.update(\"Extracting terms.\")\n            document_term_matrix = self.vectorizer.fit_transform(raw_documents)\n            console.log(\"Term extraction done.\")\n            status.update(\"Fitting model\")\n            self._train_model(embeddings, document_term_matrix, status)\n            console.log(\"Model fitting done.\")\n            document_topic_matrix = self.transform(\n                raw_documents, embeddings=embeddings\n            )\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=document_topic_matrix\n            )\n        return document_topic_matrix\n\n    def transform(\n        self, raw_documents, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Infers topic importances for new documents based on a fitted model.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n\n        Returns\n        -------\n        ndarray of shape (n_dimensions, n_topics)\n            Document-topic matrix.\n        \"\"\"\n        if embeddings is None:\n            embeddings = self.encoder_.encode(raw_documents)\n        with torch.no_grad():\n            self.model.eval()\n            theta = self.model.get_theta(\n                torch.as_tensor(embeddings),\n                torch.as_tensor(self.train_doc_embeddings),\n            )\n            theta = theta.detach().cpu().numpy()\n        return theta\n</code></pre>"},{"location":"FASTopic/#turftopic.models.fastopic.FASTopic.transform","title":"<code>transform(raw_documents, embeddings=None)</code>","text":"<p>Infers topic importances for new documents based on a fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> required <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_dimensions, n_topics)</code> <p>Document-topic matrix.</p> Source code in <code>turftopic/models/fastopic.py</code> <pre><code>def transform(\n    self, raw_documents, embeddings: Optional[np.ndarray] = None\n) -&gt; np.ndarray:\n    \"\"\"Infers topic importances for new documents based on a fitted model.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n\n    Returns\n    -------\n    ndarray of shape (n_dimensions, n_topics)\n        Document-topic matrix.\n    \"\"\"\n    if embeddings is None:\n        embeddings = self.encoder_.encode(raw_documents)\n    with torch.no_grad():\n        self.model.eval()\n        theta = self.model.get_theta(\n            torch.as_tensor(embeddings),\n            torch.as_tensor(self.train_doc_embeddings),\n        )\n        theta = theta.detach().cpu().numpy()\n    return theta\n</code></pre>"},{"location":"GMM/","title":"GMM (Gaussian Mixture Model)","text":"<p>GMM is a generative probabilistic model over the contextual embeddings. The model assumes that contextual embeddings are generated from a mixture of underlying Gaussian components. These Gaussian components are assumed to be the topics.</p> Components of a Gaussian Mixture Model (figure from scikit-learn documentation)"},{"location":"GMM/#how-does-gmm-work","title":"How does GMM work?","text":""},{"location":"GMM/#generative-modeling","title":"Generative Modeling","text":"<p>GMM assumes that the embeddings are generated according to the following stochastic process from a number of Gaussian components. Priors are optionally imposed on the model parameters. The model is fitted either using expectation maximization or variational inference.</p> Click to see formula <ol> <li>Select global topic weights: \\(\\Theta\\)</li> <li>For each component select mean \\(\\mu_z\\) and covariance matrix \\(\\Sigma_z\\) .</li> <li>For each document:<ul> <li>Draw topic label: \\(z \\sim Categorical(\\Theta)\\)</li> <li>Draw document vector: \\(\\rho \\sim \\mathcal{N}(\\mu_z, \\Sigma_z)\\)</li> </ul> </li> </ol>"},{"location":"GMM/#calculate-topic-probabilities","title":"Calculate Topic Probabilities","text":"<p>After the model is fitted, soft topic labels are inferred for each document. A document-topic-matrix (\\(T\\)) is built from the likelihoods of each component given the document encodings.</p> Click to see formula <ul> <li>For document \\(i\\) and topic \\(z\\) the matrix entry will be: \\(T_{iz} = p(\\rho_i|\\mu_z, \\Sigma_z)\\)</li> </ul>"},{"location":"GMM/#soft-c-tf-idf","title":"Soft c-TF-IDF","text":"<p>Term importances for the discovered Gaussian components are estimated post-hoc using a technique called Soft c-TF-IDF, an extension of c-TF-IDF, that can be used with continuous labels.</p> Click to see formula <p>Let \\(X\\) be the document term matrix where each element (\\(X_{ij}\\)) corresponds with the number of times word \\(j\\) occurs in a document \\(i\\). Soft Class-based tf-idf scores for terms in a topic are then calculated in the following manner:</p> <ul> <li>Estimate weight of term \\(j\\) for topic \\(z\\):  \\(tf_{zj} = \\frac{t_{zj}}{w_z}\\), where  \\(t_{zj} = \\sum_i T_{iz} \\cdot X_{ij}\\) and  \\(w_{z}= \\sum_i(|T_{iz}| \\cdot \\sum_j X_{ij})\\) </li> <li>Estimate inverse document/topic frequency for term \\(j\\): \\(idf_j = log(\\frac{N}{\\sum_z |t_{zj}|})\\), where \\(N\\) is the total number of documents.</li> <li>Calculate importance of term \\(j\\) for topic \\(z\\):  \\(Soft-c-TF-IDF{zj} = tf_{zj} \\cdot idf_j\\)</li> </ul>"},{"location":"GMM/#dynamic-modeling","title":"Dynamic Modeling","text":"<p>GMM is also capable of dynamic topic modeling. This happens by fitting one underlying mixture model over the entire corpus, as we expect that there is only one semantic model generating the documents. To gain temporal representations for topics, the corpus is divided into equal, or arbitrarily chosen time slices, and then term importances are estimated using Soft-c-TF-IDF for each of the time slices separately.</p>"},{"location":"GMM/#similarities-with-clustering-models","title":"Similarities with Clustering Models","text":"<p>Gaussian Mixtures can in some sense be considered a fuzzy clustering model.</p> <p>Since we assume the existence of a ground truth label for each document, the model technically cannot capture multiple topics in a document, only uncertainty around the topic label.</p> <p>This makes GMM better at accounting for documents which are the intersection of two or more semantically close topics.</p> <p>Another important distinction is that clustering topic models are typically transductive, while GMM is inductive. This means that in the case of GMM we are inferring some underlying semantic structure, from which the different documents are generated, instead of just describing the corpus at hand. In practical terms this means that GMM can, by default infer topic labels for documents, while (some) clustering models cannot.</p>"},{"location":"GMM/#performance-tips","title":"Performance Tips","text":"<p>GMM can be a bit tedious to run at scale. This is due to the fact, that the dimensionality of parameter space increases drastically with the number of mixture components, and with embedding dimensionality. To counteract this issue, you can use dimensionality reduction. We recommend that you use PCA, as it is a linear and interpretable method, and it can function efficiently at scale.</p> <p>Through experimentation on the 20Newsgroups dataset I found that with 20 mixture components and embeddings from the <code>all-MiniLM-L6-v2</code> embedding model  reducing the dimensionality of the embeddings to 20 with PCA resulted in no performance decrease, but ran multiple times faster.  Needless to say this difference increases with the number of topics, embedding and corpus size.</p> <pre><code>from turftopic import GMM\nfrom sklearn.decomposition import PCA\n\nmodel = GMM(20, dimensionality_reduction=PCA(20))\n\n# for very large corpora you can also use Incremental PCA with minibatches\n\nfrom sklearn.decomposition import IncrementalPCA\n\nmodel = GMM(20, dimensionality_reduction=IncrementalPCA(20))\n</code></pre>"},{"location":"GMM/#api-reference","title":"API Reference","text":""},{"location":"GMM/#turftopic.models.gmm.GMM","title":"<code>turftopic.models.gmm.GMM</code>","text":"<p>             Bases: <code>ContextualModel</code>, <code>DynamicTopicModel</code>, <code>MultimodalModel</code></p> <p>Multivariate Gaussian Mixture Model over document embeddings.     Models topics as mixture components.</p> <pre><code>```python\nfrom turftopic import GMM\n</code></pre> <p>corpus: list[str] = [\"some text\", \"more text\", ...]</p> <pre><code>model = GMM(10, weight_prior=\"dirichlet_process\").fit(corpus)\nmodel.print_topics()\n```\n</code></pre> <pre><code>Parameters\n</code></pre> <pre><code>n_components: int or \"auto\"\n    Number of topics.\n    If \"auto\", the Bayesian Information criterion\n    will be used to estimate this quantity.\n    *Note that \"auto\" can only be used when no priors as specified*.\nencoder: str or SentenceTransformer\n    Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\nvectorizer: CountVectorizer, default None\n    Vectorizer used for term extraction.\n    Can be used to prune or filter the vocabulary.\nweight_prior: 'dirichlet', 'dirichlet_process' or None, default 'dirichlet'\n    Prior to impose on component weights, if None,\n    maximum likelihood is optimized with expectation maximization,\n    otherwise variational inference is used.\ngamma: float, default None\n    Concentration parameter of the symmetric prior.\n    By default 1/n_components is used.\n    Ignored when weight_prior is None.\ndimensionality_reduction: TransformerMixin, default None\n    Optional dimensionality reduction step before GMM is run.\n    This is recommended for very large datasets with high dimensionality,\n    as the number of parameters grows vast in the model otherwise.\n    We recommend using PCA, as it is a linear solution, and will likely\n    result in Gaussian components.\n    For even larger datasets you can use IncrementalPCA to reduce\n    memory load.\nfeature_importance: LexicalWordImportance, default 'soft-c-tf-idf'\n    Feature importance method to use.\n    *Note that only lexical methods can be used with GMM,\n    not embedding-based ones.*\nrandom_state: int, default None\n    Random state to use so that results are exactly reproducible.\n</code></pre> <pre><code>Attributes\n</code></pre> <pre><code>weights_: ndarray of shape (n_components)\n    Weights of the different mixture components.\n</code></pre> Source code in <code>turftopic/models/gmm.py</code> <pre><code>class GMM(ContextualModel, DynamicTopicModel, MultimodalModel):\n    \"\"\"Multivariate Gaussian Mixture Model over document embeddings.\n        Models topics as mixture components.\n\n        ```python\n        from turftopic import GMM\n    corpus: list[str] = [\"some text\", \"more text\", ...]\n\n        model = GMM(10, weight_prior=\"dirichlet_process\").fit(corpus)\n        model.print_topics()\n        ```\n\n        Parameters\n        ----------\n        n_components: int or \"auto\"\n            Number of topics.\n            If \"auto\", the Bayesian Information criterion\n            will be used to estimate this quantity.\n            *Note that \"auto\" can only be used when no priors as specified*.\n        encoder: str or SentenceTransformer\n            Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n        vectorizer: CountVectorizer, default None\n            Vectorizer used for term extraction.\n            Can be used to prune or filter the vocabulary.\n        weight_prior: 'dirichlet', 'dirichlet_process' or None, default 'dirichlet'\n            Prior to impose on component weights, if None,\n            maximum likelihood is optimized with expectation maximization,\n            otherwise variational inference is used.\n        gamma: float, default None\n            Concentration parameter of the symmetric prior.\n            By default 1/n_components is used.\n            Ignored when weight_prior is None.\n        dimensionality_reduction: TransformerMixin, default None\n            Optional dimensionality reduction step before GMM is run.\n            This is recommended for very large datasets with high dimensionality,\n            as the number of parameters grows vast in the model otherwise.\n            We recommend using PCA, as it is a linear solution, and will likely\n            result in Gaussian components.\n            For even larger datasets you can use IncrementalPCA to reduce\n            memory load.\n        feature_importance: LexicalWordImportance, default 'soft-c-tf-idf'\n            Feature importance method to use.\n            *Note that only lexical methods can be used with GMM,\n            not embedding-based ones.*\n        random_state: int, default None\n            Random state to use so that results are exactly reproducible.\n\n        Attributes\n        ----------\n        weights_: ndarray of shape (n_components)\n            Weights of the different mixture components.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: Union[int, Literal[\"auto\"]],\n        encoder: Union[\n            Encoder, str, MultimodalEncoder\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        dimensionality_reduction: Optional[TransformerMixin] = None,\n        feature_importance: LexicalWordImportance = \"soft-c-tf-idf\",\n        weight_prior: Literal[\"dirichlet\", \"dirichlet_process\", None] = None,\n        gamma: Optional[float] = None,\n        random_state: Optional[int] = None,\n    ):\n        self.n_components = n_components\n        self.encoder = encoder\n        self.weight_prior = weight_prior\n        self.gamma = gamma\n        self.random_state = random_state\n        if isinstance(encoder, str):\n            self.encoder_ = SentenceTransformer(encoder)\n        else:\n            self.encoder_ = encoder\n        self.validate_encoder()\n        if vectorizer is None:\n            self.vectorizer = default_vectorizer()\n        else:\n            self.vectorizer = vectorizer\n        if feature_importance not in FEATURE_IMPORTANCE_METHODS:\n            valid = list(FEATURE_IMPORTANCE_METHODS.keys())\n            raise ValueError(\n                f\"{feature_importance} not in list of valid feature importance methods: {valid}\"\n            )\n        self.feature_importance = feature_importance\n        self.dimensionality_reduction = dimensionality_reduction\n        if (self.n_components == \"auto\") and (self.weight_prior is not None):\n            raise ValueError(\n                \"You cannot use N='auto' with a prior. Try setting weight_prior=None.\"\n            )\n\n    def estimate_components(\n        self,\n        feature_importance: Optional[LexicalWordImportance] = None,\n        doc_topic_matrix=None,\n        doc_term_matrix=None,\n    ) -&gt; np.ndarray:\n        feature_importance = feature_importance or self.feature_importance\n        imp_fn = FEATURE_IMPORTANCE_METHODS[feature_importance]\n        doc_topic_matrix = (\n            doc_topic_matrix\n            if doc_topic_matrix is not None\n            else self.doc_topic_matrix\n        )\n        doc_term_matrix = (\n            doc_term_matrix\n            if doc_term_matrix is not None\n            else self.doc_term_matrix\n        )\n        self.components_ = imp_fn(doc_topic_matrix, doc_term_matrix)\n        return self.components_\n\n    def _create_bic(self, embeddings: np.ndarray):\n        def f_bic(n_components: int):\n            random_state = 42\n            success = False\n            n_tries = 1\n            while not success and (n_tries &lt;= 5):\n                try:\n                    # This can sometimes run into problems especially\n                    # with covariance estimation\n                    model = GaussianMixture(\n                        n_components, random_state=self.random_state\n                    )\n                    model.fit(embeddings)\n                    success = True\n                except Exception:\n                    random_state += 1\n                    n_tries += 1\n            if n_tries &gt; 5:\n                return 0\n            return model.bic(embeddings)\n\n        return f_bic\n\n    def _init_model(self, n_components: int):\n        if self.weight_prior is not None:\n            mixture = BayesianGaussianMixture(\n                n_components=n_components,\n                weight_concentration_prior_type=(\n                    \"dirichlet_distribution\"\n                    if self.weight_prior == \"dirichlet\"\n                    else \"dirichlet_process\"\n                ),\n                weight_concentration_prior=self.gamma,\n                random_state=self.random_state,\n            )\n        else:\n            mixture = GaussianMixture(\n                n_components, random_state=self.random_state\n            )\n        return mixture\n\n    def fit_transform(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        console = Console()\n        with console.status(\"Fitting model\") as status:\n            if embeddings is None:\n                status.update(\"Encoding documents\")\n                embeddings = self.encoder_.encode(raw_documents)\n                console.log(\"Documents encoded.\")\n            self.embeddings = embeddings\n            status.update(\"Extracting terms.\")\n            self.doc_term_matrix = self.vectorizer.fit_transform(raw_documents)\n            console.log(\"Term extraction done.\")\n            X = embeddings\n            if self.dimensionality_reduction is not None:\n                status.update(\"Reducing embedding dimensionality.\")\n                X = self.dimensionality_reduction.fit_transform(embeddings)\n                console.log(\"Dimensionality reduction complete.\")\n                self.reduced_embeddings = X\n            n_components = self.n_components\n            if self.n_components == \"auto\":\n                status.update(\"Finding optimal value of N\")\n                f_bic = self._create_bic(X)\n                n_components = optimize_n_components(f_bic, verbose=True)\n                console.log(f\"Found optimal N={n_components}.\")\n            status.update(\"Fitting mixture model.\")\n            self.gmm_ = self._init_model(n_components)\n            self.gmm_.fit(X)\n            console.log(\"Mixture model fitted.\")\n            status.update(\"Estimating term importances.\")\n            self.doc_topic_matrix = self.gmm_.predict_proba(X)\n            self.components_ = self.estimate_components()\n            console.log(\"Model fitting done.\")\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=self.doc_topic_matrix\n            )\n        return self.doc_topic_matrix\n\n    def fit_transform_multimodal(\n        self,\n        raw_documents: list[str],\n        images: list[ImageRepr],\n        y=None,\n        embeddings: Optional[MultimodalEmbeddings] = None,\n    ) -&gt; np.ndarray:\n        self.validate_embeddings(embeddings)\n        console = Console()\n        self.multimodal_embeddings = embeddings\n        with console.status(\"Fitting model\") as status:\n            if self.multimodal_embeddings is None:\n                status.update(\"Encoding documents\")\n                self.multimodal_embeddings = self.encode_multimodal(\n                    raw_documents, images\n                )\n                console.log(\"Documents encoded.\")\n            status.update(\"Extracting terms.\")\n            self.doc_term_matrix = self.vectorizer.fit_transform(raw_documents)\n            console.log(\"Term extraction done.\")\n            X = self.multimodal_embeddings[\"document_embeddings\"]\n            if self.dimensionality_reduction is not None:\n                status.update(\"Reducing embedding dimensionality.\")\n                X = self.dimensionality_reduction.fit_transform(embeddings)\n                console.log(\"Dimensionality reduction complete.\")\n            n_components = self.n_components\n            if self.n_components == \"auto\":\n                status.update(\"Finding optimal value of N\")\n                f_bic = self._create_bic(X)\n                n_components = optimize_n_components(f_bic, verbose=True)\n                console.log(f\"Found optimal N={n_components}.\")\n            status.update(\"Fitting mixture model.\")\n            self.gmm_ = self._init_model(n_components)\n            self.gmm_.fit(X)\n            console.log(\"Mixture model fitted.\")\n            status.update(\"Estimating term importances.\")\n            self.doc_topic_matrix = self.gmm_.predict_proba(X)\n            self.components_ = self.estimate_components()\n            console.log(\"Model fitting done.\")\n            try:\n                self.image_topic_matrix = self.transform(\n                    raw_documents,\n                    embeddings=self.multimodal_embeddings[\"image_embeddings\"],\n                )\n            except Exception as e:\n                warnings.warn(\n                    f\"Couldn't produce image topic matrix due to exception: {e}, using doc-topic matrix.\"\n                )\n                self.image_topic_matrix = self.doc_topic_matrix\n            self.top_images: list[list[Image.Image]] = self.collect_top_images(\n                images, self.image_topic_matrix\n            )\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=self.doc_topic_matrix\n            )\n            console.log(\"Transformation done.\")\n        return self.doc_topic_matrix\n\n    @property\n    def labels_(self):\n        return np.argmax(self.doc_topic_matrix, axis=1)\n\n    @property\n    def weights_(self) -&gt; np.ndarray:\n        if isinstance(self.gmm_, Pipeline):\n            model = self.gmm_.steps[-1][1]\n        else:\n            model = self.gmm_\n        return model.weights_\n\n    def transform(\n        self, raw_documents, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Infers topic importances for new documents based on a fitted model.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n\n        Returns\n        -------\n        ndarray of shape (n_dimensions, n_topics)\n            Document-topic matrix.\n        \"\"\"\n        if embeddings is None:\n            embeddings = self.encoder_.encode(raw_documents)\n        if self.dimensionality_reduction is not None:\n            embeddings = self.dimensionality_reduction.transform(embeddings)\n        return self.gmm_.predict_proba(embeddings)\n\n    def fit_transform_dynamic(\n        self,\n        raw_documents,\n        timestamps: list[datetime],\n        embeddings: Optional[np.ndarray] = None,\n        bins: Union[int, list[datetime]] = 10,\n    ):\n        time_labels, self.time_bin_edges = self.bin_timestamps(\n            timestamps, bins\n        )\n        if hasattr(self, \"components_\"):\n            doc_topic_matrix = self.transform(\n                raw_documents, embeddings=embeddings\n            )\n        else:\n            doc_topic_matrix = self.fit_transform(\n                raw_documents, embeddings=embeddings\n            )\n        self.doc_term_matrix = self.vectorizer.transform(raw_documents)\n        n_comp, n_vocab = self.components_.shape\n        n_bins = len(self.time_bin_edges) - 1\n        self.temporal_components_ = np.zeros(\n            (n_bins, n_comp, n_vocab), dtype=self.doc_term_matrix.dtype\n        )\n        self.temporal_importance_ = np.zeros((n_bins, n_comp))\n        for i_timebin in np.unique(time_labels):\n            topic_importances = doc_topic_matrix[time_labels == i_timebin].sum(\n                axis=0\n            )\n            # Normalizing\n            topic_importances = topic_importances / topic_importances.sum()\n            components = self.estimate_components(\n                doc_topic_matrix=doc_topic_matrix[time_labels == i_timebin],\n                doc_term_matrix=self.doc_term_matrix[time_labels == i_timebin],  # type: ignore\n            )\n            self.temporal_components_[i_timebin] = components\n            self.temporal_importance_[i_timebin] = topic_importances\n        return doc_topic_matrix\n\n    def plot_components_datamapplot(\n        self,\n        coordinates: Optional[np.ndarray] = None,\n        hover_text: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Creates an interactive browser plot of the topics in your data using datamapplot.\n\n        Parameters\n        ----------\n        coordinates: np.ndarray, default None\n            Lower dimensional projection of the embeddings.\n            If None, will try to use the projections from the\n            dimensionality_reduction method of the model.\n        hover_text: list of str, optional\n            Text to show when hovering over a document.\n\n        Returns\n        -------\n        plot\n            Interactive datamap plot, you can call the `.show()` method to\n            display it in your default browser or save it as static HTML using `.write_html()`.\n        \"\"\"\n        if coordinates is None:\n            if not hasattr(self, \"reduced_embeddings\"):\n                raise ValueError(\n                    \"Coordinates not specified, but the model does not contain reduced embeddings.\"\n                )\n            coordinates = self.reduced_embeddings[:, (0, 1)]\n        labels = np.argmax(self.doc_topic_matrix, axis=1)\n        plot = build_datamapplot(\n            coordinates=coordinates,\n            topic_names=self.topic_names,\n            labels=labels,\n            classes=np.arange(self.gmm_.n_components),\n            top_words=self.get_top_words(),\n            hover_text=hover_text,\n            topic_descriptions=getattr(self, \"topic_descriptions\", None),\n            **kwargs,\n        )\n        return plot\n\n    def plot_density(\n        self,\n        hover_text: list[str] = None,\n        show_keywords=True,\n        show_points=False,\n        light_mode=False,\n    ):\n        try:\n            import plotly.graph_objects as go\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n\n        if not hasattr(self, \"reduced_embeddings\"):\n            raise ValueError(\n                \"No reduced embeddings found, can't display in 2d space.\"\n            )\n        if self.reduced_embeddings.shape[1] != 2:\n            warnings.warn(\n                \"Embeddings are not in 2d space, only using first 2 dimensions\"\n            )\n        reduced_embeddings = self.reduced_embeddings[:, :2]\n        coord_min, coord_max = np.min(reduced_embeddings), np.max(\n            reduced_embeddings\n        )\n        coord_spread = coord_max - coord_min\n        coord_min = coord_min - coord_spread * 0.05\n        coord_max = coord_max + coord_spread * 0.05\n        coord = np.linspace(coord_min, coord_max, num=100)\n        z = []\n        for yval in coord:\n            points = np.stack([coord, np.full(coord.shape, yval)]).T\n            prob = np.exp(self.gmm_.score_samples(points))\n            z.append(prob)\n        z = np.stack(z)\n        color_grid = [0.0, 0.25, 0.5, 0.75, 1.0]\n        colorscale = [\n            \"#01014B\",\n            \"#000080\",\n            \"#5D5DEF\",\n            \"#B7B7FF\",\n            \"#ffffff\",\n        ]\n        if light_mode:\n            colorscale = colorscale[::-1]\n        traces = [\n            go.Contour(\n                z=z,\n                colorscale=list(zip(color_grid, colorscale)),\n                showscale=False,\n                x=coord,\n                y=coord,\n                hoverinfo=\"skip\",\n            ),\n        ]\n        if show_points:\n            scatter = go.Scatter(\n                x=reduced_embeddings[:, 0],\n                y=reduced_embeddings[:, 1],\n                mode=\"markers\",\n                showlegend=False,\n                text=hover_text,\n                marker=dict(\n                    symbol=\"circle\",\n                    opacity=0.5,\n                    color=\"white\",\n                    size=8,\n                    line=dict(width=1),\n                ),\n            )\n            traces.append(scatter)\n        fig = go.Figure(data=traces)\n        fig = fig.update_layout(\n            showlegend=False, margin=dict(r=0, l=0, t=0, b=0)\n        )\n        fig = fig.update_xaxes(showticklabels=False)\n        fig = fig.update_yaxes(showticklabels=False)\n        for mean, name, keywords in zip(\n            self.gmm_.means_, self.topic_names, self.get_top_words()\n        ):\n            _keys = \"\"\n            if show_keywords:\n                for i, key in enumerate(keywords):\n                    if (i % 5) == 0:\n                        _keys += \"&lt;br&gt; \"\n                    _keys += key\n                    if i &lt; (len(keywords) - 1):\n                        _keys += \",\"\n                    _keys += \" \"\n            text = f\"&lt;b&gt;{name}&lt;/b&gt; &lt;i&gt;{_keys}&lt;/i&gt; \"\n            fig.add_annotation(\n                text=text,\n                x=mean[0],\n                y=mean[1],\n                align=\"left\",\n                showarrow=False,\n                xshift=0,\n                yshift=50,\n                font=dict(family=\"Roboto Mono\", size=18, color=\"black\"),\n                bgcolor=\"rgba(255,255,255,0.9)\",\n                bordercolor=\"black\",\n                borderwidth=2,\n            )\n        return fig\n\n    def plot_density_3d(self, show_keywords=False):\n        try:\n            import plotly.graph_objects as go\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n\n        if not hasattr(self, \"reduced_embeddings\"):\n            raise ValueError(\n                \"No reduced embeddings found, can't display in 2d space.\"\n            )\n        if self.reduced_embeddings.shape[1] != 2:\n            warnings.warn(\n                \"Embeddings are not in 2d space, only using first 2 dimensions\"\n            )\n        reduced_embeddings = self.reduced_embeddings[:, :2]\n        coord_min, coord_max = np.min(reduced_embeddings), np.max(\n            reduced_embeddings\n        )\n        coord_spread = coord_max - coord_min\n        coord_min = coord_min - coord_spread * 0.05\n        coord_max = coord_max + coord_spread * 0.05\n        coord = np.linspace(coord_min, coord_max, num=100)\n        z = []\n        for yval in coord:\n            points = np.stack([coord, np.full(coord.shape, yval)]).T\n            prob = np.exp(self.gmm_.score_samples(points))\n            z.append(prob)\n        z = np.stack(z)\n        means = self.gmm_.means_\n        means_z = np.exp(self.gmm_.score_samples(means))\n        annotations = []\n        for (x_mean, y_mean), z_mean, name, keywords in zip(\n            means, means_z, self.topic_names, self.get_top_words()\n        ):\n            _keys = \"\"\n            if show_keywords:\n                for i, key in enumerate(keywords):\n                    if (i % 5) == 0:\n                        _keys += \"&lt;br&gt; \"\n                    _keys += key\n                    if i &lt; (len(keywords) - 1):\n                        _keys += \",\"\n                    _keys += \" \"\n            text = f\"&lt;b&gt;{name}&lt;/b&gt; &lt;i&gt;{_keys}&lt;/i&gt; \"\n            annotations.append(\n                dict(\n                    showarrow=True,\n                    x=x_mean,\n                    y=y_mean,\n                    z=z_mean,\n                    text=text,\n                    font=dict(family=\"Roboto Mono\", size=18, color=\"black\"),\n                    bgcolor=\"rgba(255,255,255,0.9)\",\n                    bordercolor=\"black\",\n                    borderwidth=2,\n                )\n            )\n        color_grid = [0.0, 0.25, 0.5, 0.75, 1.0]\n        colorscale = [\n            \"#01014B\",\n            \"#000080\",\n            \"#5D5DEF\",\n            \"#B7B7FF\",\n            \"#ffffff\",\n        ]\n        fig = go.Figure(\n            data=[\n                go.Surface(\n                    z=z,\n                    x=coord,\n                    y=coord,\n                    colorscale=list(zip(color_grid, colorscale)),\n                    showscale=False,\n                )\n            ]\n        )\n        fig = fig.update_layout(\n            margin=dict(l=0, r=0, b=0, t=0),\n            template=\"plotly_white\",\n            scene=dict(annotations=annotations),\n        )\n        return fig\n\n    def plot_components(\n        self,\n        show_points=False,\n        show_keywords=True,\n        hover_text: Optional[list[str]] = None,\n    ):\n        try:\n            import plotly.express as px\n            import plotly.graph_objects as go\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n\n        if not hasattr(self, \"reduced_embeddings\"):\n            raise ValueError(\n                \"No reduced embeddings found, can't display in 2d space.\"\n            )\n        if self.reduced_embeddings.shape[1] != 2:\n            warnings.warn(\n                \"Embeddings are not in 2d space, only using first 2 dimensions\"\n            )\n        reduced_embeddings = self.reduced_embeddings[:, :2]\n        coord_min, coord_max = np.min(reduced_embeddings), np.max(\n            reduced_embeddings\n        )\n        coord_spread = coord_max - coord_min\n        coord_min = coord_min - coord_spread * 0.05\n        coord_max = coord_max + coord_spread * 0.05\n        coord = np.linspace(coord_min, coord_max, num=100)\n        z = []\n        for yval in coord:\n            points = np.stack([coord, np.full(coord.shape, yval)]).T\n            prob = np.exp(self.gmm_.score_samples(points))\n            z.append(prob)\n        z = np.stack(z)\n        fig = go.Figure(\n            [\n                go.Contour(\n                    z=z,\n                    x=coord,\n                    y=coord,\n                    colorscale=\"Greys\",\n                    opacity=0.25,\n                    hoverinfo=\"skip\",\n                    showscale=False,\n                ),\n            ]\n        )\n        gmm_colors = px.colors.qualitative.Dark24\n        for i_std, n_std in enumerate(np.linspace(0.1, 3.0, num=5)):\n            for name, color, mean, cov in zip(\n                self.topic_names,\n                gmm_colors,\n                self.gmm_.means_,\n                self.gmm_.covariances_,\n            ):\n                fig.add_shape(\n                    legend=\"legend\",\n                    showlegend=False,\n                    type=\"path\",\n                    path=confidence_ellipse(mean, cov, n_std=n_std),\n                    legendgroup=name,\n                    name=0,\n                    legendwidth=0,\n                    fillcolor=color,\n                    opacity=0.1,\n                )\n        for mean, name, keywords in zip(\n            self.gmm_.means_, self.topic_names, self.get_top_words()\n        ):\n            _keys = \"\"\n            if show_keywords:\n                for i, key in enumerate(keywords):\n                    if (i % 5) == 0:\n                        _keys += \"&lt;br&gt; \"\n                    _keys += key\n                    if i &lt; (len(keywords) - 1):\n                        _keys += \",\"\n                    _keys += \" \"\n            text = f\"&lt;b&gt;{name}&lt;/b&gt; &lt;i&gt;{_keys}&lt;/i&gt; \"\n            fig.add_annotation(\n                text=text,\n                x=mean[0],\n                y=mean[1],\n                align=\"left\",\n                showarrow=False,\n                xshift=0,\n                yshift=50,\n                font=dict(family=\"Roboto Mono\", size=18, color=\"black\"),\n                bgcolor=\"rgba(255,255,255,0.9)\",\n                bordercolor=\"black\",\n                borderwidth=2,\n            )\n        fig = fig.update_layout(\n            margin=dict(l=0, r=0, b=0, t=0),\n            template=\"plotly_white\",\n        )\n        if show_points:\n            for i, (name, color) in enumerate(\n                zip(self.topic_names, gmm_colors)\n            ):\n                include = self.labels_ == i\n                text = (\n                    None\n                    if hover_text is None\n                    else [\n                        text\n                        for text, in_cluster in zip(hover_text, include)\n                        if in_cluster\n                    ]\n                )\n                scatter = go.Scatter(\n                    x=reduced_embeddings[:, 0][include],\n                    y=reduced_embeddings[:, 1][include],\n                    mode=\"markers\",\n                    showlegend=False,\n                    text=text,\n                    name=name,\n                    legendgroup=name,\n                    hovertemplate=f\"&lt;b&gt;{name}&lt;/b&gt;&lt;br&gt;%{{text}}\",\n                    marker=dict(\n                        symbol=\"circle\",\n                        opacity=0.5,\n                        color=color,\n                        size=6,\n                        line=dict(width=1),\n                    ),\n                )\n                fig.add_trace(scatter)\n        fig = fig.update_layout(coloraxis=dict(showscale=False))\n        return fig\n</code></pre>"},{"location":"GMM/#turftopic.models.gmm.GMM.plot_components_datamapplot","title":"<code>plot_components_datamapplot(coordinates=None, hover_text=None, **kwargs)</code>","text":"<p>Creates an interactive browser plot of the topics in your data using datamapplot.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>Optional[ndarray]</code> <p>Lower dimensional projection of the embeddings. If None, will try to use the projections from the dimensionality_reduction method of the model.</p> <code>None</code> <code>hover_text</code> <code>Optional[list[str]]</code> <p>Text to show when hovering over a document.</p> <code>None</code> <p>Returns:</p> Type Description <code>plot</code> <p>Interactive datamap plot, you can call the <code>.show()</code> method to display it in your default browser or save it as static HTML using <code>.write_html()</code>.</p> Source code in <code>turftopic/models/gmm.py</code> <pre><code>def plot_components_datamapplot(\n    self,\n    coordinates: Optional[np.ndarray] = None,\n    hover_text: Optional[list[str]] = None,\n    **kwargs,\n):\n    \"\"\"Creates an interactive browser plot of the topics in your data using datamapplot.\n\n    Parameters\n    ----------\n    coordinates: np.ndarray, default None\n        Lower dimensional projection of the embeddings.\n        If None, will try to use the projections from the\n        dimensionality_reduction method of the model.\n    hover_text: list of str, optional\n        Text to show when hovering over a document.\n\n    Returns\n    -------\n    plot\n        Interactive datamap plot, you can call the `.show()` method to\n        display it in your default browser or save it as static HTML using `.write_html()`.\n    \"\"\"\n    if coordinates is None:\n        if not hasattr(self, \"reduced_embeddings\"):\n            raise ValueError(\n                \"Coordinates not specified, but the model does not contain reduced embeddings.\"\n            )\n        coordinates = self.reduced_embeddings[:, (0, 1)]\n    labels = np.argmax(self.doc_topic_matrix, axis=1)\n    plot = build_datamapplot(\n        coordinates=coordinates,\n        topic_names=self.topic_names,\n        labels=labels,\n        classes=np.arange(self.gmm_.n_components),\n        top_words=self.get_top_words(),\n        hover_text=hover_text,\n        topic_descriptions=getattr(self, \"topic_descriptions\", None),\n        **kwargs,\n    )\n    return plot\n</code></pre>"},{"location":"GMM/#turftopic.models.gmm.GMM.transform","title":"<code>transform(raw_documents, embeddings=None)</code>","text":"<p>Infers topic importances for new documents based on a fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> required <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_dimensions, n_topics)</code> <p>Document-topic matrix.</p> Source code in <code>turftopic/models/gmm.py</code> <pre><code>def transform(\n    self, raw_documents, embeddings: Optional[np.ndarray] = None\n) -&gt; np.ndarray:\n    \"\"\"Infers topic importances for new documents based on a fitted model.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n\n    Returns\n    -------\n    ndarray of shape (n_dimensions, n_topics)\n        Document-topic matrix.\n    \"\"\"\n    if embeddings is None:\n        embeddings = self.encoder_.encode(raw_documents)\n    if self.dimensionality_reduction is not None:\n        embeddings = self.dimensionality_reduction.transform(embeddings)\n    return self.gmm_.predict_proba(embeddings)\n</code></pre>"},{"location":"KeyNMF/","title":"KeyNMF","text":"<p>KeyNMF is a topic model that relies on contextually sensitive embeddings for keyword retrieval and term importance estimation, while taking inspiration from classical matrix-decomposition approaches for extracting topics.</p> Schematic overview of KeyNMF <p>Here's an example of how you can fit and interpret a KeyNMF model in the easiest way.</p> <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(10, encoder=\"paraphrase-MiniLM-L3-v2\")\nmodel.fit(corpus)\n\nmodel.print_topics()\n</code></pre> <p>Which Embedding model should I use</p> <ul> <li>You should probably use KeyNMF with a <code>paraphrase-</code> type embedding model. These seem to perform best in most tasks. Some examples include:<ul> <li>paraphrase-MiniLM-L3-v2 - Absolutely tiny  </li> <li>paraphrase-mpnet-base-v2 - High performance </li> <li>paraphrase-multilingual-mpnet-base-v2 - Multilingual, high-performance  </li> </ul> </li> <li>KeyNMF works remarkably well with static models, which are incredibly fast, even on your laptop:<ul> <li>sentence-transformers/static-retrieval-mrl-en-v1 - Blazing Fast  </li> <li>sentence-transformers/static-similarity-mrl-multilingual-v1 - Multilingual, Blazing Fast  </li> </ul> </li> </ul>"},{"location":"KeyNMF/#how-does-keynmf-work","title":"How does KeyNMF work?","text":""},{"location":"KeyNMF/#keyword-extraction","title":"Keyword Extraction","text":"<p>KeyNMF discovers topics based on the importances of keywords for a given document. This is done by embedding words in a document, and then extracting the cosine similarities of documents to words using a transformer-model. Only the <code>top_n</code> keywords with positive similarity are kept.</p> Click to see formula <ul> <li> <ol> <li>Let \\(x_d\\) be the document's embedding produced with the encoder model.</li> <li> <ol> <li>Let \\(v_w\\) be the word's embedding produced with the encoder model.</li> <li>Calculate cosine similarity between word and document</li> </ol> <p>For each word \\(w\\) in the document \\(d\\):</p> \\[ \\text{sim}(d, w) = \\frac{x_d \\cdot v_w}{||x_d|| \\cdot ||v_w||} \\] </li> </ol> <p>For each document \\(d\\):</p> <ol> <li>Let \\(K_d\\) be the set of \\(N\\) keywords with the highest cosine similarity to document \\(d\\).</li> </ol> \\[ K_d = \\text{argmax}_{K^*} \\sum_{w \\in K^*}\\text{sim}(d,w)\\text{, where } |K_d| = N\\text{, and } \\\\ w \\in d \\] </li> <li> <p>Arrange positive keyword similarities into a keyword matrix \\(M\\) where the rows represent documents, and columns represent unique keywords.</p> \\[ M_{dw} =  \\begin{cases} \\text{sim}(d,w), &amp; \\text{if } w \\in K_d \\text{ and } \\text{sim}(d,w) &gt; 0 \\\\ 0, &amp; \\text{otherwise}. \\end{cases} \\] </li> </ul> <p>You can do this step manually if you want to precompute the keyword matrix. Keywords are represented as dictionaries mapping words to keyword importances.</p> <pre><code>model.extract_keywords([\"Cars are perhaps the most important invention of the last couple of centuries. They have revolutionized transportation in many ways.\"])\n</code></pre> <pre><code>[{'transportation': 0.44713873,\n  'invention': 0.560524,\n  'cars': 0.5046208,\n  'revolutionized': 0.3339205,\n  'important': 0.21803442}]\n</code></pre> <p>A precomputed Keyword matrix can also be used to fit a model:</p> <pre><code>keyword_matrix = model.extract_keywords(corpus)\nmodel.fit(None, keywords=keyword_matrix)\n</code></pre>"},{"location":"KeyNMF/#topic-discovery","title":"Topic Discovery","text":"<p>Topics in this matrix are then discovered using Non-negative Matrix Factorization. Essentially the model tries to discover underlying dimensions/factors along which most of the variance in term importance can be explained.</p> Click to see formula <ul> <li> <p>Decompose \\(M\\) with non-negative matrix factorization: \\(M \\approx WH\\), where \\(W\\) is the document-topic matrix, and \\(H\\) is the topic-term matrix. Non-negative Matrix Factorization is done with the coordinate-descent algorithm, minimizing square loss:</p> \\[ L(W,H) = ||M - WH||^2 \\] </li> </ul> <p>You can fit KeyNMF on the raw corpus, with precomputed embeddings or with precomputed keywords.</p> Fitting on a corpusPre-computed embeddingsPre-computed keyword matrix <pre><code>model.fit(corpus)\n</code></pre> <pre><code>from sentence_transformers import SentenceTransformer\n\ntrf = SentenceTransformer(\"all-MiniLM-L6-v2\")\nembeddings = trf.encode(corpus)\n\nmodel = KeyNMF(10, encoder=trf)\nmodel.fit(corpus, embeddings=embeddings)\n</code></pre> <pre><code>keyword_matrix = model.extract_keywords(corpus)\nmodel.fit(None, keywords=keyword_matrix)\n</code></pre>"},{"location":"KeyNMF/#seeded-topic-modeling","title":"Seeded Topic Modeling","text":"<p>When investigating a set of documents, you might already have an idea about what aspects you would like to explore. In KeyNMF, you can describe this aspect, from which you want to investigate your corpus, using a free-text seed-phrase, which will then be used to only extract topics, which are relevant to your research question.</p> How is this done? <p>KeyNMF encodes the seed phrase into a seed-embedding. Word importance scores in a document get weighted by their similarity to the seed-embedding.</p> <ul> <li>Embed seed-phrase into a seed-embedding: \\(s\\)</li> <li>When extracting keywords from a document:<ol> <li>Let \\(x_d\\) be the document's embedding produced with the encoder model.</li> <li>Let the document's relevance be \\(r_d = \\text{sim}(d,w)\\)</li> <li>For each word \\(w\\):<ol> <li>Let the word's importance in the keyword matrix be: \\(\\text{sim}(d, w) \\cdot r_d\\) if \\(r_d &gt; 0\\), otherwise \\(0\\)</li> </ol> </li> </ol> </li> </ul> <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(5, seed_phrase=\"&lt;your seed phrase&gt;\")\nmodel.fit(corpus)\n\nmodel.print_topics()\n</code></pre> <code>'Is the death penalty moral?'</code><code>'Evidence for the existence of god'</code><code>'Operating system kernels'</code> Topic ID Highest Ranking 0 morality, moral, immoral, morals, objective, morally, animals, society, species, behavior 1 armenian, armenians, genocide, armenia, turkish, turks, soviet, massacre, azerbaijan, kurdish 2 murder, punishment, death, innocent, penalty, kill, crime, moral, criminals, executed 3 gun, guns, firearms, crime, handgun, firearm, weapons, handguns, law, criminals 4 jews, israeli, israel, god, jewish, christians, sin, christian, palestinians, christianity Topic ID Highest Ranking 0 atheist, atheists, religion, religious, theists, beliefs, christianity, christian, religions, agnostic 1 bible, christians, christian, christianity, church, scripture, religion, jesus, faith, biblical 2 god, existence, exist, exists, universe, creation, argument, creator, believe, life 3 believe, faith, belief, evidence, blindly, believing, gods, believed, beliefs, convince 4 atheism, atheists, agnosticism, belief, arguments, believe, existence, alt, believing, argument Topic ID Highest Ranking 0 windows, dos, os, microsoft, ms, apps, pc, nt, file, shareware 1 ram, motherboard, card, monitor, memory, cpu, vga, mhz, bios, intel 2 unix, os, linux, intel, systems, programming, applications, compiler, software, platform 3 disk, scsi, disks, drive, floppy, drives, dos, controller, cd, boot 4 software, mac, hardware, ibm, graphics, apple, computer, pc, modem, program"},{"location":"KeyNMF/#dynamic-topic-modeling","title":"Dynamic Topic Modeling","text":"<p>KeyNMF is also capable of modeling topics over time. This happens by fitting a KeyNMF model first on the entire corpus, then fitting individual topic-term matrices using coordinate descent based on the document-topic and document-term matrices in the given time slices.</p> Click to see formula <ol> <li>Compute keyword matrix \\(M\\) for the whole corpus.</li> <li>Decompose \\(M\\) with non-negative matrix factorization: \\(M \\approx WH\\).</li> <li> <ol> <li>Let \\(W_t\\) be the document-topic proportions for documents in time slice \\(t\\), and \\(M_t\\) be the keyword matrix for words in time slice \\(t\\).</li> <li>Obtain the topic-term matrix for the time slice, by minimizing square loss using coordinate descent and fixing \\(W_t\\):</li> </ol> <p>For each time slice \\(t\\):</p> \\[ H_t = \\text{argmin}_{H^{*}} ||M_t - W_t H^{*}||^2 \\] </li> </ol> <p>Here's an example of using KeyNMF in a dynamic modeling setting:</p> <pre><code>from datetime import datetime\n\nfrom turftopic import KeyNMF\n\ncorpus: list[str] = []\ntimestamps: list[datetime] = []\n\nmodel = KeyNMF(5, top_n=5, random_state=42)\ndocument_topic_matrix = model.fit_transform_dynamic(\n    corpus, timestamps=timestamps, bins=10\n)\n</code></pre> <p>You can use the <code>print_topics_over_time()</code> method for producing a table of the topics over the generated time slices.</p> <p>This example uses CNN news data.</p> <pre><code>model.print_topics_over_time()\n</code></pre>   | Time Slice | 0_olympics_tokyo_athletes_beijing | 1_covid_vaccine_pandemic_coronavirus | 2_olympic_athletes_ioc_athlete | 3_djokovic_novak_tennis_federer | 4_ronaldo_cristiano_messi_manchester | | - | - | - | - | - | - | | 2012 12 06 - 2013 11 10 | genocide, yugoslavia, karadzic, facts, cnn | cnn, russia, chechnya, prince, merkel | france, cnn, francois, hollande, bike | tennis, tournament, wimbledon, grass, courts | beckham, soccer, retired, david, learn | | 2013 11 10 - 2014 10 14 | keith, stones, richards, musician, author | georgia, russia, conflict, 2008, cnn | civil, rights, hear, why, should | cnn, kidneys, traffickers, organ, nepal | ronaldo, cristiano, goalscorer, soccer, player | |  |  | ... |  |  |  | | 2020 05 07 - 2021 04 10 | olympics, beijing, xinjiang, ioc, boycott | covid, vaccine, coronavirus, pandemic, vaccination | olympic, japan, medalist, canceled, tokyo | djokovic, novak, tennis, federer, masterclass | ronaldo, cristiano, messi, juventus, barcelona | | 2021 04 10 - 2022 03 16 | olympics, tokyo, athletes, beijing, medal | covid, pandemic, vaccine, vaccinated, coronavirus | olympic, athletes, ioc, medal, athlete | djokovic, novak, tennis, wimbledon, federer | ronaldo, cristiano, messi, manchester, scored |   <p>You can also display the topics over time on an interactive HTML figure. The most important words for topics get revealed by hovering over them.</p> <p>You will need to install Plotly for this to work.</p> <pre><code>pip install plotly\n</code></pre> <pre><code>model.plot_topics_over_time()\n</code></pre>  Topics over time in a Dynamic KeyNMF model."},{"location":"KeyNMF/#hierarchical-topic-modeling","title":"Hierarchical Topic Modeling","text":"<p>When you suspect that subtopics might be present in the topics you find with the model, KeyNMF can be used to discover topics further down the hierarchy.</p> <p>This is done by utilising a special case of weighted NMF, where documents are weighted by how high they score on the parent topic.</p> Click to see formula <ol> <li>Decompose keyword matrix \\(M \\approx WH\\)</li> <li>To find subtopics in topic \\(j\\), define document weights \\(w\\) as the \\(j\\)th column of \\(W\\).</li> <li>Estimate subcomponents with wNMF \\(M \\approx \\mathring{W} \\mathring{H}\\) with document weight \\(w\\)<ol> <li>Initialise \\(\\mathring{H}\\) and  \\(\\mathring{W}\\) randomly.</li> <li>Perform multiplicative updates until convergence.  \\(\\mathring{W}^T = \\mathring{W}^T \\odot \\frac{\\mathring{H} \\cdot (M^T \\odot w)}{\\mathring{H} \\cdot \\mathring{H}^T \\cdot (\\mathring{W}^T \\odot w)}\\) \\(\\mathring{H}^T = \\mathring{H}^T \\odot \\frac{ (M^T \\odot w)\\cdot \\mathring{W}}{\\mathring{H}^T \\cdot (\\mathring{W}^T \\odot w) \\cdot \\mathring{W}}\\)</li> </ol> </li> <li>To sufficiently differentiate the subcomponents from each other a pseudo-c-tf-idf weighting scheme is applied to \\(\\mathring{H}\\):<ol> <li>\\(\\mathring{H} = \\mathring{H}_{ij} \\odot ln(1 + \\frac{A}{1+\\sum_k \\mathring{H}_{kj}})\\), where \\(A\\) is the average of all elements in \\(\\mathring{H}\\)</li> </ol> </li> </ol> <p>To create a hierarchical model, you can use the <code>hierarchy</code> property of the model.</p> <pre><code># This divides each of the topics in the model to 3 subtopics.\nmodel.hierarchy.divide_children(n_subtopics=3)\nprint(model.hierarchy)\n</code></pre> Root  \u251c\u2500\u2500 0: windows, dos, os, disk, card, drivers, file, pc, files, microsoft  \u2502   \u251c\u2500\u2500 0.0: dos, file, disk, files, program, windows, disks, shareware, norton, memory  \u2502   \u251c\u2500\u2500 0.1: os, unix, windows, microsoft, apps, nt, ibm, ms, os2, platform  \u2502   \u2514\u2500\u2500 0.2: card, drivers, monitor, driver, vga, ram, motherboard, cards, graphics, ati  \u2514\u2500\u2500 1: atheism, atheist, atheists, religion, christians, religious, belief, christian, god, beliefs  .    \u251c\u2500\u2500 1.0: atheism, alt, newsgroup, reading, faq, islam, questions, read, newsgroups, readers  .    \u251c\u2500\u2500 1.1: atheists, atheist, belief, theists, beliefs, religious, religion, agnostic, gods, religions  .    \u2514\u2500\u2500 1.2: morality, bible, christian, christians, moral, christianity, biblical, immoral, god, religion  <p>For a detailed tutorial on hierarchical modeling click here.</p>"},{"location":"KeyNMF/#cross-lingual-keynmf","title":"Cross-lingual KeyNMF","text":"<p>KeyNMF, by default, does not come with cross-lingual capabilities, since only words that appear in a document can be assigned to it as keywords. We, however provide a term-matching scheme that allows you to match words across languages based on their cosine similarity in a multilingual embedding model.</p> <p>This is done by:</p> <ol> <li>Computing a similarity matrix over terms.</li> <li>Checking, which terms have similarity over a given threshold (0.9 is the default)</li> <li>Building a graph from these connections, and finding graph components.</li> <li>Adding up term importances for terms that appear in the same component for all documents.</li> </ol> <pre><code>from datasets import load_dataset\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom turftopic import KeyNMF\n\n# Loading a parallel corpus\nds = load_dataset(\n    \"aiana94/polynews-parallel\", \"deu_Latn-eng_Latn\", split=\"train\"\n)\n# Subsampling\nds = ds.train_test_split(test_size=1000)[\"test\"]\ncorpus = ds[\"src\"] + ds[\"tgt\"]\n\nmodel = KeyNMF(\n    10,\n    cross_lingual=True,\n    encoder=\"paraphrase-multilingual-MiniLM-L12-v2\",\n    vectorizer=CountVectorizer()\n)\nmodel.fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking ... 15 africa-afrikanisch-african, media-medien-medienwirksam, schwarzwald-negroe-schwarzer, apartheid, difficulties-complicated-problems, kontinents-continent-kontinent, \u00e4thiopien-ethiopia, investitionen-investiert-investierenden, satire-satirical, hundred-100-1001 16 lawmaker-judges-gesetzliche, schutz-sicherheit-gesch\u00fctzt, an-success-eintreten, australian-australien-australischen, appeal-appealing-appeals, lawyer-lawyers-attorney, regeln-rule-rules, \u00f6ffentlichkeit-\u00f6ffentliche-publicly, terrorism-terroristischer-terrorismus, convicted 17 israels-israel-israeli, pal\u00e4stinensischen-palestinians-palestine, gay-lgbtq-gays, david, blockaden-blockades-blockade, stars-star-stelle, aviv, bombardieren-bombenexplosion-bombing, milit\u00e4rischer-army-military, kampfflugzeuge-warplanes 18 russischer-russlands-russischen, facebookbeitrag-facebook-facebooks, soziale-gesellschaftliche-sozialbauten, internetnutzer-internet, activism-aktivisten-activists, webseiten-web-site, isis, netzwerken-networks-netzwerk, vkontakte, media-medien-medienwirksam 19 bundesstaates-regierenden-regiert, chinesischen-chinesische-chinesisch, pr\u00e4sidentschaft-presidential-president, regions-region-regionen, demokratien-democratic-democracy, kapitalismus-capitalist-capitalism, staatsb\u00fcrgerin-citizens-b\u00fcrger, jemen-jemenitische-yemen, angolanischen-angola, media-medien-medienwirksam"},{"location":"KeyNMF/#online-topic-modeling","title":"Online Topic Modeling","text":"<p>KeyNMF can also be fitted in an online manner. This is done by fitting NMF with batches of data instead of the whole dataset at once.</p>"},{"location":"KeyNMF/#use-cases","title":"Use Cases:","text":"<ol> <li>You can use online fitting when you have very large corpora at hand, and it would be impractical to fit a model on it at once.</li> <li>You have new data flowing in constantly, and need a model that can morph the topics based on the incoming data. You can also do this in a dynamic fashion.</li> <li>You need to finetune an already fitted topic model to novel data.</li> </ol>"},{"location":"KeyNMF/#batch-fitting","title":"Batch Fitting","text":"<p>We will use the batching function from the itertools recipes to produce batches.</p> <p>In newer versions of Python (&gt;=3.12) you can just <code>from itertools import batched</code></p> <pre><code>def batched(iterable, n: int):\n    \"Batch data into lists of length n. The last batch may be shorter.\"\n    if n &lt; 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while batch := tuple(itertools.islice(it, n)):\n        yield batch\n</code></pre> <p>You can fit a KeyNMF model to a very large corpus in batches like so:</p> <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(10, top_n=5)\n\ncorpus = [\"some string\", \"etc\", ...]\nfor batch in batched(corpus, 200):\n    batch = list(batch)\n    model.partial_fit(batch)\n</code></pre>"},{"location":"KeyNMF/#precomputing-the-keyword-matrix","title":"Precomputing the Keyword Matrix","text":"<p>If you desire the best results, it might make sense for you to go over the corpus in multiple epochs:</p> <pre><code>for epoch in range(5):\n    for batch in batched(corpus, 200):\n        model.partial_fit(batch)\n</code></pre> <p>This is mildly inefficient, however, as the texts need to be encoded on every epoch, and keywords need to be extracted. In such scenarios you might want to precompute and maybe even save the extracted keywords to disk using the <code>extract_keywords()</code> method.</p> <p>Keywords are represented as dictionaries mapping words to keyword importances.</p> <pre><code>model.extract_keywords([\"Cars are perhaps the most important invention of the last couple of centuries. They have revolutionized transportation in many ways.\"])\n</code></pre> <pre><code>[{'transportation': 0.44713873,\n  'invention': 0.560524,\n  'cars': 0.5046208,\n  'revolutionized': 0.3339205,\n  'important': 0.21803442}]\n</code></pre> <p>You can extract keywords in batches and save them to disk to a file format of your choice. In this example I will use NDJSON because of its simplicity.</p> <pre><code>import json\nfrom pathlib import Path\nfrom typing import Iterable\n\n# Here we are saving keywords to a JSONL/NDJSON file\nwith Path(\"keywords.jsonl\").open(\"w\") as keyword_file:\n    # Doing this in batches is much more efficient than individual texts because\n    # of the encoding.\n    for batch in batched(corpus, 200):\n        batch_keywords = model.extract_keywords(batch)\n        # We serialize each\n        for keywords in batch_keywords:\n            keyword_file.write(json.dumps(keywords) + \"\\n\")\n\ndef stream_keywords() -&gt; Iterable[dict[str, float]]:\n    \"\"\"This function streams keywords from the file.\"\"\"\n    with Path(\"keywords.jsonl\").open() as keyword_file:\n        for line in keyword_file:\n            yield json.loads(line.strip())\n\nfor epoch in range(5):\n    keyword_stream = stream_keywords()\n    for keyword_batch in batched(keyword_stream, 200):\n        model.partial_fit(keywords=keyword_batch)\n</code></pre>"},{"location":"KeyNMF/#dynamic-online-topic-modeling","title":"Dynamic Online Topic Modeling","text":"<p>KeyNMF can be online fitted in a dynamic manner as well. This is useful when you have large corpora of text over time, or when you want to fit the model on future information flowing in and want to analyze the topics' changes over time.</p> <p>When using dynamic online topic modeling you have to predefine the time bins that you will use, as the model can't infer these from the data.</p> <pre><code>from datetime import datetime\n\n# We will bin by years in a period of 2020-2030\nbins = [datetime(year=y, month=1, day=1) for y in range(2020, 2030 + 2, 1)]\n</code></pre> <p>You can then online fit a dynamic topic model with <code>partial_fit_dynamic()</code>.</p> <pre><code>model = KeyNMF(5, top_n=10)\n\ncorpus: list[str] = [...]\ntimestamps: list[datetime] = [...]\n\nfor batch in batched(zip(corpus, timestamps)):\n    text_batch, ts_batch = zip(*batch)\n    model.partial_fit_dynamic(text_batch, timestamps=ts_batch, bins=bins)\n</code></pre>"},{"location":"KeyNMF/#asymmetric-and-instruction-tuned-embedding-models","title":"Asymmetric and Instruction-tuned Embedding Models","text":"<p>Some embedding models can be used together with prompting, or encode queries and passages differently. This is important for KeyNMF, as it is explicitly based on keyword retrieval, and its performance can be substantially enhanced by using asymmetric or prompted embeddings. Microsoft's E5 models are, for instance, all prompted by default, and it would be detrimental to performance not to do so yourself.</p> <p>In these cases, you're better off NOT passing a string to Turftopic models, but explicitly loading the model using <code>sentence-transformers</code>.</p> <p>Here's an example of using instruct models for keyword retrieval with KeyNMF. In this case, documents will serve as the queries and words as the passages:</p> <pre><code>from turftopic import KeyNMF\nfrom sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\n    \"intfloat/multilingual-e5-large-instruct\",\n    prompts={\n        \"query\": \"Instruct: Retrieve relevant keywords from the given document. Query: \"\n        \"passage\": \"Passage: \"\n    },\n    # Make sure to set default prompt to query!\n    default_prompt_name=\"query\",\n)\nmodel = KeyNMF(10, encoder=encoder)\n</code></pre> <p>And a regular, asymmetric example:</p> <pre><code>encoder = SentenceTransformer(\n    \"intfloat/e5-large-v2\",\n    prompts={\n        \"query\": \"query: \"\n        \"passage\": \"passage: \"\n    },\n    # Make sure to set default prompt to query!\n    default_prompt_name=\"query\",\n)\nmodel = KeyNMF(10, encoder=encoder)\n</code></pre> <p>Setting the default prompt to <code>query</code> is especially important, when you are precomputing embeddings, as <code>query</code> should always be your default prompt to embed documents with.</p>"},{"location":"KeyNMF/#api-reference","title":"API Reference","text":""},{"location":"KeyNMF/#turftopic.models.keynmf.KeyNMF","title":"<code>turftopic.models.keynmf.KeyNMF</code>","text":"<p>             Bases: <code>ContextualModel</code>, <code>DynamicTopicModel</code>, <code>MultimodalModel</code></p> <p>Extracts keywords from documents based on semantic similarity of term encodings to document encodings. Topics are then extracted with non-negative matrix factorization from keywords' proximity to documents.</p> <pre><code>from turftopic import KeyNMF\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\nmodel = KeyNMF(10, top_n=10).fit(corpus)\nmodel.print_topics()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>Union[int, Literal['auto']]</code> <p>Number of topics. Auto selects the number of topics using the Bayesian Information Criterion.</p> required <code>encoder</code> <code>Union[Encoder, str, MultimodalEncoder]</code> <p>Model to encode documents/terms, all-MiniLM-L6-v2 is the default.</p> <code>'sentence-transformers/all-MiniLM-L6-v2'</code> <code>vectorizer</code> <code>Optional[CountVectorizer]</code> <p>Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.</p> <code>None</code> <code>top_n</code> <code>int</code> <p>Number of keywords to extract for each document.</p> <code>25</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state to use so that results are exactly reproducible.</p> <code>None</code> <code>metric</code> <code>Literal['cosine', 'dot']</code> <p>Similarity metric to use for keyword extraction.</p> <code>'cosine'</code> <code>seed_phrase</code> <code>Optional[str]</code> <p>Describes an aspect of the corpus that the model should explore. It can be a free-text query, such as \"Christian Denominations: Protestantism and Catholicism\"</p> <code>None</code> <code>seed_exponent</code> <code>float</code> <p>Exponent that is applied to document weight in relation to the provided seed phrase.</p> <code>2.0</code> <code>cross_lingual</code> <code>bool</code> <p>Indicates whether KeyNMF should match terms across languages. This is useful when you have a corpus containing multiple languages.</p> <code>False</code> <code>term_match_threshold</code> <code>float</code> <p>Cosine similarity threshold for matching terms across languages.</p> <code>0.9</code> Source code in <code>turftopic/models/keynmf.py</code> <pre><code>class KeyNMF(ContextualModel, DynamicTopicModel, MultimodalModel):\n    \"\"\"Extracts keywords from documents based on semantic similarity of\n    term encodings to document encodings.\n    Topics are then extracted with non-negative matrix factorization from\n    keywords' proximity to documents.\n\n    ```python\n    from turftopic import KeyNMF\n\n    corpus: list[str] = [\"some text\", \"more text\", ...]\n\n    model = KeyNMF(10, top_n=10).fit(corpus)\n    model.print_topics()\n    ```\n\n    Parameters\n    ----------\n    n_components: int or \"auto\"\n        Number of topics.\n        Auto selects the number of topics using\n        the Bayesian Information Criterion.\n    encoder: str or SentenceTransformer\n        Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n    vectorizer: CountVectorizer, default None\n        Vectorizer used for term extraction.\n        Can be used to prune or filter the vocabulary.\n    top_n: int, default 25\n        Number of keywords to extract for each document.\n    random_state: int, default None\n        Random state to use so that results are exactly reproducible.\n    metric: \"cosine\" or \"dot\", default \"cosine\"\n        Similarity metric to use for keyword extraction.\n    seed_phrase: str, default None\n        Describes an aspect of the corpus that the model should explore.\n        It can be a free-text query, such as\n        \"Christian Denominations: Protestantism and Catholicism\"\n    seed_exponent: float, default 2.0\n        Exponent that is applied to document weight in relation to the provided seed phrase.\n    cross_lingual: bool, default False\n        Indicates whether KeyNMF should match terms across languages.\n        This is useful when you have a corpus containing multiple languages.\n    term_match_threshold: float, default 0.9\n        Cosine similarity threshold for matching terms across languages.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: Union[int, Literal[\"auto\"]],\n        encoder: Union[\n            Encoder, str, MultimodalEncoder\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        top_n: int = 25,\n        random_state: Optional[int] = None,\n        metric: Literal[\"cosine\", \"dot\"] = \"cosine\",\n        seed_phrase: Optional[str] = None,\n        seed_exponent: float = 2.0,\n        cross_lingual: bool = False,\n        term_match_threshold: float = 0.9,\n    ):\n        self.random_state = random_state\n        self.n_components = n_components\n        self.top_n = top_n\n        self.seed_exponent = seed_exponent\n        self.metric = metric\n        self.encoder = encoder\n        self._has_custom_vectorizer = vectorizer is not None\n        if isinstance(encoder, str):\n            if (\n                encoder == \"sentence-transformers/all-MiniLM-L6-v2\"\n            ) and cross_lingual:\n                warnings.warn(\n                    \"all-MiniLM is incompatible with cross-lingual transfer, using paraphrase-multilingual-MiniLM-L12-v2.\"\n                )\n                encoder = \"paraphrase-multilingual-MiniLM-L12-v2\"\n            self.encoder_ = SentenceTransformer(encoder)\n        else:\n            self.encoder_ = encoder\n        self.validate_encoder()\n        if vectorizer is None:\n            self.vectorizer = default_vectorizer()\n        else:\n            self.vectorizer = vectorizer\n        self.model = KeywordNMF(\n            n_components=n_components, seed=random_state, top_n=self.top_n\n        )\n        self.extractor = SBertKeywordExtractor(\n            top_n=self.top_n,\n            vectorizer=self.vectorizer,\n            encoder=self.encoder_,\n            metric=self.metric,\n        )\n        self.seed_phrase = seed_phrase\n        self.seed_embedding = None\n        if self.seed_phrase is not None:\n            self.seed_embedding = self.encoder_.encode([self.seed_phrase])[0]\n        self.cross_lingual = cross_lingual\n        self.term_match_threshold = term_match_threshold\n\n    def extract_keywords(\n        self,\n        batch_or_document: Union[str, list[str]],\n        embeddings: Optional[np.ndarray] = None,\n        fitting: bool = True,\n    ) -&gt; list[dict[str, float]]:\n        \"\"\"Extracts keywords from a document or a batch of documents.\n\n        Parameters\n        ----------\n        batch_or_document: str | list[str]\n            A single document or a batch of documents.\n        embeddings: ndarray, optional\n            Precomputed document embeddings.\n        \"\"\"\n        if isinstance(batch_or_document, str):\n            batch_or_document = [batch_or_document]\n        keywords = self.extractor.batch_extract_keywords(\n            batch_or_document,\n            embeddings=embeddings,\n            seed_embedding=self.seed_embedding,\n            seed_exponent=self.seed_exponent,\n            fitting=fitting,\n        )\n        if self.cross_lingual:\n            keywords = self.extractor.match_terms(\n                keywords, threshold=self.term_match_threshold\n            )\n        return keywords\n\n    def vectorize(\n        self,\n        raw_documents=None,\n        embeddings: Optional[np.ndarray] = None,\n        keywords: Optional[list[dict[str, float]]] = None,\n    ) -&gt; spr.csr_array:\n        \"\"\"Creates document-term-matrix from documents.\"\"\"\n        if keywords is None:\n            keywords = self.extract_keywords(\n                raw_documents, embeddings=embeddings\n            )\n        return self.model.vectorize(keywords)\n\n    def divide_topic(\n        self,\n        node: DivisibleTopicNode,\n        n_subtopics: int,\n    ) -&gt; list[DivisibleTopicNode]:\n        document_term_matrix = getattr(self, \"document_term_matrix\", None)\n        if document_term_matrix is None:\n            raise ValueError(\n                \"document_term_matrix is needed for computing hierarchies. Perhaps you fitted the model online?\"\n            )\n        dtm = document_term_matrix\n        subtopics = []\n        weight = node.document_topic_vector\n        subcomponents, sub_doc_topic = weighted_nmf(\n            dtm, weight, n_subtopics, self.random_state, max_iter=200\n        )\n        subcomponents = subcomponents * np.log(\n            1 + subcomponents.mean() / (subcomponents.sum(axis=0) + 1)\n        )\n        subcomponents = normalize(subcomponents, axis=1, norm=\"l2\")\n        for i, component, doc_topic_vector in zip(\n            range(n_subtopics), subcomponents, sub_doc_topic.T\n        ):\n            sub = DivisibleTopicNode(\n                self,\n                path=(*node.path, i),\n                word_importance=component,\n                document_topic_vector=doc_topic_vector,\n                children=None,\n            )\n            subtopics.append(sub)\n        return subtopics\n\n    def fit_transform(\n        self,\n        raw_documents=None,\n        y=None,\n        embeddings: Optional[np.ndarray] = None,\n        keywords: Optional[list[dict[str, float]]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Fits topic model and returns topic importances for documents.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str, optional\n            Documents to fit the model on.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n        keywords: list[dict[str, float]], optional\n            Precomputed keyword dictionaries.\n\n        Returns\n        -------\n        ndarray of shape (n_dimensions, n_topics)\n            Document-topic matrix.\n        \"\"\"\n        if self.seed_phrase is not None and keywords is not None:\n            warnings.warn(\n                \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n            )\n        console = Console()\n        with console.status(\"Running KeyNMF\") as status:\n            if keywords is None:\n                status.update(\"Extracting keywords\")\n                keywords = self.extract_keywords(\n                    raw_documents, embeddings=embeddings\n                )\n                console.log(\"Keyword extraction done.\")\n            status.update(\"Decomposing with NMF\")\n            try:\n                doc_topic_matrix = self.model.transform(keywords)\n            except (NotFittedError, AttributeError):\n                doc_topic_matrix = self.model.fit_transform(keywords)\n                self.components_ = self.model.components\n            console.log(\"Model fitting done.\")\n        self.document_topic_matrix = doc_topic_matrix\n        self.document_term_matrix = self.model.vectorize(keywords)\n        self.hierarchy = DivisibleTopicNode.create_root(\n            self, self.components_, self.document_topic_matrix\n        )\n        self.top_documents = self.get_top_documents(\n            raw_documents, document_topic_matrix=doc_topic_matrix\n        )\n        return doc_topic_matrix\n\n    def fit_transform_multimodal(\n        self,\n        raw_documents: list[str],\n        images: list[ImageRepr],\n        y=None,\n        embeddings: Optional[MultimodalEmbeddings] = None,\n    ) -&gt; np.ndarray:\n        self.validate_embeddings(embeddings)\n        console = Console()\n        self.multimodal_embeddings = embeddings\n        with console.status(\"Fitting model\") as status:\n            if self.multimodal_embeddings is None:\n                status.update(\"Encoding documents\")\n                self.multimodal_embeddings = self.encode_multimodal(\n                    raw_documents, images\n                )\n                console.log(\"Documents encoded.\")\n            status.update(\"Extracting keywords\")\n            document_keywords = self.extract_keywords(\n                raw_documents,\n                embeddings=self.multimodal_embeddings[\"document_embeddings\"],\n            )\n            image_keywords = self.extract_keywords(\n                raw_documents,\n                embeddings=self.multimodal_embeddings[\"image_embeddings\"],\n            )\n            console.log(\"Keyword extraction done.\")\n            status.update(\"Decomposing with NMF\")\n            try:\n                doc_topic_matrix = self.model.transform(document_keywords)\n            except (NotFittedError, AttributeError):\n                doc_topic_matrix = self.model.fit_transform(document_keywords)\n                self.components_ = self.model.components\n            console.log(\"Model fitting done.\")\n            status.update(\"Transforming images\")\n            self.image_topic_matrix = self.model.transform(image_keywords)\n            self.top_images: list[list[Image.Image]] = self.collect_top_images(\n                images, self.image_topic_matrix\n            )\n            console.log(\"Images transformed\")\n        self.document_topic_matrix = doc_topic_matrix\n        self.document_term_matrix = self.model.vectorize(document_keywords)\n        self.hierarchy = DivisibleTopicNode.create_root(\n            self, self.components_, self.document_topic_matrix\n        )\n        self.top_documents = self.get_top_documents(\n            raw_documents, document_topic_matrix=doc_topic_matrix\n        )\n        return doc_topic_matrix\n\n    def fit(\n        self,\n        raw_documents=None,\n        y=None,\n        embeddings: Optional[np.ndarray] = None,\n        keywords: Optional[list[dict[str, float]]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Fits topic model and returns topic importances for documents.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str, optional\n            Documents to fit the model on.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n        keywords: list[dict[str, float]], optional\n            Precomputed keyword dictionaries.\n        \"\"\"\n        self.fit_transform(\n            raw_documents, y, embeddings=embeddings, keywords=keywords\n        )\n        return self\n\n    def get_vocab(self) -&gt; np.ndarray:\n        return np.array(self.model.index_to_key)\n\n    def transform(\n        self,\n        raw_documents=None,\n        embeddings: Optional[np.ndarray] = None,\n        keywords: Optional[list[dict[str, float]]] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Infers topic importances for new documents based on a fitted model.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n        keywords: list[dict[str, float]], optional\n            Precomputed keyword dictionaries.\n\n        Returns\n        -------\n        ndarray of shape (n_dimensions, n_topics)\n            Document-topic matrix.\n        \"\"\"\n        if self.seed_phrase is not None and keywords is not None:\n            warnings.warn(\n                \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n            )\n        if keywords is None and raw_documents is None:\n            raise ValueError(\n                \"You have to pass either keywords or raw_documents.\"\n            )\n        if keywords is None:\n            keywords = self.extract_keywords(\n                list(raw_documents),\n                embeddings=embeddings,\n                fitting=False,\n            )\n        return self.model.transform(keywords)\n\n    def partial_fit(\n        self,\n        raw_documents: Optional[list[str]] = None,\n        embeddings: Optional[np.ndarray] = None,\n        keywords: Optional[list[dict[str, float]]] = None,\n    ):\n        \"\"\"Online fits KeyNMF on a batch of documents.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n        keywords: list[dict[str, float]], optional\n            Precomputed keyword dictionaries.\n        \"\"\"\n        if self.seed_phrase is not None and keywords is not None:\n            warnings.warn(\n                \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n            )\n        if self.cross_lingual:\n            raise ValueError(\n                \"Cross-lingual online topic modeling is yet to be implemented in KeyNMF.\"\n            )\n        if not self._has_custom_vectorizer:\n            self.vectorizer = CountVectorizer(stop_words=\"english\")\n            self._has_custom_vectorizer = True\n        min_df = self.vectorizer.min_df\n        max_df = self.vectorizer.max_df\n        if (min_df != 1) or (max_df != 1.0):\n            warnings.warn(\n                f\"\"\"When applying partial fitting, the vectorizer is fitted batch-wise in KeyNMF.\n            You have a vectorizer with min_df={min_df}, and max_df={max_df}.\n            If you continue with these settings, all tokens might get filtered out.\n            We recommend setting min_df=1 and max_df=1.0 for online fitting.\n            `model = KeyNMF(10, vectorizer=CountVectorizer(min_df=1, max_df=1.0)`\n            \"\"\"\n            )\n        if keywords is None and raw_documents is None:\n            raise ValueError(\n                \"You have to pass either keywords or raw_documents.\"\n            )\n        if keywords is None:\n            keywords = self.extract_keywords(\n                raw_documents, embeddings=embeddings\n            )\n        self.model.partial_fit(keywords)\n        self.components_ = self.model.components\n        return self\n\n    def prepare_topic_data(\n        self,\n        corpus: Optional[list[str]],\n        embeddings: Optional[np.ndarray] = None,\n        keywords: Optional[list[dict[str, float]]] = None,\n    ) -&gt; TopicData:\n        if keywords is None and corpus is None:\n            raise ValueError(\n                \"You have to pass either keywords or raw_documents.\"\n            )\n        if self.seed_phrase is not None and keywords is not None:\n            warnings.warn(\n                \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n            )\n        console = Console()\n        with console.status(\"Running KeyNMF\") as status:\n            if embeddings is None:\n                embeddings = self.encode_documents(corpus)\n            if keywords is None:\n                status.update(\"Extracting keywords\")\n                keywords = self.extract_keywords(corpus, embeddings=embeddings)\n                console.log(\"Keyword extraction done.\")\n            if (corpus is not None) and (len(keywords) != len(corpus)):\n                raise ValueError(\n                    \"length of keywords is not the same as length of the corpus\"\n                )\n            status.update(\"Decomposing with NMF\")\n            try:\n                doc_topic_matrix = self.model.transform(keywords)\n            except (NotFittedError, AttributeError):\n                doc_topic_matrix = self.model.fit_transform(keywords)\n                self.components_ = self.model.components\n            console.log(\"Model fitting done.\")\n            document_term_matrix = self.model.vectorize(keywords)\n        self.document_topic_matrix = doc_topic_matrix\n        self.document_term_matrix = document_term_matrix\n        self.hierarchy = DivisibleTopicNode.create_root(\n            self, self.components_, self.document_topic_matrix\n        )\n        res = TopicData(\n            corpus=corpus,\n            document_term_matrix=document_term_matrix,\n            vocab=self.get_vocab(),\n            document_topic_matrix=doc_topic_matrix,\n            document_representation=embeddings,\n            topic_term_matrix=self.components_,  # type: ignore\n            transform=getattr(self, \"transform\", None),\n            topic_names=self.topic_names,\n            hierarchy=getattr(self, \"hierarchy\", None),\n        )\n        return res\n\n    def prepare_dynamic_topic_data(\n        self,\n        corpus: Optional[list[str]],\n        timestamps: list[datetime],\n        embeddings: Optional[np.ndarray] = None,\n        bins: Union[int, list[datetime]] = 10,\n        keywords: Optional[list[dict[str, float]]] = None,\n    ):\n        if self.seed_phrase is not None and keywords is not None:\n            warnings.warn(\n                \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n            )\n        if ((corpus is not None) and (keywords is not None)) and (\n            len(keywords) != len(corpus)\n        ):\n            raise ValueError(\n                \"Length of keywords is not the same as length of the corpus\"\n            )\n        if (corpus is None) and (keywords is None):\n            raise TypeError(\n                \"You have to pass keywords or a corpus, but both are None.\"\n            )\n        if embeddings is None:\n            embeddings = self.encode_documents(corpus)\n        console = Console()\n        with console.status(\"Running KeyNMF\") as status:\n            if embeddings is None:\n                embeddings = self.encode_documents(corpus)\n            if keywords is None:\n                status.update(\"Extracting keywords\")\n                keywords = self.extract_keywords(corpus, embeddings=embeddings)\n                console.log(\"Keyword extraction done.\")\n            if (corpus is not None) and (len(keywords) != len(corpus)):\n                raise ValueError(\n                    \"length of keywords is not the same as length of the corpus\"\n                )\n            status.update(\"Decomposing with NMF\")\n            try:\n                doc_topic_matrix = self.model.transform(keywords)\n            except (NotFittedError, AttributeError):\n                doc_topic_matrix = self.fit_transform_dynamic(\n                    corpus,\n                    embeddings=embeddings,\n                    keywords=keywords,\n                    bins=bins,\n                    timestamps=timestamps,\n                )\n            console.log(\"Model fitting done.\")\n            document_term_matrix = self.model.vectorize(keywords)\n        try:\n            classes = self.classes_\n        except AttributeError:\n            classes = list(range(self.components_.shape[0]))\n        res = TopicData(\n            corpus=corpus,\n            document_term_matrix=document_term_matrix,\n            vocab=self.get_vocab(),\n            document_topic_matrix=doc_topic_matrix,\n            document_representation=embeddings,\n            topic_term_matrix=self.components_,  # type: ignore\n            transform=getattr(self, \"transform\", None),\n            topic_names=self.topic_names,\n            classes=classes,\n            temporal_components=self.temporal_components_,\n            temporal_importance=self.temporal_importance_,\n            time_bin_edges=self.time_bin_edges,\n        )\n        return res\n\n    def fit_transform_dynamic(\n        self,\n        raw_documents=None,\n        timestamps: Optional[list[datetime]] = None,\n        embeddings: Optional[np.ndarray] = None,\n        keywords: Optional[list[dict[str, float]]] = None,\n        bins: Union[int, list[datetime]] = 10,\n    ) -&gt; np.ndarray:\n        if self.seed_phrase is not None and keywords is not None:\n            warnings.warn(\n                \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n            )\n        if timestamps is None:\n            raise TypeError(\n                \"You have to pass timestamps when fitting a dynamic model.\"\n            )\n        if keywords is None and raw_documents is None:\n            raise ValueError(\n                \"You have to pass either keywords or raw_documents.\"\n            )\n        if keywords is None:\n            keywords = self.extract_keywords(\n                raw_documents, embeddings=embeddings\n            )\n        time_labels, self.time_bin_edges = self.bin_timestamps(\n            timestamps, bins\n        )\n        doc_topic_matrix = self.model.fit_transform_dynamic(\n            keywords, time_labels, self.time_bin_edges\n        )\n        self.temporal_importance_ = (\n            self.model.temporal_importance_.T\n            / self.model.temporal_importance_.sum(axis=1)\n        ).T\n        self.temporal_components_ = self.model.temporal_components\n        self.components_ = self.model.components\n        self.document_topic_matrix = doc_topic_matrix\n        self.document_term_matrix = self.model.vectorize(keywords)\n        self.hierarchy = DivisibleTopicNode.create_root(\n            self, self.components_, self.document_topic_matrix\n        )\n        self.top_documents = self.get_top_documents(\n            raw_documents, document_topic_matrix=doc_topic_matrix\n        )\n        return doc_topic_matrix\n\n    def partial_fit_dynamic(\n        self,\n        raw_documents=None,\n        timestamps: Optional[list[datetime]] = None,\n        embeddings: Optional[np.ndarray] = None,\n        keywords: Optional[list[dict[str, float]]] = None,\n        bins: Union[int, list[datetime]] = 10,\n    ):\n        \"\"\"Online fits Dynamic KeyNMF on a batch of documents.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n        keywords: list[dict[str, float]], optional\n            Precomputed keyword dictionaries.\n        timestamps: list[datetime], optional\n            List of timestamps for the batch.\n        bins: list[datetime]\n            Explicit time bin edges for the dynamic model.\n        \"\"\"\n        if self.seed_phrase is not None and keywords is not None:\n            warnings.warn(\n                \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n            )\n        if self.cross_lingual:\n            raise ValueError(\n                \"Cross-lingual online topic modeling is yet to be implemented in KeyNMF.\"\n            )\n        if timestamps is None:\n            raise TypeError(\n                \"You have to pass timestamps when fitting a dynamic model.\"\n            )\n        if keywords is None and raw_documents is None:\n            raise ValueError(\n                \"You have to pass either keywords or raw_documents.\"\n            )\n        time_bin_edges = getattr(self, \"time_bin_edges\", None)\n        if time_bin_edges is None:\n            if isinstance(bins, int):\n                raise TypeError(\n                    \"You have to pass explicit time bins (list of time bin edges) when partial \"\n                    \"fitting KeyNMF, at least at the first call.\"\n                )\n            else:\n                self.time_bin_edges = bins\n        time_labels, self.time_bin_edges = self.bin_timestamps(\n            timestamps, self.time_bin_edges\n        )\n        if keywords is None:\n            keywords = self.extract_keywords(\n                raw_documents, embeddings=embeddings\n            )\n        self.model.partial_fit_dynamic(\n            keywords, time_labels, self.time_bin_edges\n        )\n        self.temporal_importance_ = (\n            self.model.temporal_importance_.T\n            / self.model.temporal_importance_.sum(axis=1)\n        ).T\n        self.temporal_components_ = self.model.temporal_components\n        self.components_ = self.model.components\n        return self\n</code></pre>"},{"location":"KeyNMF/#turftopic.models.keynmf.KeyNMF.extract_keywords","title":"<code>extract_keywords(batch_or_document, embeddings=None, fitting=True)</code>","text":"<p>Extracts keywords from a document or a batch of documents.</p> <p>Parameters:</p> Name Type Description Default <code>batch_or_document</code> <code>Union[str, list[str]]</code> <p>A single document or a batch of documents.</p> required <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document embeddings.</p> <code>None</code> Source code in <code>turftopic/models/keynmf.py</code> <pre><code>def extract_keywords(\n    self,\n    batch_or_document: Union[str, list[str]],\n    embeddings: Optional[np.ndarray] = None,\n    fitting: bool = True,\n) -&gt; list[dict[str, float]]:\n    \"\"\"Extracts keywords from a document or a batch of documents.\n\n    Parameters\n    ----------\n    batch_or_document: str | list[str]\n        A single document or a batch of documents.\n    embeddings: ndarray, optional\n        Precomputed document embeddings.\n    \"\"\"\n    if isinstance(batch_or_document, str):\n        batch_or_document = [batch_or_document]\n    keywords = self.extractor.batch_extract_keywords(\n        batch_or_document,\n        embeddings=embeddings,\n        seed_embedding=self.seed_embedding,\n        seed_exponent=self.seed_exponent,\n        fitting=fitting,\n    )\n    if self.cross_lingual:\n        keywords = self.extractor.match_terms(\n            keywords, threshold=self.term_match_threshold\n        )\n    return keywords\n</code></pre>"},{"location":"KeyNMF/#turftopic.models.keynmf.KeyNMF.fit","title":"<code>fit(raw_documents=None, y=None, embeddings=None, keywords=None)</code>","text":"<p>Fits topic model and returns topic importances for documents.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> <code>None</code> <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <code>keywords</code> <code>Optional[list[dict[str, float]]]</code> <p>Precomputed keyword dictionaries.</p> <code>None</code> Source code in <code>turftopic/models/keynmf.py</code> <pre><code>def fit(\n    self,\n    raw_documents=None,\n    y=None,\n    embeddings: Optional[np.ndarray] = None,\n    keywords: Optional[list[dict[str, float]]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Fits topic model and returns topic importances for documents.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str, optional\n        Documents to fit the model on.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n    keywords: list[dict[str, float]], optional\n        Precomputed keyword dictionaries.\n    \"\"\"\n    self.fit_transform(\n        raw_documents, y, embeddings=embeddings, keywords=keywords\n    )\n    return self\n</code></pre>"},{"location":"KeyNMF/#turftopic.models.keynmf.KeyNMF.fit_transform","title":"<code>fit_transform(raw_documents=None, y=None, embeddings=None, keywords=None)</code>","text":"<p>Fits topic model and returns topic importances for documents.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> <code>None</code> <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <code>keywords</code> <code>Optional[list[dict[str, float]]]</code> <p>Precomputed keyword dictionaries.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_dimensions, n_topics)</code> <p>Document-topic matrix.</p> Source code in <code>turftopic/models/keynmf.py</code> <pre><code>def fit_transform(\n    self,\n    raw_documents=None,\n    y=None,\n    embeddings: Optional[np.ndarray] = None,\n    keywords: Optional[list[dict[str, float]]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Fits topic model and returns topic importances for documents.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str, optional\n        Documents to fit the model on.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n    keywords: list[dict[str, float]], optional\n        Precomputed keyword dictionaries.\n\n    Returns\n    -------\n    ndarray of shape (n_dimensions, n_topics)\n        Document-topic matrix.\n    \"\"\"\n    if self.seed_phrase is not None and keywords is not None:\n        warnings.warn(\n            \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n        )\n    console = Console()\n    with console.status(\"Running KeyNMF\") as status:\n        if keywords is None:\n            status.update(\"Extracting keywords\")\n            keywords = self.extract_keywords(\n                raw_documents, embeddings=embeddings\n            )\n            console.log(\"Keyword extraction done.\")\n        status.update(\"Decomposing with NMF\")\n        try:\n            doc_topic_matrix = self.model.transform(keywords)\n        except (NotFittedError, AttributeError):\n            doc_topic_matrix = self.model.fit_transform(keywords)\n            self.components_ = self.model.components\n        console.log(\"Model fitting done.\")\n    self.document_topic_matrix = doc_topic_matrix\n    self.document_term_matrix = self.model.vectorize(keywords)\n    self.hierarchy = DivisibleTopicNode.create_root(\n        self, self.components_, self.document_topic_matrix\n    )\n    self.top_documents = self.get_top_documents(\n        raw_documents, document_topic_matrix=doc_topic_matrix\n    )\n    return doc_topic_matrix\n</code></pre>"},{"location":"KeyNMF/#turftopic.models.keynmf.KeyNMF.partial_fit","title":"<code>partial_fit(raw_documents=None, embeddings=None, keywords=None)</code>","text":"<p>Online fits KeyNMF on a batch of documents.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <code>Optional[list[str]]</code> <p>Documents to fit the model on.</p> <code>None</code> <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <code>keywords</code> <code>Optional[list[dict[str, float]]]</code> <p>Precomputed keyword dictionaries.</p> <code>None</code> Source code in <code>turftopic/models/keynmf.py</code> <pre><code>def partial_fit(\n    self,\n    raw_documents: Optional[list[str]] = None,\n    embeddings: Optional[np.ndarray] = None,\n    keywords: Optional[list[dict[str, float]]] = None,\n):\n    \"\"\"Online fits KeyNMF on a batch of documents.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n    keywords: list[dict[str, float]], optional\n        Precomputed keyword dictionaries.\n    \"\"\"\n    if self.seed_phrase is not None and keywords is not None:\n        warnings.warn(\n            \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n        )\n    if self.cross_lingual:\n        raise ValueError(\n            \"Cross-lingual online topic modeling is yet to be implemented in KeyNMF.\"\n        )\n    if not self._has_custom_vectorizer:\n        self.vectorizer = CountVectorizer(stop_words=\"english\")\n        self._has_custom_vectorizer = True\n    min_df = self.vectorizer.min_df\n    max_df = self.vectorizer.max_df\n    if (min_df != 1) or (max_df != 1.0):\n        warnings.warn(\n            f\"\"\"When applying partial fitting, the vectorizer is fitted batch-wise in KeyNMF.\n        You have a vectorizer with min_df={min_df}, and max_df={max_df}.\n        If you continue with these settings, all tokens might get filtered out.\n        We recommend setting min_df=1 and max_df=1.0 for online fitting.\n        `model = KeyNMF(10, vectorizer=CountVectorizer(min_df=1, max_df=1.0)`\n        \"\"\"\n        )\n    if keywords is None and raw_documents is None:\n        raise ValueError(\n            \"You have to pass either keywords or raw_documents.\"\n        )\n    if keywords is None:\n        keywords = self.extract_keywords(\n            raw_documents, embeddings=embeddings\n        )\n    self.model.partial_fit(keywords)\n    self.components_ = self.model.components\n    return self\n</code></pre>"},{"location":"KeyNMF/#turftopic.models.keynmf.KeyNMF.partial_fit_dynamic","title":"<code>partial_fit_dynamic(raw_documents=None, timestamps=None, embeddings=None, keywords=None, bins=10)</code>","text":"<p>Online fits Dynamic KeyNMF on a batch of documents.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> <code>None</code> <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <code>keywords</code> <code>Optional[list[dict[str, float]]]</code> <p>Precomputed keyword dictionaries.</p> <code>None</code> <code>timestamps</code> <code>Optional[list[datetime]]</code> <p>List of timestamps for the batch.</p> <code>None</code> <code>bins</code> <code>Union[int, list[datetime]]</code> <p>Explicit time bin edges for the dynamic model.</p> <code>10</code> Source code in <code>turftopic/models/keynmf.py</code> <pre><code>def partial_fit_dynamic(\n    self,\n    raw_documents=None,\n    timestamps: Optional[list[datetime]] = None,\n    embeddings: Optional[np.ndarray] = None,\n    keywords: Optional[list[dict[str, float]]] = None,\n    bins: Union[int, list[datetime]] = 10,\n):\n    \"\"\"Online fits Dynamic KeyNMF on a batch of documents.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n    keywords: list[dict[str, float]], optional\n        Precomputed keyword dictionaries.\n    timestamps: list[datetime], optional\n        List of timestamps for the batch.\n    bins: list[datetime]\n        Explicit time bin edges for the dynamic model.\n    \"\"\"\n    if self.seed_phrase is not None and keywords is not None:\n        warnings.warn(\n            \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n        )\n    if self.cross_lingual:\n        raise ValueError(\n            \"Cross-lingual online topic modeling is yet to be implemented in KeyNMF.\"\n        )\n    if timestamps is None:\n        raise TypeError(\n            \"You have to pass timestamps when fitting a dynamic model.\"\n        )\n    if keywords is None and raw_documents is None:\n        raise ValueError(\n            \"You have to pass either keywords or raw_documents.\"\n        )\n    time_bin_edges = getattr(self, \"time_bin_edges\", None)\n    if time_bin_edges is None:\n        if isinstance(bins, int):\n            raise TypeError(\n                \"You have to pass explicit time bins (list of time bin edges) when partial \"\n                \"fitting KeyNMF, at least at the first call.\"\n            )\n        else:\n            self.time_bin_edges = bins\n    time_labels, self.time_bin_edges = self.bin_timestamps(\n        timestamps, self.time_bin_edges\n    )\n    if keywords is None:\n        keywords = self.extract_keywords(\n            raw_documents, embeddings=embeddings\n        )\n    self.model.partial_fit_dynamic(\n        keywords, time_labels, self.time_bin_edges\n    )\n    self.temporal_importance_ = (\n        self.model.temporal_importance_.T\n        / self.model.temporal_importance_.sum(axis=1)\n    ).T\n    self.temporal_components_ = self.model.temporal_components\n    self.components_ = self.model.components\n    return self\n</code></pre>"},{"location":"KeyNMF/#turftopic.models.keynmf.KeyNMF.transform","title":"<code>transform(raw_documents=None, embeddings=None, keywords=None)</code>","text":"<p>Infers topic importances for new documents based on a fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> <code>None</code> <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <code>keywords</code> <code>Optional[list[dict[str, float]]]</code> <p>Precomputed keyword dictionaries.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_dimensions, n_topics)</code> <p>Document-topic matrix.</p> Source code in <code>turftopic/models/keynmf.py</code> <pre><code>def transform(\n    self,\n    raw_documents=None,\n    embeddings: Optional[np.ndarray] = None,\n    keywords: Optional[list[dict[str, float]]] = None,\n) -&gt; np.ndarray:\n    \"\"\"Infers topic importances for new documents based on a fitted model.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n    keywords: list[dict[str, float]], optional\n        Precomputed keyword dictionaries.\n\n    Returns\n    -------\n    ndarray of shape (n_dimensions, n_topics)\n        Document-topic matrix.\n    \"\"\"\n    if self.seed_phrase is not None and keywords is not None:\n        warnings.warn(\n            \"Seed phrase is specified, but keyword matrix is pre-computed. The seed phrase will be ignored. Note that this is not a problem if you already calculated the keyword matrix using the seed phrase.\"\n        )\n    if keywords is None and raw_documents is None:\n        raise ValueError(\n            \"You have to pass either keywords or raw_documents.\"\n        )\n    if keywords is None:\n        keywords = self.extract_keywords(\n            list(raw_documents),\n            embeddings=embeddings,\n            fitting=False,\n        )\n    return self.model.transform(keywords)\n</code></pre>"},{"location":"KeyNMF/#turftopic.models.keynmf.KeyNMF.vectorize","title":"<code>vectorize(raw_documents=None, embeddings=None, keywords=None)</code>","text":"<p>Creates document-term-matrix from documents.</p> Source code in <code>turftopic/models/keynmf.py</code> <pre><code>def vectorize(\n    self,\n    raw_documents=None,\n    embeddings: Optional[np.ndarray] = None,\n    keywords: Optional[list[dict[str, float]]] = None,\n) -&gt; spr.csr_array:\n    \"\"\"Creates document-term-matrix from documents.\"\"\"\n    if keywords is None:\n        keywords = self.extract_keywords(\n            raw_documents, embeddings=embeddings\n        )\n    return self.model.vectorize(keywords)\n</code></pre>"},{"location":"SensTopic/","title":"SensTopic (BETA)","text":"<p>SensTopic is a version of Semantic Signal Separation, that only discovers positive signals, while allowing components to be unbounded. This is achieved with an algorithm called Semi-nonnegative Matrix Factorization or SNMF.</p> <p> This model is still in an experimental phase. More documentation and a paper are on their way. </p> <p>SensTopic uses a very efficient implementation of the SNMF algorithm, that is implemented in raw NumPy, but also in JAX. If you want to enable hardware acceleration and JIT compilation, make sure to install JAX before running the model.</p> <pre><code>pip install jax\n</code></pre> <p>SensTopic produces the same quality of topics as \\(S^3\\) without having to interpret negative topic descriptions, and also has better scaling properties. Our implementation of SNMF is considerably faster than FastICA, which \\(S^3\\) is based on.</p>  FastICA vs SNMF runtime on the 20 newsgroups dataset with different number of topics.  <p>Here's an example of running SensTopic on the 20 Newsgroups dataset:</p> <pre><code>from sklearn.datasets import fetch_20newsgroups\nfrom turftopic import SensTopic\n\ncorpus = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n).data\n\nmodel = SensTopic(25)\nmodel.fit(corpus)\n\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking ... 8 gospels, mormon, catholics, protestant, mormons, synagogues, seminary, catholic, liturgy, churches 9 encryption, encrypt, encrypting, crypt, cryptosystem, cryptography, cryptosystems, decryption, encrypted, spying 10 palestinians, israelis, palestinian, israeli, gaza, israel, gazans, palestine, zionist, aviv 11 nasa, spacecraft, spaceflight, satellites, interplanetary, astronomy, astronauts, astronomical, orbiting, astronomers 12 imagewriter, colormaps, bitmap, bitmaps, pkzip, imagemagick, colormap, formats, adobe, ghostscript ..."},{"location":"SensTopic/#sparsity","title":"Sparsity","text":"<p>SensTopic has a sparsity hyper-parameter, that roughly dictates how many documents will be assigned to a single document, where many topics per document get penalized. This means that the model is both a matrix factorization model, but can also function as a soft clustering model, depending on this parameter. Unlike clustering models, however, it may assign multiple topics to documents that have them, and won't force every document to contain only one topic.</p> <p>Higher values will make your model more like a clustering model, while lower values will make it more like a decomposition model:</p> Click to see code <pre><code>import pandas as pd\nimport numpy as np\nimport plotly.express as px\nfrom sentence_transformers import SentenceTransformer\nfrom datasets import load_dataset\n\nfrom turftopic import SensTopic\n\nds = load_dataset(\"gopalkalpande/bbc-news-summary\", split=\"train\")\ncorpus = list(ds[\"Summaries\"])\n\nencoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\nembeddings = encoder.encode(corpus, show_progress_bar=True)\n\nmodels = []\ndoc_topic_ms = []\nsparsities = np.array(\n    [\n        0.05,\n        0.1,\n        0.25,\n        0.5,\n        0.75,\n        1.0,\n        2.5,\n        5.0,\n        10.0,\n    ]\n)\nfor i, sparsity in enumerate(sparsities):\n    model = SensTopic(\n        n_components=3, random_state=42, sparsity=sparsity, encoder=encoder\n    )\n    doc_topic = model.fit_transform(corpus, embeddings=embeddings)\n    doc_topic = (doc_topic.T / doc_topic.sum(axis=1)).T\n    models.append(model)\n    doc_topic_ms.append(doc_topic)\na_name, b_name, c_name = models[0].topic_names\nrecords = []\nfor i, doc_topic in enumerate(doc_topic_ms):\n    for dt in doc_topic:\n        a, b, c, *_ = dt\n        records.append(\n            {\n                \"sparsity\": sparsities[i],\n                a_name: a,\n                b_name: b,\n                c_name: c,\n                \"topic\": models[0].topic_names[np.argmax(dt)],\n            }\n        )\ndf = pd.DataFrame.from_records(records)\nfig = px.scatter_ternary(\n    df, a=a_name, b=b_name, c=c_name, animation_frame=\"sparsity\", color=\"topic\"\n)\nfig.show()\n</code></pre>  Ternary plot of topic distribution in a 3 topic SensTopic model varying with sparsity.  <p>You can see that as the sparsity increases, topics get clustered much more clearly, and more weight gets allocated to the edges of the graph.</p> <p>To see how many topics there are in your document you can use the <code>plot_topic_decay()</code> method, that shows you how topic weights get assigned to documents.</p> <pre><code>model.plot_topic_decay()\n</code></pre>  Topic Decay in a SensTopic Model with sparsity=1."},{"location":"SensTopic/#automatic-number-of-topics","title":"Automatic number of topics","text":"<p>SensTopic can learn the number of topics in a given dataset. In order to determine this quantity, we use a version of the Bayesian Information Criterion modified for NMF. This does not work equally well for all corpora, but it can be a powerful tool when the number of topics is not known a-priori.</p> <p>In this example the model finds 6 topics in the BBC News dataset:</p> <pre><code># pip install datasets\nfrom datasets import load_dataset\n\nds = load_dataset(\"gopalkalpande/bbc-news-summary\", split=\"train\")\ncorpus = list(ds[\"Summaries\"])\n\nmodel = SensTopic(\"auto\")\nmodel.fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking 0 liverpool, mourinho, chelsea, premiership, arsenal, striker, madrid, midfield, uefa, manchester 1 oscar, bafta, oscars, cast, cinema, hollywood, actor, screenplay, actors, films 2 mobile, mobiles, broadband, devices, digital, internet, computers, microsoft, phones, telecoms 3 tory, blair, minister, ministers, parliamentary, mps, parliament, politicians, constituency, ukip 4 tennis, competing, federer, wimbledon, iaaf, olympic, tournament, athlete, rugby, olympics 5 gdp, stock, economy, earnings, investments, investment, invest, exports, finance, economies"},{"location":"SensTopic/#api-reference","title":"API Reference","text":""},{"location":"SensTopic/#turftopic.models.senstopic.SensTopic","title":"<code>turftopic.models.senstopic.SensTopic</code>","text":"<p>             Bases: <code>ContextualModel</code>, <code>DynamicTopicModel</code>, <code>MultimodalModel</code></p> <p>Semi-nonnegative Semantic Signal Separation.</p> <pre><code>from turftopic import SensTopic\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\nmodel = SensTopic(10).fit(corpus)\nmodel.print_topics()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>Union[int, Literal['auto']]</code> <p>Number of topics. If \"auto\", the number of topics is determined using BIC.</p> <code>'auto'</code> <code>encoder</code> <code>Union[Encoder, str, MultimodalEncoder]</code> <p>Model to encode documents/terms, all-MiniLM-L6-v2 is the default.</p> <code>'sentence-transformers/all-MiniLM-L6-v2'</code> <code>vectorizer</code> <code>Optional[CountVectorizer]</code> <p>Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.</p> <code>None</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations for S-NMF.</p> <code>200</code> <code>feature_importance</code> <code>Literal['axial', 'angular', 'combined']</code> <p>Defines whether the word's position on an axis ('axial'), it's angle to the axis ('angular') or their combination ('combined') should determine the word's importance for a topic.</p> <code>'combined'</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state to use so that results are exactly reproducible.</p> <code>None</code> <code>sparsity</code> <code>float</code> <p>L1 penalty applied to document-topic proportions. Higher values push the model to assign fewer topics to a single document, while lower values will distribute topics across documents.</p> <code>1</code> Source code in <code>turftopic/models/senstopic.py</code> <pre><code>class SensTopic(ContextualModel, DynamicTopicModel, MultimodalModel):\n    \"\"\"Semi-nonnegative Semantic Signal Separation.\n\n    ```python\n    from turftopic import SensTopic\n\n    corpus: list[str] = [\"some text\", \"more text\", ...]\n\n    model = SensTopic(10).fit(corpus)\n    model.print_topics()\n    ```\n\n    Parameters\n    ----------\n    n_components: int, default \"auto\"\n        Number of topics.\n        If \"auto\", the number of topics is determined using BIC.\n    encoder: str or SentenceTransformer\n        Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n    vectorizer: CountVectorizer, default None\n        Vectorizer used for term extraction.\n        Can be used to prune or filter the vocabulary.\n    max_iter: int, default 200\n        Maximum number of iterations for S-NMF.\n    feature_importance: \"axial\", \"angular\" or \"combined\", default \"combined\"\n        Defines whether the word's position on an axis ('axial'), it's angle to the axis ('angular')\n        or their combination ('combined') should determine the word's importance for a topic.\n    random_state: int, default None\n        Random state to use so that results are exactly reproducible.\n    sparsity: float, default 1\n        L1 penalty applied to document-topic proportions.\n        Higher values push the model to assign fewer topics to a single document,\n        while lower values will distribute topics across documents.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: Union[int, Literal[\"auto\"]] = \"auto\",\n        encoder: Union[\n            Encoder, str, MultimodalEncoder\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        max_iter: int = 200,\n        feature_importance: Literal[\n            \"axial\", \"angular\", \"combined\"\n        ] = \"combined\",\n        random_state: Optional[int] = None,\n        sparsity: float = 1,\n    ):\n        self.n_components = n_components\n        self.encoder = encoder\n        self.feature_importance = feature_importance\n        if isinstance(encoder, str):\n            self.encoder_ = SentenceTransformer(encoder)\n        else:\n            self.encoder_ = encoder\n        self.validate_encoder()\n        if vectorizer is None:\n            self.vectorizer = default_vectorizer()\n        else:\n            self.vectorizer = vectorizer\n        self.max_iter = max_iter\n        self.random_state = random_state\n        self.sparsity = sparsity\n\n    def estimate_components(\n        self, feature_importance: Literal[\"axial\", \"angular\", \"combined\"]\n    ) -&gt; np.ndarray:\n        \"\"\"Reestimates components based on the chosen feature_importance method.\"\"\"\n        if feature_importance == \"axial\":\n            self.components_ = self.axial_components_\n        elif feature_importance == \"angular\":\n            self.components_ = self.angular_components_\n        elif feature_importance == \"combined\":\n            self.components_ = (\n                np.square(self.axial_components_) * self.angular_components_\n            )\n        if hasattr(self, \"axial_temporal_components_\"):\n            if feature_importance == \"axial\":\n                self.temporal_components_ = self.axial_temporal_components_\n            elif feature_importance == \"angular\":\n                self.temporal_components_ = self.angular_temporal_components_\n            elif feature_importance == \"combined\":\n                self.temporal_components_ = (\n                    np.square(self.axial_temporal_components_)\n                    * self.angular_temporal_components_\n                )\n        return self.components_\n\n    def fit_transform(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        console = Console()\n        self.embeddings = embeddings\n        with console.status(\"Fitting model\") as status:\n            if self.embeddings is None:\n                status.update(\"Encoding documents\")\n                self.embeddings = self.encoder_.encode(raw_documents)\n                console.log(\"Documents encoded.\")\n            if self.n_components == \"auto\":\n                status.update(\"Finding the number of components.\")\n                self.n_components_ = optimize_n_components(\n                    partial(\n                        bic_snmf, X=self.embeddings, sparsity=self.sparsity\n                    ),\n                    min_n=1,\n                    verbose=True,\n                )\n                console.log(\"N components set at: \" + str(self.n_components_))\n            else:\n                self.n_components_ = self.n_components\n            self.decomposition = SNMF(\n                self.n_components_,\n                max_iter=self.max_iter,\n                sparsity=self.sparsity,\n                random_state=self.random_state,\n            )\n            status.update(\"Decomposing embeddings\")\n            doc_topic = self.decomposition.fit_transform(self.embeddings, y=y)\n            console.log(\"Decomposition done.\")\n            status.update(\"Extracting terms.\")\n            vocab = self.vectorizer.fit(raw_documents).get_feature_names_out()\n            console.log(\"Term extraction done.\")\n            if getattr(self, \"vocab_embeddings\", None) is None:\n                status.update(\"Encoding vocabulary\")\n                self.vocab_embeddings = self.encoder_.encode(vocab)\n            if self.vocab_embeddings.shape[1] != self.embeddings.shape[1]:\n                raise ValueError(\n                    NOT_MATCHING_ERROR.format(\n                        n_dims=self.embeddings.shape[1],\n                        n_word_dims=self.vocab_embeddings.shape[1],\n                    )\n                )\n            console.log(\"Vocabulary encoded.\")\n            status.update(\"Estimating term importances\")\n            vocab_topic = self.decomposition.transform(self.vocab_embeddings)\n            self.axial_components_ = vocab_topic.T\n            if self.feature_importance == \"axial\":\n                self.components_ = self.axial_components_\n            elif self.feature_importance == \"angular\":\n                self.components_ = self.angular_components_\n            elif self.feature_importance == \"combined\":\n                self.components_ = (\n                    np.square(self.axial_components_)\n                    * self.angular_components_\n                )\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=doc_topic\n            )\n            self.document_topic_matrix = doc_topic\n            console.log(\"Model fitting done.\")\n        return doc_topic\n\n    def transform(self, raw_documents, embeddings=None):\n        if embeddings is None:\n            embeddings = self.encoder_.encode(raw_documents)\n        return self.decomposition.transform(embeddings)\n\n    def fit_transform_multimodal(\n        self,\n        raw_documents: list[str],\n        images: list[ImageRepr],\n        y=None,\n        embeddings: Optional[MultimodalEmbeddings] = None,\n    ) -&gt; np.ndarray:\n        self.validate_embeddings(embeddings)\n        console = Console()\n        self.images = images\n        self.multimodal_embeddings = embeddings\n        with console.status(\"Fitting model\") as status:\n            if self.multimodal_embeddings is None:\n                status.update(\"Encoding documents\")\n                self.multimodal_embeddings = self.encode_multimodal(\n                    raw_documents, images\n                )\n                console.log(\"Documents encoded.\")\n            self.embeddings = self.multimodal_embeddings[\"document_embeddings\"]\n            if self.n_components == \"auto\":\n                status.update(\"Finding the number of components.\")\n                self.n_components_ = optimize_n_components(\n                    partial(\n                        bic_snmf, X=self.embeddings, sparsity=self.sparsity\n                    ),\n                    min_n=1,\n                    verbose=True,\n                )\n                console.log(\"N components set at: \" + str(self.n_components_))\n            else:\n                self.n_components_ = self.n_components\n            self.decomposition = SNMF(\n                self.n_components_,\n                max_iter=self.max_iter,\n                sparsity=self.sparsity,\n                random_state=self.random_state,\n            )\n            status.update(\"Decomposing embeddings\")\n            doc_topic = self.decomposition.fit_transform(self.embeddings, y=y)\n            console.log(\"Decomposition done.\")\n            status.update(\"Extracting terms.\")\n            vocab = self.vectorizer.fit(raw_documents).get_feature_names_out()\n            console.log(\"Term extraction done.\")\n            status.update(\"Encoding vocabulary\")\n            self.vocab_embeddings = self.encode_documents(vocab)\n            if self.vocab_embeddings.shape[1] != self.embeddings.shape[1]:\n                raise ValueError(\n                    NOT_MATCHING_ERROR.format(\n                        n_dims=self.embeddings.shape[1],\n                        n_word_dims=self.vocab_embeddings.shape[1],\n                    )\n                )\n            console.log(\"Vocabulary encoded.\")\n            status.update(\"Estimating term importances\")\n            vocab_topic = self.decomposition.transform(self.vocab_embeddings)\n            self.axial_components_ = vocab_topic.T\n            if self.feature_importance == \"axial\":\n                self.components_ = self.axial_components_\n            elif self.feature_importance == \"angular\":\n                self.components_ = self.angular_components_\n            elif self.feature_importance == \"combined\":\n                self.components_ = (\n                    np.square(self.axial_components_)\n                    * self.angular_components_\n                )\n            console.log(\"Model fitting done.\")\n            status.update(\"Transforming images\")\n            self.image_topic_matrix = self.transform(\n                [], embeddings=self.multimodal_embeddings[\"image_embeddings\"]\n            )\n            self.top_images = self.collect_top_images(\n                images, self.image_topic_matrix\n            )\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=doc_topic\n            )\n            console.log(\"Images transformed\")\n        return doc_topic\n\n    def fit_transform_dynamic(\n        self,\n        raw_documents,\n        timestamps: list[datetime],\n        embeddings: Optional[np.ndarray] = None,\n        bins: Union[int, list[datetime]] = 10,\n    ) -&gt; np.ndarray:\n        document_topic_matrix = self.fit_transform(\n            raw_documents, embeddings=embeddings\n        )\n        time_labels, self.time_bin_edges = self.bin_timestamps(\n            timestamps, bins\n        )\n        n_comp, n_vocab = self.components_.shape\n        n_bins = len(self.time_bin_edges) - 1\n        self.axial_temporal_components_ = np.full(\n            (n_bins, n_comp, n_vocab),\n            np.nan,\n            dtype=self.components_.dtype,\n        )\n        self.temporal_importance_ = np.zeros((n_bins, n_comp))\n        # doc_topic = np.dot(X, self.components_.T)\n        for i_timebin in np.unique(time_labels):\n            topic_importances = document_topic_matrix[\n                time_labels == i_timebin\n            ].mean(axis=0)\n            self.temporal_importance_[i_timebin, :] = topic_importances\n            t_doc_topic = document_topic_matrix[time_labels == i_timebin]\n            t_embeddings = self.embeddings[time_labels == i_timebin]\n            t_components = self.decomposition.fit_timeslice(\n                t_embeddings, t_doc_topic\n            )\n            ax_t = np.maximum(\n                self.vocab_embeddings @ np.linalg.pinv(t_components), 0\n            )\n            self.axial_temporal_components_[i_timebin, :, :] = ax_t.T\n        self.estimate_components(self.feature_importance)\n        return document_topic_matrix\n\n    @property\n    def angular_components_(self):\n        \"\"\"Reweights words based on their angle in ICA-space to the axis\n        base vectors.\n        \"\"\"\n        if not hasattr(self, \"axial_components_\"):\n            raise NotFittedError(\"Model has not been fitted yet.\")\n        word_vectors = self.axial_components_.T\n        n_topics = self.axial_components_.shape[0]\n        axis_vectors = np.eye(n_topics)\n        cosine_components = cosine_similarity(axis_vectors, word_vectors)\n        return cosine_components\n\n    @property\n    def angular_temporal_components_(self):\n        \"\"\"Reweights words based on their angle in ICA-space to the axis\n        base vectors in a dynamic model.\n        \"\"\"\n        if not hasattr(self, \"axial_temporal_components_\"):\n            raise NotFittedError(\"Model has not been fitted dynamically.\")\n        components = []\n        for axial_components in self.axial_temporal_components_:\n            word_vectors = axial_components.T\n            n_topics = axial_components.shape[0]\n            axis_vectors = np.eye(n_topics)\n            cosine_components = cosine_similarity(axis_vectors, word_vectors)\n            components.append(cosine_components)\n        return np.stack(components)\n\n    def plot_topic_decay(self):\n        try:\n            import plotly.graph_objects as go\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n        doc_topic = self.document_topic_matrix\n        topic_proportions = []\n        for dt in doc_topic:\n            sum_dt = dt.sum()\n            if sum_dt &gt; 0:\n                dt /= sum_dt\n            dt = -np.sort(-dt)\n            topic_proportions.append(dt)\n        topic_proportions = np.stack(topic_proportions)\n        med_prop = np.median(topic_proportions, axis=0)\n        upper = np.quantile(topic_proportions, 0.975, axis=0)\n        lower = np.quantile(topic_proportions, 0.025, axis=0)\n        fig = go.Figure(\n            [\n                go.Scatter(\n                    name=\"Median\",\n                    x=np.arange(self.n_components_),\n                    y=med_prop,\n                    mode=\"lines\",\n                    line=dict(color=\"rgb(31, 119, 180)\"),\n                ),\n                go.Scatter(\n                    name=\"Upper Bound\",\n                    x=np.arange(self.n_components_),\n                    y=upper,\n                    mode=\"lines\",\n                    marker=dict(color=\"#444\"),\n                    line=dict(width=0),\n                    showlegend=False,\n                ),\n                go.Scatter(\n                    name=\"Lower Bound\",\n                    x=np.arange(self.n_components_),\n                    y=lower,\n                    marker=dict(color=\"#444\"),\n                    line=dict(width=0),\n                    mode=\"lines\",\n                    fillcolor=\"rgba(68, 68, 68, 0.3)\",\n                    fill=\"tonexty\",\n                    showlegend=False,\n                ),\n            ]\n        )\n        fig = fig.update_layout(\n            template=\"plotly_white\",\n            xaxis_title=\"Topic Rank\",\n            yaxis_title=\"Topic Proportion\",\n            title=\"Topic Decay\",\n            font=dict(family=\"Merriweather\", size=16),\n        )\n        return fig\n\n    def plot_components(\n        self, hover_text: Optional[list[str]] = None, **kwargs\n    ):\n        \"\"\"Creates an interactive browser plot of the topics in your data using plotly.\n\n        Parameters\n        ----------\n        hover_text: list of str, optional\n            Text to show when hovering over a document.\n\n        Returns\n        -------\n        plot\n            Interactive datamap plot, you can call the `.show()` method to\n            display it in your default browser or save it as static HTML using `.write_html()`.\n        \"\"\"\n        doc_topic = self.document_topic_matrix\n        coords = TSNE(2, metric=\"cosine\").fit_transform(doc_topic)\n        labels = np.argmax(doc_topic, axis=1)\n        print(np.unique_counts(labels))\n        topics_present = np.sort(np.unique(labels))\n        names = [self.topic_names[i] for i in topics_present]\n        if getattr(self, \"topic_descriptions\", None) is not None:\n            desc = [self.topic_descriptions[i] for i in topics_present]\n        else:\n            desc = None\n        all_words = self.get_top_words()\n        keywords = [all_words[i] for i in topics_present]\n        fig = build_datamapplot(\n            coords,\n            labels=labels,\n            topic_names=names,\n            top_words=keywords,\n            hover_text=hover_text,\n            topic_descriptions=desc,\n            classes=topics_present,\n            # Boundaries are unlikely to be very clear\n            cluster_boundary_polygons=False,\n        )\n        return fig\n\n    def plot_components_datamapplot(\n        self, hover_text: Optional[list[str]] = None, **kwargs\n    ):\n        \"\"\"Creates an interactive browser plot of the topics in your data using datamapplot.\n\n        Parameters\n        ----------\n        hover_text: list of str, optional\n            Text to show when hovering over a document.\n\n        Returns\n        -------\n        plot\n            Interactive datamap plot, you can call the `.show()` method to\n            display it in your default browser or save it as static HTML using `.write_html()`.\n        \"\"\"\n        doc_topic = self.document_topic_matrix\n        coords = TSNE(2, metric=\"cosine\").fit_transform(doc_topic)\n        labels = np.argmax(doc_topic, axis=1)\n        print(np.unique_counts(labels))\n        topics_present = np.sort(np.unique(labels))\n        names = [self.topic_names[i] for i in topics_present]\n        if getattr(self, \"topic_descriptions\", None) is not None:\n            desc = [self.topic_descriptions[i] for i in topics_present]\n        else:\n            desc = None\n        all_words = self.get_top_words()\n        keywords = [all_words[i] for i in topics_present]\n        fig = build_datamapplot(\n            coords,\n            labels=labels,\n            topic_names=names,\n            top_words=keywords,\n            hover_text=hover_text,\n            topic_descriptions=desc,\n            classes=topics_present,\n            # Boundaries are unlikely to be very clear\n            cluster_boundary_polygons=False,\n        )\n        return fig\n</code></pre>"},{"location":"SensTopic/#turftopic.models.senstopic.SensTopic.angular_components_","title":"<code>angular_components_</code>  <code>property</code>","text":"<p>Reweights words based on their angle in ICA-space to the axis base vectors.</p>"},{"location":"SensTopic/#turftopic.models.senstopic.SensTopic.angular_temporal_components_","title":"<code>angular_temporal_components_</code>  <code>property</code>","text":"<p>Reweights words based on their angle in ICA-space to the axis base vectors in a dynamic model.</p>"},{"location":"SensTopic/#turftopic.models.senstopic.SensTopic.estimate_components","title":"<code>estimate_components(feature_importance)</code>","text":"<p>Reestimates components based on the chosen feature_importance method.</p> Source code in <code>turftopic/models/senstopic.py</code> <pre><code>def estimate_components(\n    self, feature_importance: Literal[\"axial\", \"angular\", \"combined\"]\n) -&gt; np.ndarray:\n    \"\"\"Reestimates components based on the chosen feature_importance method.\"\"\"\n    if feature_importance == \"axial\":\n        self.components_ = self.axial_components_\n    elif feature_importance == \"angular\":\n        self.components_ = self.angular_components_\n    elif feature_importance == \"combined\":\n        self.components_ = (\n            np.square(self.axial_components_) * self.angular_components_\n        )\n    if hasattr(self, \"axial_temporal_components_\"):\n        if feature_importance == \"axial\":\n            self.temporal_components_ = self.axial_temporal_components_\n        elif feature_importance == \"angular\":\n            self.temporal_components_ = self.angular_temporal_components_\n        elif feature_importance == \"combined\":\n            self.temporal_components_ = (\n                np.square(self.axial_temporal_components_)\n                * self.angular_temporal_components_\n            )\n    return self.components_\n</code></pre>"},{"location":"SensTopic/#turftopic.models.senstopic.SensTopic.plot_components","title":"<code>plot_components(hover_text=None, **kwargs)</code>","text":"<p>Creates an interactive browser plot of the topics in your data using plotly.</p> <p>Parameters:</p> Name Type Description Default <code>hover_text</code> <code>Optional[list[str]]</code> <p>Text to show when hovering over a document.</p> <code>None</code> <p>Returns:</p> Type Description <code>plot</code> <p>Interactive datamap plot, you can call the <code>.show()</code> method to display it in your default browser or save it as static HTML using <code>.write_html()</code>.</p> Source code in <code>turftopic/models/senstopic.py</code> <pre><code>def plot_components(\n    self, hover_text: Optional[list[str]] = None, **kwargs\n):\n    \"\"\"Creates an interactive browser plot of the topics in your data using plotly.\n\n    Parameters\n    ----------\n    hover_text: list of str, optional\n        Text to show when hovering over a document.\n\n    Returns\n    -------\n    plot\n        Interactive datamap plot, you can call the `.show()` method to\n        display it in your default browser or save it as static HTML using `.write_html()`.\n    \"\"\"\n    doc_topic = self.document_topic_matrix\n    coords = TSNE(2, metric=\"cosine\").fit_transform(doc_topic)\n    labels = np.argmax(doc_topic, axis=1)\n    print(np.unique_counts(labels))\n    topics_present = np.sort(np.unique(labels))\n    names = [self.topic_names[i] for i in topics_present]\n    if getattr(self, \"topic_descriptions\", None) is not None:\n        desc = [self.topic_descriptions[i] for i in topics_present]\n    else:\n        desc = None\n    all_words = self.get_top_words()\n    keywords = [all_words[i] for i in topics_present]\n    fig = build_datamapplot(\n        coords,\n        labels=labels,\n        topic_names=names,\n        top_words=keywords,\n        hover_text=hover_text,\n        topic_descriptions=desc,\n        classes=topics_present,\n        # Boundaries are unlikely to be very clear\n        cluster_boundary_polygons=False,\n    )\n    return fig\n</code></pre>"},{"location":"SensTopic/#turftopic.models.senstopic.SensTopic.plot_components_datamapplot","title":"<code>plot_components_datamapplot(hover_text=None, **kwargs)</code>","text":"<p>Creates an interactive browser plot of the topics in your data using datamapplot.</p> <p>Parameters:</p> Name Type Description Default <code>hover_text</code> <code>Optional[list[str]]</code> <p>Text to show when hovering over a document.</p> <code>None</code> <p>Returns:</p> Type Description <code>plot</code> <p>Interactive datamap plot, you can call the <code>.show()</code> method to display it in your default browser or save it as static HTML using <code>.write_html()</code>.</p> Source code in <code>turftopic/models/senstopic.py</code> <pre><code>def plot_components_datamapplot(\n    self, hover_text: Optional[list[str]] = None, **kwargs\n):\n    \"\"\"Creates an interactive browser plot of the topics in your data using datamapplot.\n\n    Parameters\n    ----------\n    hover_text: list of str, optional\n        Text to show when hovering over a document.\n\n    Returns\n    -------\n    plot\n        Interactive datamap plot, you can call the `.show()` method to\n        display it in your default browser or save it as static HTML using `.write_html()`.\n    \"\"\"\n    doc_topic = self.document_topic_matrix\n    coords = TSNE(2, metric=\"cosine\").fit_transform(doc_topic)\n    labels = np.argmax(doc_topic, axis=1)\n    print(np.unique_counts(labels))\n    topics_present = np.sort(np.unique(labels))\n    names = [self.topic_names[i] for i in topics_present]\n    if getattr(self, \"topic_descriptions\", None) is not None:\n        desc = [self.topic_descriptions[i] for i in topics_present]\n    else:\n        desc = None\n    all_words = self.get_top_words()\n    keywords = [all_words[i] for i in topics_present]\n    fig = build_datamapplot(\n        coords,\n        labels=labels,\n        topic_names=names,\n        top_words=keywords,\n        hover_text=hover_text,\n        topic_descriptions=desc,\n        classes=topics_present,\n        # Boundaries are unlikely to be very clear\n        cluster_boundary_polygons=False,\n    )\n    return fig\n</code></pre>"},{"location":"Topeax/","title":"Topeax","text":"<p>Topeax is a probabilistic topic model based on the Peax clustering model, which finds topics based on peaks in point density in the embedding space. The model can recover the number of topics automatically.</p> <p>In the following example I run a Topeax model on the BBC News corpus, and plot the steps of the algorithm to inspect how our documents have been clustered and why:</p> <pre><code># pip install datasets, plotly\nfrom datasets import load_dataset\nfrom turftopic import Topeax\n\nds = load_dataset(\"gopalkalpande/bbc-news-summary\", split=\"train\")\ntopeax = Topeax(random_state=42)\ndoc_topic = topeax.fit_transform(list(ds[\"Summaries\"]))\n\nfig = topeax.plot_steps(hover_text=[text[:200] for text in corpus])\nfig.show()\n</code></pre>  Figure 1: Steps in a Topeax model fitted on BBC News displayed on an interactive graph.  <pre><code>topeax.print_topics()\n</code></pre> Topic ID Highest Ranking 0 mobile, microsoft, digital, technology, broadband, phones, devices, internet, mobiles, computer 1 economy, growth, economic, deficit, prices, gdp, inflation, currency, rates, exports 2 profits, shareholders, shares, takeover, shareholder, company, profit, merger, investors, financial 3 film, actor, oscar, films, actress, oscars, bafta, movie, awards, actors 4 band, album, song, singer, concert, rock, songs, rapper, rap, grammy 5 tory, blair, labour, ukip, mps, minister, election, tories, mr, ministers 6 olympic, tennis, iaaf, federer, wimbledon, doping, roddick, champion, athletics, olympics 7 rugby, liverpool, england, mourinho, chelsea, premiership, arsenal, gerrard, hodgson, gareth"},{"location":"Topeax/#how-does-topeax-work","title":"How does Topeax work?","text":"<p>The Topeax algorithm, similar to clustering topic models consists of two consecutive steps. One of them discovers the underlying clusters in the data, the other one estimates term importance scores for each topic in the corpus.</p> <p></p> Figure 2: Schematic overview of the steps of the Peax clustering algorithm"},{"location":"Topeax/#1-clustering","title":"1. Clustering","text":"<p>Documents embeddings first get projected into two-dimensional space using t-SNE. In order to identify clusters, we first calculate a Kernel Density Estimate over the embedding space, then find local maxima in the KDE by grid approximation. When we discover local maxima (peaks), we assume these to be cluster means. Cluster density is then approximated with a Gaussian Mixture, where we fix means to the density peaks and then use expectation-maximization to fit the rest of the parameters. (see Figure 2) Documents are then assigned to the component with the highest responsibility:</p> \\[\\hat{z_d} = arg max_k (r_{kd}); r_{kd}=p(z_k=1 | \\hat{x}_d)\\] <p>where \\(z_d\\) is the cluster label for document \\(d\\), \\(r_{kd}\\) is the responsibility of component \\(k\\) for document \\(d\\) and \\(\\hat{x}_d\\) is the 2D embedding of document \\(d\\).</p>"},{"location":"Topeax/#2-term-importance-estimation","title":"2. Term Importance Estimation","text":"<p>Topeax uses a combined semantic-lexical term importance, which is the geometric mean of the NPMI method (see Clustering Topic Models for more detail) and a slightly modified centroid-based method. The modified centroids are calculated like so:</p> \\[t_k = \\frac{\\sum_d r_{kd} \\cdot x_d}{\\sum_d r_{kd}}\\] <p>where \\(t_k\\) is the embedding of topic \\(k\\) and \\(x_d\\) is the embedding of document \\(d\\).</p>"},{"location":"Topeax/#visualization","title":"Visualization","text":"<p>Topeax has a number of plots available that can aid you when interpreting your results:</p>"},{"location":"Topeax/#density-plots","title":"Density Plots","text":"<p>One can plot the kernel density estimate on both a 2D and a 3D plot.</p> <pre><code>topeax.plot_density()\n</code></pre>  Figure 2: Density contour plot of the Topeax model.  <pre><code>topeax.plot_density3d()\n</code></pre>  Figure 3: 3D Density Surface of the Topeax model."},{"location":"Topeax/#component-plots","title":"Component Plots","text":"<p>You can also create a plot over the mixture components/clusters found by the model.</p> <pre><code>topeax.plot_components()\n</code></pre>  Figure 4: Gaussian components estimated for the model.  <p>You can also create a datamapplot figure similar to clustering models:</p> <pre><code># pip install turftopic[datamapplot]\ntopeax.plot_components_datamapplot()\n</code></pre>  Figure 5: Datapoints colored by mixture components on a datamapplot."},{"location":"Topeax/#api-reference","title":"API Reference","text":""},{"location":"Topeax/#turftopic.models.topeax.Topeax","title":"<code>turftopic.models.topeax.Topeax</code>","text":"<p>             Bases: <code>GMM</code></p> <p>Topic model based on the Peax clustering algorithm. The algorithm discovers the number of topics automatically, and is based on GMM.</p> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Union[Encoder, str, MultimodalEncoder]</code> <p>Model to encode documents/terms, all-MiniLM-L6-v2 is the default.</p> <code>'sentence-transformers/all-MiniLM-L6-v2'</code> <code>vectorizer</code> <code>Optional[CountVectorizer]</code> <p>Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.</p> <code>None</code> <code>perplexity</code> <code>int</code> <p>Number of neighbours to take into account when running TSNE.</p> <code>50</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state to use so that results are exactly reproducible.</p> <code>None</code> Source code in <code>turftopic/models/topeax.py</code> <pre><code>class Topeax(GMM):\n    \"\"\"Topic model based on the Peax clustering algorithm.\n    The algorithm discovers the number of topics automatically, and is based on GMM.\n\n    Parameters\n    ----------\n    encoder: str or SentenceTransformer\n        Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n    vectorizer: CountVectorizer, default None\n        Vectorizer used for term extraction.\n        Can be used to prune or filter the vocabulary.\n    perplexity: int, default 50\n        Number of neighbours to take into account when running TSNE.\n    random_state: int, default None\n        Random state to use so that results are exactly reproducible.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: Union[\n            Encoder, str, MultimodalEncoder\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        perplexity: int = 50,\n        random_state: Optional[int] = None,\n    ):\n        dimensionality_reduction = TSNE(\n            2,\n            metric=\"cosine\",\n            perplexity=perplexity,\n            random_state=random_state,\n        )\n        self.perplexity = perplexity\n        super().__init__(\n            n_components=0,\n            encoder=encoder,\n            vectorizer=vectorizer,\n            dimensionality_reduction=dimensionality_reduction,\n            random_state=random_state,\n        )\n\n    def estimate_components(\n        self,\n        feature_importance: Optional[LexicalWordImportance] = None,\n        doc_topic_matrix=None,\n        doc_term_matrix=None,\n    ) -&gt; np.ndarray:\n        doc_topic_matrix = (\n            doc_topic_matrix\n            if doc_topic_matrix is not None\n            else self.doc_topic_matrix\n        )\n        doc_term_matrix = (\n            doc_term_matrix\n            if doc_term_matrix is not None\n            else self.doc_term_matrix\n        )\n        lexical_components = super().estimate_components(\n            \"npmi\", doc_topic_matrix, doc_term_matrix\n        )\n        vocab = self.get_vocab()\n        if getattr(self, \"vocab_embeddings\", None) is None or (\n            self.vocab_embeddings.shape[0] != vocab.shape[0]\n        ):\n            self.vocab_embeddings = self.encode_documents(vocab)\n        topic_embeddings = []\n        for weight in doc_topic_matrix.T:\n            topic_embeddings.append(\n                np.average(self.embeddings, axis=0, weights=weight)\n            )\n        self.topic_embeddings = np.stack(topic_embeddings)\n        semantic_components = cosine_similarity(\n            self.topic_embeddings, self.vocab_embeddings\n        )\n        # Transforming to positive values from 0 to 1\n        # Then taking geometric average of the two values\n        self.components_ = np.sqrt(\n            ((1 + lexical_components) / 2) * ((1 + semantic_components) / 2)\n        )\n        return self.components_\n\n    def _init_model(self, n_components: int):\n        mixture = Peax()\n        return mixture\n\n    def plot_steps(self, hover_text=None):\n        try:\n            import plotly.express as px\n            from plotly.subplots import make_subplots\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n        dens_3d = self.plot_density_3d()\n        component_plot = self.plot_components(\n            show_points=True, hover_text=hover_text\n        )\n        points_plot = px.scatter(\n            x=self.reduced_embeddings[:, 0],\n            y=self.reduced_embeddings[:, 1],\n            template=\"plotly_white\",\n        )\n        points_plot = points_plot.update_layout(\n            margin=dict(l=0, r=0, b=0, t=0),\n        )\n        points_plot = points_plot.update_traces(\n            marker=dict(\n                color=\"#B7B7FF\",\n                size=6,\n                opacity=0.5,\n                line=dict(color=\"#01014B\", width=2),\n            )\n        )\n        colormap = {\n            name: color\n            for name, color in zip(\n                self.topic_names, px.colors.qualitative.Dark24\n            )\n        }\n        bar = px.bar(\n            y=self.topic_names,\n            x=self.weights_,\n            template=\"plotly_white\",\n            color_discrete_map=colormap,\n            color=self.topic_names,\n            text=[f\"{p:.2f}\" for p in self.weights_],\n        )\n        bar = bar.update_traces(\n            marker_line_color=\"black\",\n            marker_line_width=1.5,\n            opacity=0.8,\n        )\n\n        def update_annotation(a):\n            name = a.text.removeprefix(\"&lt;b&gt;\").split(\"&lt;\")[0]\n            return a.update(\n                # text=name,\n                font=dict(size=8, color=colormap[name]),\n                arrowsize=1,\n                arrowhead=1,\n                arrowwidth=1,\n                bgcolor=None,\n                opacity=0.7,\n                # bgcolor=colormap[name],\n                bordercolor=colormap[name],\n                borderwidth=0,\n            )\n\n        fig = make_subplots(\n            horizontal_spacing=0.0,\n            vertical_spacing=0.1,\n            rows=2,\n            cols=2,\n            subplot_titles=[\n                \"t-SN Embeddings\",\n                \"Peaks in Kernel Density Estimate\",\n                \"Gaussian Mixture Approximation\",\n                \"Component Probabilities\",\n            ],\n            specs=[\n                [\n                    {\"type\": \"xy\"},\n                    {\"type\": \"surface\"},\n                ],\n                [\n                    {\"type\": \"xy\"},\n                    {\"type\": \"bar\"},\n                ],\n            ],\n        )\n        for i, sub in enumerate([points_plot, dens_3d, component_plot, bar]):\n            row = i // 2\n            col = i % 2\n            for trace in sub.data:\n                fig.add_trace(trace, row=row + 1, col=col + 1)\n            for shape in sub.layout.shapes:\n                fig.add_shape(shape, row=row + 1, col=col + 1)\n        fig = fig.update_layout(\n            template=\"plotly_white\",\n            font=dict(family=\"Merriweather\", size=14, color=\"black\"),\n            width=1200,\n            height=800,\n            autosize=False,\n            margin=dict(r=0, l=0, t=40, b=0),\n        )\n        fig = fig.update_scenes(\n            annotations=[\n                update_annotation(annotation)\n                for annotation in dens_3d.layout.scene.annotations\n            ],\n            col=2,\n            row=1,\n        )\n        fig = fig.for_each_annotation(lambda a: a.update(yshift=0))\n        fig = fig.update_yaxes(visible=False, row=2, col=2)\n        fig = fig.update_xaxes(\n            title=dict(text=\"$P(z)$\", font=dict(size=16)), row=2, col=2\n        )\n        return fig\n</code></pre>"},{"location":"Topeax/#turftopic.models.topeax.Peax","title":"<code>turftopic.models.topeax.Peax</code>","text":"<p>             Bases: <code>ClusterMixin</code>, <code>BaseEstimator</code></p> <p>Clustering model based on density peaks.</p> <p>Parameters:</p> Name Type Description Default <code>random_state</code> <code>Optional[int]</code> <p>Random seed to use for fitting gaussian mixture to peaks.</p> <code>None</code> Source code in <code>turftopic/models/topeax.py</code> <pre><code>class Peax(ClusterMixin, BaseEstimator):\n    \"\"\"Clustering model based on density peaks.\n\n    Parameters\n    ----------\n    random_state: int, default None\n        Random seed to use for fitting gaussian mixture to peaks.\n    \"\"\"\n\n    def __init__(self, random_state: Optional[int] = None):\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        self.X_range = np.min(X), np.max(X)\n        self.density = gaussian_kde(X.T, \"scott\")\n        coord = np.linspace(*self.X_range, num=100)\n        z = []\n        for yval in coord:\n            points = np.stack([coord, np.full(coord.shape, yval)]).T\n            prob = np.exp(self.density.logpdf(points.T))\n            z.append(prob)\n        z = np.stack(z)\n        peaks = detect_peaks(z.T)\n        peak_ind = np.nonzero(peaks)\n        peak_pos = np.stack([coord[peak_ind[0]], coord[peak_ind[1]]]).T\n        weights = self.density.pdf(peak_pos.T)\n        weights = weights / weights.sum()\n        self.gmm_ = FixedMeanGaussianMixture(\n            peak_pos.shape[0],\n            means_init=peak_pos,\n            weights_init=weights,\n            random_state=self.random_state,\n        )\n        self.labels_ = self.gmm_.fit_predict(X)\n        # Checking whether there are close to zero components\n        is_zero = np.isclose(self.gmm_.weights_, 0)\n        n_zero = np.sum(is_zero)\n        if n_zero &gt; 0:\n            print(\n                f\"{n_zero} components have zero weight, removing them and refitting.\"\n            )\n        peak_pos = peak_pos[~is_zero]\n        weights = self.gmm_.weights_[~is_zero]\n        weights = weights / weights.sum()\n        self.gmm_ = FixedMeanGaussianMixture(\n            peak_pos.shape[0],\n            means_init=peak_pos,\n            weights_init=weights,\n            random_state=self.random_state,\n        )\n        self.labels_ = self.gmm_.fit_predict(X)\n        self.classes_ = np.sort(np.unique(self.labels_))\n        self.means_ = self.gmm_.means_\n        self.weights_ = self.gmm_.weights_\n        self.covariances_ = self.gmm_.covariances_\n        return self.labels_\n\n    @property\n    def n_components(self) -&gt; int:\n        return self.gmm_.n_components\n\n    def predict_proba(self, X):\n        return self.gmm_.predict_proba(X)\n\n    def score_samples(self, X):\n        return self.density.logpdf(X.T)\n\n    def score(self, X):\n        return np.mean(self.score_samples(X))\n</code></pre>"},{"location":"analyzers/","title":"Topic Analysis with LLMs","text":"<p>Topic analyzers are large language models, that are capable of interpreting topics' contents and can give human-readable descriptions of topics. This can be incredibly useful when it would require excessive manual labour to label and understand topics.</p> The role of analyzers in topic modelling. <p>Analyzers can do the following tasks:</p> <ul> <li>Summarize documents to make it easier for your topic model to consume.</li> <li>Name topics topics in a sensible and human-readable way based on top documents and keywords</li> <li>Describe topics in a couple of sentences</li> </ul> <p>While previously, smaller language models were not able to meaningfully accomplish this task, advances in in the field now allow you to generate highly accurate topic descriptions on your own laptop using the power of small LLMs.</p> <p>Warning</p> <p>The <code>namers</code> API is now deprecated and will be removed in Turftopic 1.1.0. Analyzers have full feature parity, and are able to accomplish way more.</p>"},{"location":"analyzers/#getting-started","title":"Getting Started","text":"<p>There are multiple types of analyzers in Turftopic that you can utilize for these tasks, all of which can be imported for the <code>analyzers</code> module:</p> <p>Choose an analyzer</p> Local LLM (recommended)OpenAI APIT5 <p>LLMs from HF Hub are natively supported in Turftopic. Our default choice of LLM is SmolLM3-3B, as it runs effortlessly on consumer hardware, is permissively licensed, allowing commercial use, and generates high-quality output.</p> <p>You can specify your model of choice by specifying <code>model_name=\"&lt;your_model_here&gt;\"</code>.</p> <p>SmolLM is also fine-tuned for reasoning. This is disabled by default to reduce computational burden, but you can enable it by specifying <code>enable_thinking=True</code>.</p> <pre><code>from turftopic.analyzers import LLMAnalyzer\n\n# We enable document summaries for topic analysis\nanalyzer = LLMAnalyzer(use_summaries=True)\n</code></pre> <p>You will have to install OpenAI, as it is not installed by default: <pre><code>pip install turftopic[openai]\nexport OPENAI_API_KEY=\"sk-&lt;your key goes here&gt;\"\n</code></pre></p> <p>The default model is <code>gpt-5-nano</code>, which is the cheapest new model in OpenAI's arsenal, and we found it generates satisfactory results.</p> <pre><code>from turftopic.analyzers import OpenAIAnalyzer\n\nanalyzer = OpenAIAnalyzer('gpt-5-nano')\n</code></pre> <p>T5 is less resource-intensive then causal language models, but it also generates lower quality results. You might have to fiddle around with it to get satisfactory results.</p> <pre><code>from turftopic import T5Analyzer\n\nmodel = T5Analyzer(\"google/flan-t5-large\")\n</code></pre>"},{"location":"analyzers/#document-summarization","title":"Document summarization","text":"<p>You can utilize large-language models for summarizing documents as a pre-processing step. This might make it easier for certain topic models to find patterns. You can also instruct the language model to summarize documents from a certain aspect.</p> <pre><code>from turftopic import KeyNMF\n\n# Your documents\ncorpus: list[str] = [...]\n\nsummarized_documents = [analyzer.summarize_document(doc) for doc in corpus]\n\n# Then we fit the topic model on the document summaries, which might be easier to analyze\nmodel = KeyNMF(10)\nmodel.fit(summarized_documents)\n</code></pre>"},{"location":"analyzers/#topic-analysis","title":"Topic analysis","text":"<p>You can also use LLMs after having trained a topic model to analyze topics' contents. Analysis in this case consists of: </p> <ol> <li>Naming the topics in a model and</li> <li>giving a short description of its contents.</li> </ol> <p>There are a number of options you should be aware of when doing this:</p> <ul> <li>The LLMs will always utilize the top keywords extracted by a topic model</li> <li>When <code>use_documents</code> is set to <code>True</code> (default), the analyzer will also use the top 10 documents from the topic model.</li> <li>When <code>use_summaries</code> is active, the analyzer first summarizes top 10 documents before feeding them to the analyzer. This can be a massive help, since it makes it easier for the analyzer to process the content, and makes sure that the analyzer's context length is enough. It does require more computation, though.</li> </ul> <p>Let's see what this looks like in action:</p> <p>Analyze topics</p> with <code>model</code>with <code>topic_data</code> <pre><code>from turftopic import KeyNMF\nfrom turftopic.analyzers import LLMAnalyzer\n\nanalyzer = LLMAnalyzer(use_summaries=False)\n\nmodel = KeyNMF(10).fit(corpus)\nanalysis_result = model.analyze_topics(analyzer, use_documents=True)\n</code></pre> <pre><code>from turftopic import KeyNMF\nfrom turftopic.analyzers import LLMAnalyzer\n\nanalyzer = LLMAnalyzer(use_summaries=False)\n\nmodel = KeyNMF(10)\ntopic_data = model.prepare_topic_data(corpus)\nanalysis_result = topic_data.analyze_topics(analyzer, use_documents=True)\n</code></pre> <p>Topic Naming</p> <p>If you only wish to assign topic names, but not generate a full analysis, you can still use <code>rename_topics</code>: <pre><code>model.rename_topics(analyzer, use_documents=False)\n</code></pre></p> <p>This will do multiple things:</p> <ol> <li>Return an <code>AnalysisResults</code> object which contains: <code>topic_names</code>, <code>topic_descriptions</code> and <code>document_summaries</code>, which are the top documents' summaries, when applicable</li> <li>Set these properties on the object it gets called on (<code>model</code> or <code>topic_data</code>)</li> </ol> <p><code>AnalysisResults</code> can also be turned into a DataFrame or dictionary, by calling <code>to_df()</code> and <code>to_dict()</code> respectively.</p> <pre><code>analysis_result.to_df()\n</code></pre> <pre><code>                                         topic_names                                 topic_descriptions\n0                         Dialogue and Communication  This topic examines how conversation functions...\n1  AI Assistant: Requesting Detailed User Informa...  It describes an assistant that asks the user f...\n2          Ethical Generative AI and Language Models  It covers the design and deployment of generat...\n3   French\u2013English Translation in Law and Literature  It examines translation between French and Eng...\n4  France: Social, Economic, Legal Information an...  It covers how social conversations in France e...\n5                   Email-based Python code requests  It depicts a user making requests that involve...\n6           Lesson Planning and Classroom Activities  It covers the school-based process of teaching...\n7         French cultural conversations for children  It explores how people talk about culture in F...\n8            Data Analytics Training and Development  It focuses on structured training programs tha...\n9                 Sustainable Energy and Environment  It explores how energy production and use infl...\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.base.AnalysisResults","title":"<code>turftopic.analyzers.base.AnalysisResults</code>  <code>dataclass</code>","text":"<p>Container class for results of topic analysis.</p> <p>Attributes:</p> Name Type Description <code>topic_names</code> <code>list[str]</code> <p>Generated topic names.</p> <code>topic_descriptions</code> <code>list[str]</code> <p>Genreated topic descriptions.</p> <code>document_summaries</code> <code>list[list[str]], default None</code> <p>Summaries of top 10 documents for each topic, when use_summaries is enabled.</p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>@dataclass\nclass AnalysisResults:\n    \"\"\"Container class for results of topic analysis.\n\n    Attributes\n    ----------\n    topic_names: list[str]\n        Generated topic names.\n    topic_descriptions: list[str]\n        Genreated topic descriptions.\n    document_summaries: list[list[str]], default None\n        Summaries of top 10 documents for each topic, when use_summaries is enabled.\n    \"\"\"\n\n    topic_names: list[str]\n    topic_descriptions: list[str]\n    document_summaries: Optional[list[list[str]]] = None\n\n    def to_dict(self) -&gt; dict:\n        \"\"\"Returns the analysis result as a dictionary\"\"\"\n        res = dict(\n            topic_names=self.topic_names,\n            topic_descriptions=self.topic_descriptions,\n        )\n        if self.document_summaries is not None:\n            res[\"document_summaries\"] = self.document_summaries\n        return res\n\n    def to_df(self):\n        \"\"\"Turns analysis result object into a dataframe\"\"\"\n        try:\n            import pandas as pd\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"You need to pip install pandas to be able to use dataframes.\"\n            )\n        return pd.DataFrame(self.to_dict())\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.base.AnalysisResults.to_df","title":"<code>to_df()</code>","text":"<p>Turns analysis result object into a dataframe</p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>def to_df(self):\n    \"\"\"Turns analysis result object into a dataframe\"\"\"\n    try:\n        import pandas as pd\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"You need to pip install pandas to be able to use dataframes.\"\n        )\n    return pd.DataFrame(self.to_dict())\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.base.AnalysisResults.to_dict","title":"<code>to_dict()</code>","text":"<p>Returns the analysis result as a dictionary</p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>def to_dict(self) -&gt; dict:\n    \"\"\"Returns the analysis result as a dictionary\"\"\"\n    res = dict(\n        topic_names=self.topic_names,\n        topic_descriptions=self.topic_descriptions,\n    )\n    if self.document_summaries is not None:\n        res[\"document_summaries\"] = self.document_summaries\n    return res\n</code></pre>"},{"location":"analyzers/#prompting","title":"Prompting","text":"<p>You can instruct analyzers to specifically deal with the task you are trying to accomplish by using prompts. Here we will give an overview of how you can do this.</p>"},{"location":"analyzers/#providing-task-context","title":"Providing Task Context","text":"<p>Sometimes you might have a specific task that might require additional information to analyze correctly. You can add information to the prompts by using the <code>context</code> attribute:</p> <pre><code>from turftopic.analyzers import LLMAnalyzer\n\nanalyzer = LLMAnalyzer(context=\"Analyze topical content in financial documents published by the central bank.\")\n</code></pre>"},{"location":"analyzers/#fully-custom-prompts","title":"Fully Custom Prompts","text":"<p>Since all analyzers are generative language models, you can prompt them however you wish. We provide default prompts, which we found to prove well, but you are more than free to modify these.</p> <p>Prompts internally get formatted with <code>str.format()</code>, so all templated content should be in-between curly brackets. Analyzers have a number of prompts:</p> <pre><code>system_prompt = DEFAULT_SYSTEM_PROMPT\nsummary_prompt = SUMMARY_PROMPT\nnamer_prompt = NAMER_PROMPT\ndescription_prompt = DESCRIPTION_PROMPT\n</code></pre> <ol> <li><code>system_prompt</code> describes the general role of the language model, and is not templated.</li> <li><code>summary_prompt</code>, which is responsible for providing document summaries, and is templated with <code>{document}</code></li> <li><code>namer_prompt</code>, which describes how topics should be named, and is templated with <code>{keywords}</code></li> <li><code>description_prompt</code>, which dictates how topic descriptions should be generated and is templated with <code>{keywords}</code></li> </ol> <p>Documents are added at the end, when <code>use_documents=True</code>.</p> Click to see example <pre><code>from turftopic.analyzers import LLMAnalyzer\n\nsystem_prompt = \"\"\"\nYou are a topic analyzer.\nFollow instructions closely and exactly.\n\"\"\"\n\nnamer_prompt = \"\"\"\nPlease provide a human-readable name for a topic.\nThe topic is described by the following set of keywords: {keywords}.\n\"\"\"\n\ndescription_prompt = \"\"\"\nDescribe the following topic in a couple of sentences.\nThe topic is described by the following set of keywords: {keywords}.\n\"\"\"\n\nsummary_prompt = \"\"\"\nSummarize the following document: {document}\n\"\"\"\n\nnamer = LLMAnalyzer(\n    system_prompt=system_prompt,\n    namer_prompt=namer_prompt,\n    description_prompt=description_prompt,\n    summary_prompt=summary_prompt\n)\n</code></pre>"},{"location":"analyzers/#api-reference","title":"API Reference","text":""},{"location":"analyzers/#turftopic.analyzers.base.Analyzer","title":"<code>turftopic.analyzers.base.Analyzer</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>class Analyzer(ABC):\n    system_prompt = DEFAULT_SYSTEM_PROMPT\n    summary_prompt = SUMMARY_PROMPT\n    namer_prompt = NAMER_PROMPT\n    description_prompt = DESCRIPTION_PROMPT\n    context = None\n    use_summaries = False\n\n    @abstractmethod\n    def generate_text(self, prompt: str) -&gt; str:\n        \"\"\"Generates response to a given prompt.\"\"\"\n        pass\n\n    def summarize_document(self, document: str) -&gt; str:\n        \"\"\"Summarizes document so that analysis becomes easier.\"\"\"\n        prompt = self.summary_prompt.format(document=document)\n        return self.generate_text(prompt)\n\n    def describe_topic(\n        self,\n        keywords: list[str],\n        documents: Optional[list] = None,\n    ):\n        \"\"\"Gives abstract summarization of topic content.\"\"\"\n        _keys = \", \".join(keywords)\n        prompt = self.description_prompt.format(keywords=_keys)\n        if documents:\n            prompt += self.template_documents(documents)\n        if self.context:\n            prompt += CONTEXT_TEMPLATE.format(context=self.context)\n        return self.generate_text(prompt)\n\n    def name_topic(\n        self,\n        keywords: list[str],\n        documents: Optional[list] = None,\n    ) -&gt; str:\n        \"\"\"Names one topic based on top descriptive aspects.\"\"\"\n        _keys = \", \".join(keywords)\n        prompt = self.namer_prompt.format(keywords=_keys)\n        if documents:\n            prompt += self.template_documents(documents)\n        if self.context:\n            prompt += CONTEXT_TEMPLATE.format(context=self.context)\n        return self.generate_text(prompt)\n\n    def name_topics(\n        self,\n        keywords: list[list[str]],\n        documents: list[list[str]] = None,\n    ) -&gt; list[str]:\n        \"\"\"Names all topics based on top descriptive terms.\n\n        Parameters\n        ----------\n        keywords: list[list[str]]\n            Top K highest ranking terms on the topics.\n        documents: list[list[str]], optional\n            Top K relevant documents to each topic.\n\n        Returns\n        -------\n        list[str]\n            Topic names returned by the namer.\n        \"\"\"\n        names = []\n        if documents is not None:\n            key_doc = list(zip(keywords, documents))\n            for keys, docs in track(key_doc, description=\"Naming topics...\"):\n                names.append(self.name_topic(keys, documents=docs))\n        else:\n            for keys in track(keywords, description=\"Naming topics...\"):\n                names.append(self.name_topic(keys))\n        return names\n\n    def template_documents(self, documents: list[str]) -&gt; str:\n        doc_list = \"\\n\".join([f\" - {doc}\" for doc in documents])\n        return \"\"\"\n        In addition the topic is characterized by the following documents:\n        {documents}\n        \"\"\".format(\n            documents=doc_list\n        )\n\n    def analyze_topics(\n        self,\n        keywords: list[list[str]],\n        documents: Optional[list[list[str]]] = None,\n        use_summaries: Optional[bool] = None,\n    ) -&gt; AnalysisResults:\n        \"\"\"Analyzes topic model with a language model.\n        Generates topic names, descriptions and document summaries (optional).\n\n        Parameters\n        ----------\n        keywords: list[list[str]]\n            Keywords for each topic.\n        documents: list[list[str]], optional\n            Top documents for each topic.\n        use_summaries: bool, optional\n            Indicates whether the analyzer should summarize documents\n            prior to analyzing the topic.\n\n        Returns\n        -------\n        dict\n            Dictionary containing `topic_names`, `topic_descriptions` and `document_summaries` if relevant.\n        \"\"\"\n        console = Console()\n        output = {\"topic_names\": [], \"topic_descriptions\": []}\n        use_summaries = (\n            use_summaries if use_summaries is not None else self.use_summaries\n        )\n        if documents is not None:\n            if use_summaries:\n                output[\"document_summaries\"] = []\n                for docs in track(\n                    documents, description=\"Summarizing documents\"\n                ):\n                    _sums = []\n                    for doc in docs:\n                        _sums.append(self.summarize_document(doc))\n                    output[\"document_summaries\"].append(_sums)\n                console.log(\"Documents summarized.\")\n                # Updating parameter so summaries are used down-stream\n                documents = output[\"document_summaries\"]\n            # Organizing into a list so we can iterate and know the length at the same time.\n            key_doc_pairs = list(zip(keywords, documents))\n            for keys, docs in track(\n                key_doc_pairs, description=\"Generating topic names\"\n            ):\n                output[\"topic_names\"].append(\n                    self.name_topic(keys, documents=docs)\n                )\n            console.log(\"Topic names generated.\")\n            for keys, docs in track(\n                key_doc_pairs, description=\"Generating topic descriptions.\"\n            ):\n                output[\"topic_descriptions\"].append(\n                    self.describe_topic(keys, documents=docs)\n                )\n            console.log(\"Topic descriptions generated.\")\n        else:\n            for keys in track(keywords, description=\"Naming\"):\n                output[\"topic_names\"].append(\n                    self.name_topic(keys, documents=None)\n                )\n            console.log(\"Topic names generated.\")\n            for keys in track(keywords, description=\"Describing topics.\"):\n                output[\"topic_descriptions\"].append(\n                    self.describe_topic(keys, documents=None)\n                )\n            console.log(\"Topic descriptions generated.\")\n        return AnalysisResults(**output)\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.base.Analyzer.analyze_topics","title":"<code>analyze_topics(keywords, documents=None, use_summaries=None)</code>","text":"<p>Analyzes topic model with a language model. Generates topic names, descriptions and document summaries (optional).</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>list[list[str]]</code> <p>Keywords for each topic.</p> required <code>documents</code> <code>Optional[list[list[str]]]</code> <p>Top documents for each topic.</p> <code>None</code> <code>use_summaries</code> <code>Optional[bool]</code> <p>Indicates whether the analyzer should summarize documents prior to analyzing the topic.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>Dictionary containing <code>topic_names</code>, <code>topic_descriptions</code> and <code>document_summaries</code> if relevant.</p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>def analyze_topics(\n    self,\n    keywords: list[list[str]],\n    documents: Optional[list[list[str]]] = None,\n    use_summaries: Optional[bool] = None,\n) -&gt; AnalysisResults:\n    \"\"\"Analyzes topic model with a language model.\n    Generates topic names, descriptions and document summaries (optional).\n\n    Parameters\n    ----------\n    keywords: list[list[str]]\n        Keywords for each topic.\n    documents: list[list[str]], optional\n        Top documents for each topic.\n    use_summaries: bool, optional\n        Indicates whether the analyzer should summarize documents\n        prior to analyzing the topic.\n\n    Returns\n    -------\n    dict\n        Dictionary containing `topic_names`, `topic_descriptions` and `document_summaries` if relevant.\n    \"\"\"\n    console = Console()\n    output = {\"topic_names\": [], \"topic_descriptions\": []}\n    use_summaries = (\n        use_summaries if use_summaries is not None else self.use_summaries\n    )\n    if documents is not None:\n        if use_summaries:\n            output[\"document_summaries\"] = []\n            for docs in track(\n                documents, description=\"Summarizing documents\"\n            ):\n                _sums = []\n                for doc in docs:\n                    _sums.append(self.summarize_document(doc))\n                output[\"document_summaries\"].append(_sums)\n            console.log(\"Documents summarized.\")\n            # Updating parameter so summaries are used down-stream\n            documents = output[\"document_summaries\"]\n        # Organizing into a list so we can iterate and know the length at the same time.\n        key_doc_pairs = list(zip(keywords, documents))\n        for keys, docs in track(\n            key_doc_pairs, description=\"Generating topic names\"\n        ):\n            output[\"topic_names\"].append(\n                self.name_topic(keys, documents=docs)\n            )\n        console.log(\"Topic names generated.\")\n        for keys, docs in track(\n            key_doc_pairs, description=\"Generating topic descriptions.\"\n        ):\n            output[\"topic_descriptions\"].append(\n                self.describe_topic(keys, documents=docs)\n            )\n        console.log(\"Topic descriptions generated.\")\n    else:\n        for keys in track(keywords, description=\"Naming\"):\n            output[\"topic_names\"].append(\n                self.name_topic(keys, documents=None)\n            )\n        console.log(\"Topic names generated.\")\n        for keys in track(keywords, description=\"Describing topics.\"):\n            output[\"topic_descriptions\"].append(\n                self.describe_topic(keys, documents=None)\n            )\n        console.log(\"Topic descriptions generated.\")\n    return AnalysisResults(**output)\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.base.Analyzer.describe_topic","title":"<code>describe_topic(keywords, documents=None)</code>","text":"<p>Gives abstract summarization of topic content.</p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>def describe_topic(\n    self,\n    keywords: list[str],\n    documents: Optional[list] = None,\n):\n    \"\"\"Gives abstract summarization of topic content.\"\"\"\n    _keys = \", \".join(keywords)\n    prompt = self.description_prompt.format(keywords=_keys)\n    if documents:\n        prompt += self.template_documents(documents)\n    if self.context:\n        prompt += CONTEXT_TEMPLATE.format(context=self.context)\n    return self.generate_text(prompt)\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.base.Analyzer.generate_text","title":"<code>generate_text(prompt)</code>  <code>abstractmethod</code>","text":"<p>Generates response to a given prompt.</p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>@abstractmethod\ndef generate_text(self, prompt: str) -&gt; str:\n    \"\"\"Generates response to a given prompt.\"\"\"\n    pass\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.base.Analyzer.name_topic","title":"<code>name_topic(keywords, documents=None)</code>","text":"<p>Names one topic based on top descriptive aspects.</p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>def name_topic(\n    self,\n    keywords: list[str],\n    documents: Optional[list] = None,\n) -&gt; str:\n    \"\"\"Names one topic based on top descriptive aspects.\"\"\"\n    _keys = \", \".join(keywords)\n    prompt = self.namer_prompt.format(keywords=_keys)\n    if documents:\n        prompt += self.template_documents(documents)\n    if self.context:\n        prompt += CONTEXT_TEMPLATE.format(context=self.context)\n    return self.generate_text(prompt)\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.base.Analyzer.name_topics","title":"<code>name_topics(keywords, documents=None)</code>","text":"<p>Names all topics based on top descriptive terms.</p> <p>Parameters:</p> Name Type Description Default <code>keywords</code> <code>list[list[str]]</code> <p>Top K highest ranking terms on the topics.</p> required <code>documents</code> <code>list[list[str]]</code> <p>Top K relevant documents to each topic.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>Topic names returned by the namer.</p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>def name_topics(\n    self,\n    keywords: list[list[str]],\n    documents: list[list[str]] = None,\n) -&gt; list[str]:\n    \"\"\"Names all topics based on top descriptive terms.\n\n    Parameters\n    ----------\n    keywords: list[list[str]]\n        Top K highest ranking terms on the topics.\n    documents: list[list[str]], optional\n        Top K relevant documents to each topic.\n\n    Returns\n    -------\n    list[str]\n        Topic names returned by the namer.\n    \"\"\"\n    names = []\n    if documents is not None:\n        key_doc = list(zip(keywords, documents))\n        for keys, docs in track(key_doc, description=\"Naming topics...\"):\n            names.append(self.name_topic(keys, documents=docs))\n    else:\n        for keys in track(keywords, description=\"Naming topics...\"):\n            names.append(self.name_topic(keys))\n    return names\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.base.Analyzer.summarize_document","title":"<code>summarize_document(document)</code>","text":"<p>Summarizes document so that analysis becomes easier.</p> Source code in <code>turftopic/analyzers/base.py</code> <pre><code>def summarize_document(self, document: str) -&gt; str:\n    \"\"\"Summarizes document so that analysis becomes easier.\"\"\"\n    prompt = self.summary_prompt.format(document=document)\n    return self.generate_text(prompt)\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.hf_llm.LLMAnalyzer","title":"<code>turftopic.analyzers.hf_llm.LLMAnalyzer</code>","text":"<p>             Bases: <code>Analyzer</code></p> <p>Analyze topic model with an open LLM from HF Hub.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Open LLM to use from HF Hub.</p> <code>'HuggingFaceTB/SmolLM3-3B'</code> <code>use_summaries</code> <code>bool</code> <p>Indicates whether the language model should summarize documents before analyzing the topics.</p> <code>False</code> <code>context</code> <code>Optional[str]</code> <p>Additional context provided to the analyzer for analysis. e.g. \"Analyze topics from blog posts related to morality and religion\"</p> <code>None</code> <code>system_prompt</code> <code>Optional[str]</code> <p>Ignored, exists for compatibility</p> <code>None</code> <code>summary_prompt</code> <code>Optional[str]</code> <p>Prompt to use for abstractive summarization.</p> <code>None</code> <code>namer_prompt</code> <code>Optional[str]</code> <p>Prompt template for naming topics.</p> <code>None</code> <code>description_prompt</code> <code>Optional[str]</code> <p>Prompt template for generating topic descriptions.</p> <code>None</code> <code>device</code> <code>str</code> <p>ID of the device to run the language model on.</p> <code>'cpu'</code> <code>max_new_tokens</code> <code>int</code> <p>Max new tokens to generate when analyzing.</p> <code>32768</code> <code>enable_thinking</code> <code>bool</code> <p>Indicates whether thinking mode should be enabled.</p> <code>False</code> Source code in <code>turftopic/analyzers/hf_llm.py</code> <pre><code>class LLMAnalyzer(Analyzer):\n    \"\"\"Analyze topic model with an open LLM from HF Hub.\n\n    Parameters\n    ----------\n    model_name: str, default 'HuggingFaceTB/SmolLM3-3B'\n        Open LLM to use from HF Hub.\n    use_summaries: bool, default False\n        Indicates whether the language model should summarize documents before\n        analyzing the topics.\n    context: str, default None\n        Additional context provided to the analyzer for analysis.\n        e.g. \"Analyze topics from blog posts related to morality and religion\"\n    system_prompt: str, default None\n        Ignored, exists for compatibility\n    summary_prompt: str, default None\n        Prompt to use for abstractive summarization.\n    namer_prompt: str, default None\n        Prompt template for naming topics.\n    description_prompt: str, default None\n        Prompt template for generating topic descriptions.\n    device: str, default \"cpu\"\n        ID of the device to run the language model on.\n    max_new_tokens: int, default 32768\n        Max new tokens to generate when analyzing.\n    enable_thinking: bool, default False\n        Indicates whether thinking mode should be enabled.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"HuggingFaceTB/SmolLM3-3B\",\n        context: Optional[str] = None,\n        use_summaries: bool = False,\n        system_prompt: Optional[str] = None,\n        summary_prompt: Optional[str] = None,\n        namer_prompt: Optional[str] = None,\n        description_prompt: Optional[str] = None,\n        max_new_tokens: int = 32768,\n        device: str = \"cpu\",\n        enable_thinking: bool = False,\n    ):\n        self.device = device\n        self.model_name = model_name\n        # load the tokenizer and the model\n        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n        self.model = AutoModelForCausalLM.from_pretrained(\n            self.model_name,\n        ).to(self.device)\n        self.summary_prompt = summary_prompt or self.summary_prompt\n        self.namer_prompt = namer_prompt or self.namer_prompt\n        self.description_prompt = description_prompt or self.description_prompt\n        self.use_summaries = use_summaries\n        self.max_new_tokens = max_new_tokens\n        self.enable_thinking = enable_thinking\n\n    def generate_text(self, prompt: str) -&gt; str:\n        thinking = \"/think\" if self.enable_thinking else \"/no_think\"\n        system_prompt = self.system_prompt + thinking\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n        text = self.tokenizer.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n        )\n        model_inputs = self.tokenizer([text], return_tensors=\"pt\").to(\n            self.model.device\n        )\n        # Generate the output\n        generated_ids = self.model.generate(\n            **model_inputs, max_new_tokens=32768\n        )\n        # Get and decode the output\n        output_ids = generated_ids[0][len(model_inputs.input_ids[0]) :]\n        result = self.tokenizer.decode(output_ids, skip_special_tokens=True)\n        result = remove_thinking_trace(result)\n        return result\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.openai.OpenAIAnalyzer","title":"<code>turftopic.analyzers.openai.OpenAIAnalyzer</code>","text":"<p>             Bases: <code>Analyzer</code></p> <p>Analyze topic model with an OpenAI LLM.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>OpenAI model to use.</p> <code>'gpt-5-nano'</code> <code>use_summaries</code> <code>bool</code> <p>Indicates whether the language model should summarize documents before analyzing the topics.</p> <code>False</code> <code>context</code> <code>Optional[str]</code> <p>Additional context provided to the analyzer for analysis. e.g. \"Analyze topics from blog posts related to morality and religion\"</p> <code>None</code> <code>system_prompt</code> <code>Optional[str]</code> <p>System prompt to use for the language model.</p> <code>None</code> <code>summary_prompt</code> <code>Optional[str]</code> <p>Prompt to use for abstractive summarization.</p> <code>None</code> <code>namer_prompt</code> <code>Optional[str]</code> <p>Prompt template for naming topics.</p> <code>None</code> <code>description_prompt</code> <code>Optional[str]</code> <p>Prompt template for generating topic descriptions.</p> <code>None</code> Source code in <code>turftopic/analyzers/openai.py</code> <pre><code>class OpenAIAnalyzer(Analyzer):\n    \"\"\"Analyze topic model with an OpenAI LLM.\n\n    Parameters\n    ----------\n    model_name: str, default 'gpt-5-nano'\n        OpenAI model to use.\n    use_summaries: bool, default False\n        Indicates whether the language model should summarize documents before\n        analyzing the topics.\n    context: str, default None\n        Additional context provided to the analyzer for analysis.\n        e.g. \"Analyze topics from blog posts related to morality and religion\"\n    system_prompt: str, default None\n        System prompt to use for the language model.\n    summary_prompt: str, default None\n        Prompt to use for abstractive summarization.\n    namer_prompt: str, default None\n        Prompt template for naming topics.\n    description_prompt: str, default None\n        Prompt template for generating topic descriptions.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"gpt-5-nano\",\n        context: Optional[str] = None,\n        use_summaries: bool = False,\n        system_prompt: Optional[str] = None,\n        summary_prompt: Optional[str] = None,\n        namer_prompt: Optional[str] = None,\n        description_prompt: Optional[str] = None,\n    ):\n        self.client = openai.OpenAI()\n        self.model_name = model_name\n        self.system_prompt = system_prompt or self.system_prompt\n        self.summary_prompt = summary_prompt or self.summary_prompt\n        self.namer_prompt = namer_prompt or self.namer_prompt\n        self.description_prompt = description_prompt or self.description_prompt\n        self.use_summaries = use_summaries\n\n    def generate_text(self, prompt: str) -&gt; str:\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": prompt},\n        ]\n        response = self.client.chat.completions.create(\n            messages=messages,\n            model=self.model_name,\n        )\n        return response.choices[0].message.content\n</code></pre>"},{"location":"analyzers/#turftopic.analyzers.t5.T5Analyzer","title":"<code>turftopic.analyzers.t5.T5Analyzer</code>","text":"<p>             Bases: <code>Analyzer</code></p> <p>Analyze topic model with a text-to-text model.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Text-to-text model to use for analyses.</p> <code>'google/flan-t5-small'</code> <code>use_summaries</code> <code>bool</code> <p>Indicates whether the language model should summarize documents before analyzing the topics.</p> <code>False</code> <code>context</code> <code>Optional[str]</code> <p>Additional context provided to the analyzer for analysis. e.g. \"Analyze topics from blog posts related to morality and religion\"</p> <code>None</code> <code>system_prompt</code> <code>Optional[str]</code> <p>Ignored, exists for compatibility</p> <code>None</code> <code>summary_prompt</code> <code>Optional[str]</code> <p>Prompt to use for abstractive summarization.</p> <code>None</code> <code>namer_prompt</code> <code>Optional[str]</code> <p>Prompt template for naming topics.</p> <code>T5_NAME_PROMPT</code> <code>description_prompt</code> <code>Optional[str]</code> <p>Prompt template for generating topic descriptions.</p> <code>T5_DESC_PROMPT</code> <code>device</code> <code>str</code> <p>ID of the device to run the language model on.</p> <code>'cpu'</code> Source code in <code>turftopic/analyzers/t5.py</code> <pre><code>class T5Analyzer(Analyzer):\n    \"\"\"Analyze topic model with a text-to-text model.\n\n    Parameters\n    ----------\n    model_name: str, default 'google/flan-t5-small'\n        Text-to-text model to use for analyses.\n    use_summaries: bool, default False\n        Indicates whether the language model should summarize documents before\n        analyzing the topics.\n    context: str, default None\n        Additional context provided to the analyzer for analysis.\n        e.g. \"Analyze topics from blog posts related to morality and religion\"\n    system_prompt: str, default None\n        Ignored, exists for compatibility\n    summary_prompt: str, default None\n        Prompt to use for abstractive summarization.\n    namer_prompt: str, default None\n        Prompt template for naming topics.\n    description_prompt: str, default None\n        Prompt template for generating topic descriptions.\n    device: str, default \"cpu\"\n        ID of the device to run the language model on.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = \"google/flan-t5-small\",\n        context: Optional[str] = None,\n        use_summaries: bool = False,\n        system_prompt: Optional[str] = None,\n        summary_prompt: Optional[str] = None,\n        namer_prompt: Optional[str] = T5_NAME_PROMPT,\n        description_prompt: Optional[str] = T5_DESC_PROMPT,\n        device: str = \"cpu\",\n    ):\n        self.device = device\n        self.model_name = model_name\n        self.pipeline = pipeline(\n            task=\"text2text-generation\",\n            model=self.model_name,\n            device=self.device,\n        )\n        self.summary_prompt = summary_prompt or self.summary_prompt\n        self.namer_prompt = namer_prompt or self.namer_prompt\n        self.description_prompt = description_prompt or self.description_prompt\n        self.use_summaries = use_summaries\n\n    def generate_text(self, prompt: str) -&gt; str:\n        return self.pipeline(prompt)\n</code></pre>"},{"location":"benchmark/","title":"Model Leaderboard","text":"<p>To aid you in choosing the best model for your use case, we have made a topic model benchmark and leaderboard. The benchmark consists of all English P2P clustering tasks from the most recent version of MTEB, plus a tweet and a news dataset, as these are not present in MTEB.</p> <p>Models were tested for topic quality using the methodology of Kardos et al. 2025, and cluster quality using adjusted mutual information (AMI), the Fowlkes-Mallows index (FMI) and V-measure scores. All models were run on an older, but still powerful Dell Precision laptop, with 32 GBs of RAM, and i7, which was apparently not enough, as some models ran out of memory on some of the larger datasets. Due to this, and the fact that the scale of the scores is different for different tasks, we present the average percentile scores on these metrics in the table bellow.</p> Click to see Benchmark code <pre><code>import argparse\nimport json\nimport time\nfrom itertools import chain, combinations\nfrom pathlib import Path\nfrom typing import Callable, Iterable\n\nimport gensim.downloader as api\nimport mteb\nimport numpy as np\nfrom datasets import load_dataset\nfrom glovpy import GloVe\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom turftopic import (GMM, AutoEncodingTopicModel, BERTopic, FASTopic, KeyNMF,\n                      SemanticSignalSeparation, SensTopic, Top2Vec, Topeax)\n\ntopic_models = {\n    \"Topeax(Auto)\": lambda encoder, n_components: Topeax(\n        encoder=encoder, random_state=42\n    ),\n    \"BERTopic(Auto)\": lambda encoder, n_components: BERTopic(\n        encoder=encoder, random_state=42\n    ),\n    \"Top2Vec(Auto)\": lambda encoder, n_components: Top2Vec(\n        encoder=encoder, random_state=42\n    ),\n    \"SensTopic(Auto)\": lambda encoder, n_components: SensTopic(\n        n_components=\"auto\", encoder=encoder, random_state=42\n    ),\n    \"SensTopic\": lambda encoder, n_components: SensTopic(\n        n_components=n_components, encoder=encoder, random_state=42\n    ),\n    \"KeyNMF(Auto)\": lambda encoder, n_components: KeyNMF(\n        n_components=\"auto\", encoder=encoder, random_state=42\n    ),\n    \"KeyNMF\": lambda encoder, n_components: KeyNMF(\n        n_components=n_components, encoder=encoder, random_state=42\n    ),\n    \"GMM\": lambda encoder, n_components: GMM(\n        n_components=n_components, encoder=encoder, random_state=42\n    ),\n    \"Top2Vec(Reduce)\": lambda encoder, n_components: Top2Vec(\n        n_reduce_to=n_components, encoder=encoder, random_state=42\n    ),\n    \"BERTopic(Reduce)\": lambda encoder, n_components: BERTopic(\n        n_reduce_to=n_components, encoder=encoder, random_state=42\n    ),\n    \"ZeroShotTM\": lambda encoder, n_components: AutoEncodingTopicModel(\n        n_components=n_components, encoder=encoder, random_state=42, combined=False\n    ),\n    \"SemanticSignalSeparation\": lambda encoder, n_components: SemanticSignalSeparation(\n        n_components=n_components, encoder=encoder, random_state=42\n    ),\n    \"FASTopic\": lambda encoder, n_components: FASTopic(\n        n_components=n_components, encoder=encoder, random_state=42\n    ),\n}\n\n\ndef load_corpora() -&gt; Iterable[tuple[str, Callable]]:\n    mteb_tasks = mteb.get_tasks(\n        [\n            \"ArXivHierarchicalClusteringP2P\",\n            \"BiorxivClusteringP2P.v2\",\n            \"MedrxivClusteringP2P.v2\",\n            \"StackExchangeClusteringP2P.v2\",\n            \"TwentyNewsgroupsClustering.v2\",\n        ]\n    )\n    for task in mteb_tasks:\n\n        def _load_dataset():\n            task.load_data()\n            ds = task.dataset[\"test\"]\n            corpus = list(ds[\"sentences\"])\n            if isinstance(ds[\"labels\"][0], list):\n                true_labels = [label[0] for label in ds[\"labels\"]]\n            else:\n                true_labels = list(ds[\"labels\"])\n            return corpus, true_labels\n\n        yield task.metadata.name, _load_dataset\n\n    def _load_dataset():\n        # Taken from here cardiffnlp/tweet_topic_single with \"train_all\"\n        ds = load_dataset(\"kardosdrur/tweet_topic_clustering\", split=\"train_all\")\n        corpus = list(ds[\"text\"])\n        labels = list(ds[\"label\"])\n        return corpus, labels\n\n    yield \"TweetTopicClustering\", _load_dataset\n\n    def _load_dataset():\n        ds = load_dataset(\"gopalkalpande/bbc-news-summary\", split=\"train\")\n        corpus = list(ds[\"Summaries\"])\n        labels = list(ds[\"File_path\"])\n        return corpus, labels\n\n    yield \"BBCNewsClustering\", _load_dataset\n\n\ndef diversity(keywords: list[list[str]]) -&gt; float:\n    all_words = list(chain.from_iterable(keywords))\n    unique_words = set(all_words)\n    total_words = len(all_words)\n    return float(len(unique_words) / total_words)\n\n\ndef word_embedding_coherence(keywords, wv):\n    arrays = []\n    for index, topic in enumerate(keywords):\n        if len(topic) &gt; 0:\n            local_simi = []\n            for word1, word2 in combinations(topic, 2):\n                if word1 in wv.index_to_key and word2 in wv.index_to_key:\n                    local_simi.append(wv.similarity(word1, word2))\n            arrays.append(np.nanmean(local_simi))\n    return float(np.nanmean(arrays))\n\n\ndef evaluate_clustering(true_labels, pred_labels) -&gt; dict[str, float]:\n    res = {}\n    for metric in [\n        metrics.fowlkes_mallows_score,\n        metrics.homogeneity_score,\n        metrics.completeness_score,\n        metrics.adjusted_mutual_info_score,\n    ]:\n        res[metric.__name__] = metric(true_labels, pred_labels)\n    return res\n\n\ndef get_keywords(model) -&gt; list[list[str]]:\n    \"\"\"Get top words and ignore outlier topic.\"\"\"\n    n_topics = model.components_.shape[0]\n    try:\n        classes = model.classes_\n    except AttributeError:\n        classes = list(range(n_topics))\n    res = []\n    for topic_id, words in zip(classes, model.get_top_words()):\n        if topic_id != -1:\n            res.append(words)\n    return res\n\n\ndef evaluate_topic_quality(keywords, ex_wv, in_wv) -&gt; dict[str, float]:\n    res = {\n        \"diversity\": diversity(keywords),\n        \"c_in\": word_embedding_coherence(keywords, in_wv),\n        \"c_ex\": word_embedding_coherence(keywords, ex_wv),\n    }\n    return res\n\n\ndef load_cache(out_path):\n    cache_entries = []\n    with out_path.open() as cache_file:\n        for line in cache_file:\n            entry = json.loads(line.strip())\n            cache_entry = (entry[\"task\"], entry[\"model\"])\n            cache_entries.append(cache_entry)\n    return set(cache_entries)\n\n\ndef main(encoder_name: str = \"all-MiniLM-L6-v2\"):\n    out_dir = Path(\"results\")\n    out_dir.mkdir(exist_ok=True)\n    encoder_path_name = encoder_name.replace(\"/\", \"__\")\n    out_path = out_dir.joinpath(f\"{encoder_path_name}.jsonl\")\n    if out_path.is_file():\n        cache = load_cache(out_path)\n    else:\n        cache = set()\n        # Create file if doesn't exist\n        with out_path.open(\"w\"):\n            pass\n    print(\"Loading external word embeddings\")\n    ex_wv = api.load(\"word2vec-google-news-300\")\n    print(\"Loading benchmark\")\n    tasks = load_corpora()\n    for task_name, load in tasks:\n        if all([(task_name, model_name) in cache for model_name in topic_models]):\n            print(\"All models already completed, skipping.\")\n            continue\n        print(\"Load corpus\")\n        corpus, true_labels = load()\n        print(\"Training internal word embeddings using GloVe...\")\n        tokenizer = CountVectorizer().build_analyzer()\n        glove = GloVe(vector_size=50)\n        tokenized_corpus = [tokenizer(text) for text in corpus]\n        glove.train(tokenized_corpus)\n        in_wv = glove.wv\n        encoder = SentenceTransformer(encoder_name, device=\"cpu\")\n        print(\"Encoding task corpus.\")\n        embeddings = encoder.encode(corpus, show_progress_bar=True)\n        for model_name in topic_models:\n            if (task_name, model_name) in cache:\n                print(f\"{model_name} already done, skipping.\")\n                continue\n            print(f\"Running {model_name}.\")\n            true_n = len(set(true_labels))\n            model = topic_models[model_name](encoder=encoder, n_components=true_n)\n            start_time = time.time()\n            doc_topic_matrx = model.fit_transform(corpus, embeddings=embeddings)\n            end_time = time.time()\n            labels = getattr(model, \"labels_\", None)\n            if labels is None:\n                labels = np.argmax(doc_topic_matrx, axis=1)\n            keywords = get_keywords(model)\n            print(\"Evaluating model.\")\n            clust_scores = evaluate_clustering(true_labels, labels)\n            topic_scores = evaluate_topic_quality(keywords, ex_wv, in_wv)\n            runtime = end_time - start_time\n            res = {\n                \"encoder\": encoder_name,\n                \"task\": task_name,\n                \"model\": model_name,\n                \"auto\": \"(Auto)\" in model_name,\n                \"runtime\": runtime,\n                \"dps\": len(corpus) / runtime,\n                \"n_components\": model.components_.shape[0],\n                \"true_n\": len(set(true_labels)),\n                **clust_scores,\n                **topic_scores,\n            }\n            print(\"Results: \", res)\n            res[\"keywords\"] = keywords\n            with out_path.open(\"a\") as out_file:\n                out_file.write(json.dumps(res) + \"\\n\")\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(prog=\"Evaluate clustering.\")\n    parser.add_argument(\"embedding_model\")\n    args = parser.parse_args()\n    encoder = args.embedding_model\n    main(encoder)\n    print(\"DONE\")\n</code></pre> <p> </p> <p>For models that are able to detect the number of topics, we ran the test with this setting, this is marked as (Auto) in our tables and plots. For models, where users can set the number of topics, we also ran the benchmark setting the correct number of topics a-priori.</p>"},{"location":"benchmark/#topic-quality","title":"Topic Quality","text":"<p>It seems, that Auto models, and, in particular, Topeax, SensTopic, KeyNMF and GMM were best at generating high quality topics, as can be seen from interpretability scores. Out of non-auto models, KeyNMF, GMM, ZeroShotTM, FASTopic and SensTopic did best, though ZeroShotTM and FASTopic did not run on some of the more challenging datasets due to running out of memory.</p>"},{"location":"benchmark/#cluster-quality","title":"Cluster Quality","text":"<p>Clear winners in cluster quality were GMM, Topeax(also GMM-based) and SensTopic. FASTopic also did reasonably well when recovering gold clusters in the data.</p> Performance profile of all models on different metrics.     Top 5 models on average performance are highlighted, click on legend to show the others."},{"location":"benchmark/#computational-efficiency","title":"Computational Efficiency","text":""},{"location":"benchmark/#speed","title":"Speed","text":"<p>We recorded the amount of documents a model could process per second for each of the runs. It seems that matrix factorization approaches were fastest (\\(S^3\\), SensTopic, KeyNMF), while neural approaches (FASTopic, ZeroShotTM) the slowest. While in our investigations, SensTopic seems slower than SemanticSignalSeparation, it is important to note, that SensTopic has built-in JIT compilation capabilities, once JAX is installed, and is therefore likely to be even faster than \\(S^3\\). For more detail, see SensTopic. We plotted model speed versus performance on the interactive graph to the right. Model size represents the Fowlkes-Mallows Index.</p>"},{"location":"benchmark/#out-of-memory","title":"Out of Memory","text":"<p>While we did not record memory usage, three models stood out for being unable to complete some of the more challenging tasks on the test hardware. FASTopic failed twice, on some of the larger corpora, while Top2Vec and BERTopic had problems when trying to reduce the number of topics to a desired amount. This is likely due to the computational and memory burden of hierarchical clustering, and thus we recommend that you do not use topic reduction if you are unsure whether your hardware will be able to handle it. If you got your heart set on using FASTopic, we recommend that you get a lot of memory, and preferably a GPU too. Unfortunately neural topic modelling still takes a lot of resources to run.</p>"},{"location":"benchmark/#discovering-the-number-of-topics","title":"Discovering the Number of Topics","text":"<p>A number of methods are, in theory, able to discover the number of topics in a dataset. We have tested this, and found that this claim is rather exaggerated, especially in the case of BERTopic and Top2Vec, which consistently overestimated the number of topics, sometimes by orders of magnitude. This effect gets worse with larger corpora. Topeax was the most accurate at this task, mostly when run on larger corpora, but it was still very much off most of the time. KeyNMF and SensTopic also got reasonably close sometimes, while completely missing the mark in others.</p> <p>We conclude that this area needs a lot of improvement.</p> Model ArXivHierarchical (23) BBCNews (5) Biorxiv (26) Medrxiv (51) StackExchange (524) TweetTopic (6) TwentyNewsgroups (20) BERTopic 25 42 602 1583 2542 76 1861 KeyNMF 3 5 250 250 250 2 10 SensTopic 8 6 14 14 6 11 4 Top2Vec 18 18 405 1000 1495 49 1612 Topeax 6 8 19 23 21 8 13"},{"location":"benchmark/#cite-the-leaderboard","title":"Cite the Leaderboard","text":"<p>If you intend to reference the Topic Leaderboard in your research, please cite us:</p> <pre><code>@online{TopicLeaderboard2026,\n  title     = {The Topic Model Leaderboard},\n  author    = {M\u00e1rton Kardos},\n  year      = 2026,\n  url       = {https://x-tabdeveloping.github.io/turftopic/benchmark/},\n  urldate   = {2026-02-03}\n}\n</code></pre>"},{"location":"chinese/","title":"Topic Modeling in Chinese","text":"<p>Topic modeling in Chinese is a substantially different endeavour from doing it in Indo-European languages, such as English. To offset for the complexity introduced by this, we include a submodule in Turftopic for Chinese topic modeling.</p> <p>We will need to alter both the vectorizer and the encoder model in our topic model.</p>"},{"location":"chinese/#data","title":"Data","text":"<p>For this demonstration I will use the ThuNews corpus from the Chinese MTEB. You will need to install <code>datasets</code> to be able to download the dataset.</p> <pre><code>import itertools\nimport random\n\nfrom datasets import load_dataset\n\n# Loads the dataset\nds = load_dataset(\"C-MTEB/ThuNewsClusteringP2P\", split=\"test\")\n# Wrangles the dataset from a list of lists to a single list\ncorpus = list(itertools.chain.from_iterable(ds[\"sentences\"]))\n# Subsampling the corpus so that the script runs faster\nrandom.seed(42)\ncorpus = random.sample(corpus, 10000)\n</code></pre>"},{"location":"chinese/#tokenization","title":"Tokenization","text":"<p>Turftopic uses the jieba tokenizer for tokenizing Chinese text. We include a Chinese version of <code>CountVectorizer</code> that uses Jieba by default, and has an optionally applicable Chinese stop word list.</p> <p>You will have to install <code>jieba</code> for this to work: <pre><code>pip install turftopic[jieba]\n</code></pre></p> <pre><code>from turftopic.vectorizers.chinese import ChineseCountVectorizer\n\nvectorizer = ChineseCountVectorizer(min_df=10, stop_words=\"chinese\")\n</code></pre> <p>We also provide a sensible default which is just a <code>ChineseCountVectorizer</code> with <code>min_df=10</code> and <code>stop_words=\"chinese\"</code>:</p> <pre><code>from turftopic.chinese import default_chinese_vectorizer\n\nvectorizer = default_chinese_vectorizer()\n</code></pre>"},{"location":"chinese/#encoder","title":"Encoder","text":"<p>You will need to use a different encoder model for your topic models, since <code>all-MiniLM-L6-v2</code> does not support Chinese by default. We recommend the BGE-zh model family for this purpose.</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\"BAAI/bge-small-zh-v1.5\")\n</code></pre>"},{"location":"chinese/#defining-a-chinese-topic-model","title":"Defining a Chinese topic model","text":"<p>Once having defined these steps, you can pass these arguments when initializing your topic model.</p> <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(\n    n_components=20,\n    encoder=SentenceTransformer(\"BAAI/bge-small-zh-v1.5\"),\n    vectorizer=default_chinese_vectorizer(),\n    random_state=42,\n)\nmodel.fit(corpus)\n\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking 0 \u200b\u6d88\u606f\u200b, \u200b\u65f6\u95f4\u200b, \u200b\u79d1\u6280\u200b, \u200b\u5a92\u4f53\u62a5\u9053\u200b, \u200b\u7f8e\u56fd\u200b, \u200b\u636e\u200b, \u200b\u56fd\u5916\u200b, \u200b\u8baf\u200b, \u200b\u5ba3\u5e03\u200b, \u200b\u79f0\u200b 1 \u200b\u4f53\u80b2\u8baf\u200b, \u200b\u65b0\u6d6a\u200b, \u200b\u7403\u5458\u200b, \u200b\u7403\u961f\u200b, \u200b\u8d5b\u5b63\u200b, \u200b\u706b\u7bad\u200b, nba, \u200b\u5df2\u7ecf\u200b, \u200b\u4e3b\u573a\u200b, \u200b\u65f6\u95f4\u200b 2 \u200b\u8bb0\u8005\u200b, \u200b\u672c\u62a5\u8baf\u200b, \u200b\u6628\u65e5\u200b, \u200b\u83b7\u6089\u200b, \u200b\u65b0\u534e\u7f51\u200b, \u200b\u57fa\u91d1\u200b, \u200b\u901a\u8baf\u5458\u200b, \u200b\u91c7\u8bbf\u200b, \u200b\u7537\u5b50\u200b, \u200b\u6628\u5929\u200b 3 \u200b\u80a1\u200b, \u200b\u4e0b\u8dcc\u200b, \u200b\u4e0a\u6da8\u200b, \u200b\u9707\u8361\u200b, \u200b\u677f\u5757\u200b, \u200b\u5927\u76d8\u200b, \u200b\u80a1\u6307\u200b, \u200b\u6da8\u5e45\u200b, \u200b\u6caa\u200b, \u200b\u53cd\u5f39\u200b 4 \u200b\u50cf\u7d20\u200b, \u200b\u76f8\u673a\u200b, \u200b\u4f73\u80fd\u200b, \u200b\u955c\u5934\u200b, \u200b\u6570\u7801\u76f8\u673a\u200b, \u200b\u5355\u53cd\u200b, \u200b\u62a5\u4ef7\u200b, \u200b\u4ef7\u683c\u200b, \u200b\u5355\u53cd\u76f8\u673a\u200b, \u200b\u5c3c\u5eb7\u200b 5 \u200b\u80a1\u7968\u200b, \u200b\u8be5\u80a1\u200b, \u200b\u6295\u8d44\u8005\u200b, \u200b\u5206\u6790\u200b, \u200b\u65b0\u6d6a\u200b, \u200b\u4e2a\u4eba\u89c2\u70b9\u200b, \u200b\u4ee5\u6caa\u6df1\u200b, \u200b\u4ea4\u6613\u6240\u200b, \u200b\u6ce8\u610f\u200b, \u200b\u65b0\u95fb\u62a5\u9053\u200b 6 \u200b\u65b0\u6d6a\u200b, \u200b\u5a31\u4e50\u200b, \u200b\u8baf\u200b, \u200b\u51fa\u5e2d\u200b, \u200b\u8fd1\u65e5\u200b, \u200b\u9999\u6e2f\u200b, \u200b\u4eae\u76f8\u200b, \u200b\u5fae\u535a\u200b, \u200b\u4e3e\u884c\u200b, \u200b\u62cd\u6444\u200b 7 \u200b\u6237\u578b\u200b, \u200b\u6837\u677f\u95f4\u200b, \u200b\u623f\u4ea7\u200b, \u200b\u5747\u4ef7\u200b, \u200b\u9879\u76ee\u200b, \u200b\u5e73\u7c73\u200b, \u200b\u5165\u4f4f\u200b, \u200b\u4f4d\u4e8e\u200b, \u200b\u5730\u56fe\u641c\u7d22\u200b, \u200b\u8bba\u575b\u200b 8 \u200b\u5e02\u573a\u200b, \u200b\u7ecf\u6d4e\u200b, \u200b\u9884\u671f\u200b, \u200b\u56fd\u5185\u200b, \u200b\u8d44\u91d1\u200b, \u200b\u884c\u4e1a\u200b, \u200b\u6295\u8d44\u200b, \u200b\u4ef7\u683c\u200b, \u200b\u54c1\u724c\u200b, \u200b\u5f71\u54cd\u200b 9 \u200b\u5317\u4eac\u200b, \u200b\u65f6\u95f4\u200b, \u200b\u51cc\u6668\u200b, \u200b\u65b0\u6d6a\u200b, 8, 11, \u200b\u79d1\u6280\u200b, \u200b\u665a\u95f4\u200b, \u200b\u7f8e\u5143\u200b, 10 10 \u200b\u5c06\u200b, \u200b\u5ba3\u5e03\u200b, \u200b\u4e3e\u884c\u200b, \u200b\u8868\u793a\u200b, \u200b\u6b63\u5f0f\u200b, \u200b\u4e0e\u200b, \u200b\u8fdb\u884c\u200b, \u200b\u9884\u8ba1\u200b, \u200b\u53ef\u80fd\u200b, \u200b\u79f0\u200b 11 \u200b\u62a5\u9053\u200b, \u200b\u636e\u200b, \u200b\u9999\u6e2f\u200b, \u200b\u9999\u6e2f\u5a92\u4f53\u200b, \u200b\u65e5\u7535\u200b, \u200b\u4e2d\u65b0\u7f51\u200b, \u200b\u5a92\u4f53\u200b, \u200b\u672c\u62a5\u8bb0\u8005\u200b, \u200b\u82f1\u56fd\u200b, \u200b\u56fd\u9645\u200b 12 \u200b\u4e2d\u56fd\u200b, \u200b\u53d1\u5c55\u200b, \u200b\u56fd\u9645\u200b, \u200b\u5168\u7403\u200b, 2010, \u200b\u4e3e\u884c\u200b, \u200b\u4f01\u4e1a\u200b, \u200b\u53d1\u5e03\u200b, \u200b\u56fd\u5185\u200b, \u200b\u4e92\u8054\u7f51\u200b 13 \u200b\u516c\u53f8\u200b, \u200b\u6295\u8d44\u200b, \u200b\u57fa\u91d1\u200b, \u200b\u80a1\u4efd\u200b, \u200b\u4ebf\u5143\u200b, \u200b\u5ba3\u5e03\u200b, \u200b\u6536\u8d2d\u200b, \u200b\u8be5\u200b, \u200b\u4e0a\u5e02\u200b, \u200b\u8bc1\u5238\u200b 14 \u200b\u8003\u751f\u200b, \u200b\u8003\u8bd5\u200b, \u200b\u9ad8\u8003\u200b, \u200b\u62db\u751f\u200b, \u200b\u516c\u5e03\u200b, \u200b\u4eca\u5e74\u200b, \u200b\u5f55\u53d6\u200b, \u200b\u6559\u80b2\u200b, 2010, \u200b\u6210\u7ee9\u200b 15 \u200b\u8d22\u7ecf\u200b, \u200b\u8baf\u200b, \u200b\u65b0\u6d6a\u200b, \u200b\u76c8\u5229\u200b, \u200b\u6d88\u606f\u200b, \u200b\u8bc4\u7ea7\u200b, \u200b\u9884\u671f\u200b, \u200b\u4e1a\u7ee9\u200b, \u200b\u589e\u957f\u200b, \u200b\u6e2f\u5143\u200b 16 \u200b\u5bf9\u200b, \u200b\u4e00\u4e2a\u200b, \u200b\u65b0\u200b, \u200b\u5df2\u7ecf\u200b, \u200b\u8868\u793a\u200b, \u200b\u6210\u4e3a\u200b, \u200b\u95ee\u9898\u200b, \u200b\u8fdb\u884c\u200b, \u200b\u8fd8\u200b, \u200b\u4f1a\u200b 17 \u200b\u6bd4\u8d5b\u200b, \u200b\u4e3b\u573a\u200b, \u200b\u4f53\u80b2\u8baf\u200b, \u200b\u7ed3\u675f\u200b, \u200b\u4e00\u573a\u200b, \u200b\u5bf9\u624b\u200b, \u200b\u8054\u8d5b\u200b, \u200b\u6700\u540e\u200b, \u200b\u4e2d\u56fd\u961f\u200b, \u200b\u51a0\u519b\u200b 18 3, 10, 4, 5, \u200b\u65e5\u7535\u200b, 6, 2, \u200b\u540e\u200b, \u200b\u53d1\u751f\u200b, 7 19 \u200b\u76ee\u524d\u200b, \u200b\u5143\u200b, \u200b\u7b14\u8bb0\u672c\u200b, \u200b\u4ea7\u54c1\u200b, \u200b\u4e00\u6b3e\u200b, \u200b\u552e\u4ef7\u200b, \u200b\u6db2\u6676\u7535\u89c6\u200b, \u200b\u4ef7\u683c\u200b, \u200b\u5904\u7406\u5668\u200b, \u200b\u63a8\u51fa"},{"location":"chinese/#reference","title":"Reference","text":"<p>Kristensen-McLachlan, R. D., Hicke, R. M. M., Kardos, M., &amp; Thun\u00f8, M. (2024, October 16). Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media. arXiv.org. https://arxiv.org/abs/2410.12791 <pre><code>@misc{kristensenmclachlan2024contextkeynmfmodellingtopical,\n      title={Context is Key(NMF): Modelling Topical Information Dynamics in Chinese Diaspora Media}, \n      author={Ross Deans Kristensen-McLachlan and Rebecca M. M. Hicke and M\u00e1rton Kardos and Mette Thun\u00f8},\n      year={2024},\n      eprint={2410.12791},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2410.12791}, \n}\n</code></pre></p>"},{"location":"clustering/","title":"Clustering Topic Models","text":"<p>Clustering topic models conceptualize topic modeling as a clustering task. Essentially a topic for these models is a tightly packed group of documents in semantic space. The first contextually sensitive clustering topic model was introduced with Top2Vec, and BERTopic has also iterated on this idea.</p> <p>If you are looking for a probabilistic/soft-clustering model you should also check out GMM.</p>  Figure 1: Interactive figure to explore cluster structure in a clustering topic model."},{"location":"clustering/#how-do-clustering-models-work","title":"How do clustering models work?","text":""},{"location":"clustering/#step-1-dimensionality-reduction","title":"Step 1: Dimensionality Reduction","text":"<p>It is common practice to reduce the dimensionality of the embeddings before clustering them. This is to avoid the curse of dimensionality, an issue, which many clustering models are affected by. Dimensionality reduction by default is done with TSNE in Turftopic, but users are free to specify the model that will be used for dimensionality reduction.</p> <p>Choose a dimensionality reduction method</p> TSNE (default)UMAP (Top2Vec; BERTopic)PCA (fast) <p><pre><code>from sklearn.manifold import TSNE\nfrom turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(dimensionality_reduction=TSNE(n_components=2, metric=\"cosine\"))\n</code></pre> TSNE is a classic method for producing non-linear lower-dimensional representations of high-simensional embeddings. While it is widely used, it has many well-known issues, such as poor representation of global relations, and artificial clusters.</p> <p>Use openTSNE for better performance!</p> <p>By default, a scikit-learn implementation is used, but if you have the openTSNE package installed on your system, Turftopic will automatically use it. You can potentially speed up your clustering topic models by multiple orders of magnitude. <pre><code>pip install turftopic[opentsne]\n</code></pre></p> <pre><code>pip install umap-learn\n</code></pre> <pre><code>from umap import UMAP\nfrom turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(dimensionality_reduction=UMAP(n_components=2, metric=\"cosine\"))\n</code></pre> <p>UMAP is universally usable non-linear dimensionality reduction method and is typically the default choice for topic discovery in clustering topic models. UMAP is faster than TSNE and is also substantially better at representing global structures in your dataset.</p> <pre><code>from sklearn.decomposition import PCA\nfrom turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(dimensionality_reduction=PCA(n_components=2))\n</code></pre> <p>Principal Component Analysis is one of the most widely used dimensionality reduction techniques in machine learning. It is a linear method, that projects embeddings onto the first N principal components by the amount of variance they capture in the data. PCA is substantially faster than manifold methods, but is not as good at aiding clustering models as TSNE and UMAP.</p>"},{"location":"clustering/#step-2-document-clustering","title":"Step 2: Document Clustering","text":"<p>After the dimensionality of document embeddings is reduced, topics are discovered by clustering document-embeddings in this lower dimensional space. Turftopic is entirely clustering-model agnostic, and as such, any type of model may be used.</p> <p>Choose a clustering method</p> HDBSCAN (default)KMeans (fast) <pre><code>from sklearn.cluster import HDBSCAN\nfrom turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(clustering=HDBSCAN())\n</code></pre> <p>HDBSCAN is a density-based clustering method, that can find clusters with varying densities. It can find the number of clusters in the data, and can also find outliers. While HDBSCAN has many advantageous properties, it can be hard to make an informed choice about its hyperparameters.</p> <pre><code>from sklearn.cluster import KMeans\nfrom turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(clustering=KMeans(n_clusters=10))\n</code></pre> <p>The KMeans algorithm finds clusters by locating a prespecified number of mean vectors that minimize square distance of embeddings in a cluster to their mean. KMeans is a very fast algorithm, but makes very strong assumptions about cluster shapes, can't detect outliers and you have to specify the number of clusters prior to model fitting.</p>"},{"location":"clustering/#step-3-calculate-term-importance-scores","title":"Step 3: Calculate term importance scores","text":"<p>Clustering topic models rely on post-hoc term importance estimation, meaning that topic descriptions are calculated based on already discovered clusters. Multiple methods are available in Turftopic for estimating words'/phrases' importance scores for topics. You can manipulate how these scores are calculated by changing the <code>feature_importance</code> parameter of your topic models. By and large there are two types of methods that can be used for importance estimation:</p> <ol> <li>Lexical methods, which estimate term importance solely based on word counts in each cluster:<ul> <li>Generally faster, since the vocabulary does not need to be encoded.</li> <li>Can capture more particular word use.</li> <li>Usually cover the topics' content better.</li> </ul> </li> <li>Semantic methods, which estimate term importance using the semantic space of the model:<ul> <li>They typically produce cleaner and more specific topics.</li> <li>Can be used in a multilingual context.</li> <li>Generally less sensitive to stop- and junk words.</li> </ul> </li> </ol> Importance method Type Description Advantages <code>soft-c-tf-idf</code> (default) Lexical A c-tf-idf mehod that can interpret soft cluster assignments. Can interpret soft cluster assignment in models like Gaussian Mixtures, less sensitive to stop words than vanilla c-tf-idf. <code>fighting-words</code> (NEW) Lexical Compute word importance based on cluster differences using the Fightin' Words algorithm by Monroe et al. A theoretically motivated probabilistic model that was explicitly designed for discovering lexical differences in groups of text. See Fightin' Words paper. <code>npmi</code> (NEW) Lexical Estimate term importance from mutual information between cluster labels and term occurrence. Theoretically motivated, fast, and usually produces clean topics. <code>c-tf-idf</code> Lexical Compute how unique terms are in a cluster with a tf-idf style weighting scheme. This is the default in BERTopic. Very fast, easy to understand and is not affected by cluster shape. <code>centroid</code> Semantic Word importance based on words' proximity to cluster centroid vectors. This is the default in Top2Vec. Produces clean topics, easily interpretable. <code>linear</code> (NEW, EXPERIMENTAL) Semantic Project words onto the parameter vectors of a linear classifier (LDA). Topic differences are measured in embedding space and are determined by predictive power, and are therefore accurate and clean. <p>Choose a term importance estimation method</p> soft-c-TF-IDF (Default)c-TF-IDF (BERTopic)Centroid Proximity (Top2Vec)Fighting' WordsLinear ProbingNPMI <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(feature_importance=\"soft-c-tf-idf\")\n</code></pre> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(feature_importance=\"c-tf-idf\")\n</code></pre> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(feature_importance=\"centroid\")\n</code></pre> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(feature_importance=\"fighting-words\")\n</code></pre> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(feature_importance=\"linear\")\n</code></pre> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(feature_importance=\"npmi\")\n</code></pre> <p>You can also choose to recalculate term importances with a different method after fitting the model:</p> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel().fit(corpus)\nmodel.estimate_components(feature_importance=\"centroid\")\nmodel.estimate_components(feature_importance=\"soft-c-tf-idf\")\n</code></pre>"},{"location":"clustering/#formula","title":"Formula:","text":"<ul> <li>Let \\(X\\) be the document term matrix where each element (\\(X_{ij}\\)) corresponds with the number of times word \\(j\\) occurs in a document \\(i\\).</li> <li>Estimate weight of term \\(j\\) for topic \\(z\\):  \\(tf_{zj} = \\frac{t_{zj}}{w_z}\\), where  \\(t_{zj} = \\sum_{i \\in z} X_{ij}\\) is the number of occurrences of a word in a topic and  \\(w_{z}= \\sum_{j} t_{zj}\\) is all words in the topic </li> <li>Estimate inverse document/topic frequency for term \\(j\\): \\(idf_j = log(\\frac{N}{\\sum_z |t_{zj}|})\\), where \\(N\\) is the total number of documents.</li> <li>Calculate importance of term \\(j\\) for topic \\(z\\):  \\(Soft-c-TF-IDF{zj} = tf_{zj} \\cdot idf_j\\)</li> </ul>"},{"location":"clustering/#formula_1","title":"Formula:","text":"<ul> <li>Let \\(X\\) be the document term matrix where each element (\\(X_{ij}\\)) corresponds with the number of times word \\(j\\) occurs in a document \\(i\\).</li> <li>\\(tf_{zj} = \\frac{t_{zj}}{w_z}\\), where  \\(t_{zj} = \\sum_{i \\in z} X_{ij}\\) is the number of occurrences of a word in a topic and  \\(w_{z}= \\sum_{j} t_{zj}\\) is all words in the topic </li> <li>Estimate inverse document/topic frequency for term \\(j\\): \\(idf_j = log(1 + \\frac{A}{\\sum_z |t_{zj}|})\\), where \\(A = \\frac{\\sum_z \\sum_j t_{zj}}{Z}\\) is the average number of words per topic, and \\(Z\\) is the number of topics.</li> <li>Calculate importance of term \\(j\\) for topic \\(z\\):  \\(c-TF-IDF{zj} = tf_{zj} \\cdot idf_j\\)</li> </ul>"},{"location":"clustering/#bertopic-and-top2vec-in-turftopic","title":"BERTopic and Top2Vec in Turftopic","text":"<p>Since BERTopic and Top2Vec are also just clustering topic models with specific characteristics, you can easily use the same models in Turftopic. We have added convenience classes, that inherit from <code>ClusteringTopicModel</code> that make it very easy to create a BERTopic or Top2Vec model in the library.</p> <pre><code>pip install turftopic[umap-learn]\n</code></pre> <p>Create BERTopic and Top2Vec models</p> BERTopicTop2Vec <pre><code>from turftopic import BERTopic\n\nberttopic = BERTopic()\nberttopic.fit(corpus)\n</code></pre> <pre><code>from turftopic import Top2Vec\n\ntop2vec = Top2Vec()\ntop2vec.fit(corpus)\n</code></pre> <p>Are these different from the original?</p> <p>Theoretically the model descriptions above should result in the same behaviour as the other two packages, but there might be minor changes in implementation.</p>"},{"location":"clustering/#hierarchical-topic-merging","title":"Hierarchical Topic Merging","text":"<p>A weakness of clustering approaches based on density-based clustering methods, is that all too frequently they find a very large number of topics. To limit the number of topics in a topic model you can hierarchically merge topics, until you get the desired number. Turftopic allows you to use a number of popular methods for merging topics in clustering models.</p> <p>Choose a topic reduction method</p> Agglomerative Clustering (BERTopic)Smallest -&gt; Closest (Top2Vec) <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(n_reduce_to=10, reduction_method=\"average\")\n# or \nmodel.reduce_topics(10, reduction_method=\"single\", metric=\"cosine\")\n</code></pre> <p>Topics discovered by a clustering model can be merged using agglomerative clustering. For a detailed discussion of linkage methods and hierarchical clustering, consult SciPy's documentation. All linkage methods compatible with SciPy can be used as topic reduction methods in Turftopic.</p> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(n_reduce_to=10, reduction_method=\"smallest\")\n# or \nmodel.reduce_topics(10, reduction_method=\"smallest\", metric=\"cosine\")\n</code></pre> <p>The approach used in the Top2Vec package is to always merge the smallest topic into the one closest to it (except the outlier-cluster) until the number of topics is down to the desired amount. This approach is remarkably fast, and usually quite effective, since it doesn't require computing full linkages.</p> <p>As such, all clustering models have a <code>hierarchy</code> property, with which you can explore the topic hierarchy discovered by your models. For a detailed discussion of hierarchical modeling, check out the Hierarchical modeling page.</p> <pre><code>print(model.hierarchy)\n</code></pre> Root:  \u251c\u2500\u2500 -1: documented, obsolete, et4000, concerns, dubious, embedded, hardware, xfree86, alternative, seeking \u251c\u2500\u2500 20: hitter, pitching, batting, hitters, pitchers, fielder, shortstop, inning, baseman, pitcher \u251c\u2500\u2500 284: nhl, goaltenders, canucks, sabres, hockey, bruins, puck, oilers, canadiens, flyers \u2502   \u251c\u2500\u2500 242: sportschannel, espn, nbc, nhl, broadcasts, broadcasting, broadcast, mlb, cbs, cbc \u2502   \u2502   \u251c\u2500\u2500 171: stadium, tickets, mlb, ticket, sportschannel, mets, inning, nationals, schedule, cubs \u2502   \u2502   \u2502   \u2514\u2500\u2500 ... \u2502   \u2502   \u2514\u2500\u2500 21: sportschannel, nbc, espn, nhl, broadcasting, broadcasts, broadcast, hockey, cbc, cbs \u2502   \u2514\u2500\u2500 236: nhl, goaltenders, canucks, sabres, puck, oilers, andreychuk, bruins, goaltender, leafs ...  <p>You can also manually merge topics by using the <code>join_topics()</code> method of cluster hierarchies.</p> <pre><code># Joins topics 0,1 and 2 and creates a merged topics with ID 4\nmodel.hierarchy.join_topics([0, 1, 2], joint_id=4)\n</code></pre> <p>If you want to reset topics to their original state, you can call <code>reset_topics()</code> <pre><code>model.reset_topics()\n</code></pre></p>"},{"location":"clustering/#dynamic-topic-modeling","title":"Dynamic Topic Modeling","text":"<p>Clustering models are also capable of dynamic topic modeling. This happens by fitting a clustering model over the entire corpus, as we expect that there is only one semantic model generating the documents.</p> <p>For a detailed discussion, see Dynamic Models. <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel().fit_dynamic(corpus, timestamps=ts, bins=10)\nmodel.print_topics_over_time()\n</code></pre></p>"},{"location":"clustering/#semi-supervised-topic-modeling","title":"Semi-supervised Topic Modeling","text":"<p>Some dimensionality reduction methods are capable of designing features that are effective at predicting class labels. This way, you can provide a supervisory signal, but also let the model discover new topics that you have not specified.</p> <p>Warning</p> <p>TSNE, the default dimensionality reduction method in Turftopic is not capable of semi-supervised modelling. You will have to use a different algorithm.</p> <p>Use a dimensionality reduction method for semi-supervised modeling.</p> with UMAPwith Linear Discriminant Analysis <pre><code>pip install turftopic[umap-learn]\n</code></pre> <pre><code>from umap import UMAP\nfrom turftopic import ClusteringTopicModel\n\ncorpus: list[str] = [...]\n\n# UMAP can also understand missing class labels if you only have them on some examples\n# Specify these with -1 or NaN labels\nlabels: list[int] = [0, 2, -1, -1, 0, 0...]\n\nmodel = ClusteringTopicModel(dimensionality_reduction=UMAP())\nmodel.fit(corpus, y=labels)\n</code></pre> <pre><code>from sklearn.discriminant_analysis import LinearDisciminantAnalysis\nfrom turftopic import ClusteringTopicModel\n\ncorpus: list[str] = [...]\nlabels: list[int] = [...]\n\nmodel = ClusteringTopicModel(dimensionality_reduction=LinearDisciminantAnalysis(n_components=5))\nmodel.fit(corpus, y=labels)\n</code></pre>"},{"location":"clustering/#visualization","title":"Visualization","text":"<p>You can interactively explore clusters using datamapplot directly in Turftopic! You will first have to install <code>datamapplot</code> for this to work:</p> <pre><code>pip install turftopic[datamapplot]\n</code></pre> <pre><code>from turftopic import ClusteringTopicModel\nfrom turftopic.analyzers import OpenAIAnalyzer\n\nmodel = ClusteringTopicModel(feature_importance=\"centroid\").fit(corpus)\n\nanalyzer = OpenAIAnalyzer(\"gpt-5-nano\")\nanalysis_res = model.analyze_topics(analyzer)\n\nfig = model.plot_clusters_datamapplot()\nfig.save(\"clusters_visualization.html\")\nfig\n</code></pre> <p>See Figure 1</p> <p>Info</p> <p>If you are not running Turftopic from a Jupyter notebook, make sure to call <code>fig.show()</code>. This will open up a new browser tab with the interactive figure.</p>"},{"location":"clustering/#api-reference","title":"API Reference","text":""},{"location":"clustering/#turftopic.models.cluster.ClusteringTopicModel","title":"<code>turftopic.models.cluster.ClusteringTopicModel</code>","text":"<p>             Bases: <code>ContextualModel</code>, <code>ClusterMixin</code>, <code>DynamicTopicModel</code>, <code>MultimodalModel</code></p> <p>Topic models, which assume topics to be clusters of documents in semantic space. Models also include a dimensionality reduction step to aid clustering.</p> <pre><code>from turftopic import ClusteringTopicModel\nfrom sklearn.cluster import HDBSCAN\nimport umap\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\n# Construct a Top2Vec-like model\nmodel = ClusteringTopicModel(\n    dimensionality_reduction=umap.UMAP(5),\n    clustering=HDBSCAN(),\n    feature_importance=\"centroid\"\n).fit(corpus)\nmodel.print_topics()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>encoder</code> <code>Union[Encoder, str, MultimodalEncoder]</code> <p>Model to encode documents/terms, all-MiniLM-L6-v2 is the default.</p> <code>'sentence-transformers/all-MiniLM-L6-v2'</code> <code>vectorizer</code> <code>Optional[CountVectorizer]</code> <p>Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.</p> <code>None</code> <code>dimensionality_reduction</code> <code>Optional[TransformerMixin]</code> <p>Dimensionality reduction step to run before clustering. Defaults to TSNE with cosine distance. To imitate the behavior of BERTopic or Top2Vec you should use UMAP.</p> <code>None</code> <code>clustering</code> <code>Optional[ClusterMixin]</code> <p>Clustering method to use for finding topics. Defaults to OPTICS with 25 minimum cluster size. To imitate the behavior of BERTopic or Top2Vec you should use HDBSCAN.</p> <code>None</code> <code>feature_importance</code> <code>WordImportance</code> <p>Method for estimating term importances. 'centroid' uses distances from cluster centroid similarly to Top2Vec. 'c-tf-idf' uses BERTopic's c-tf-idf. 'soft-c-tf-idf' uses Soft c-TF-IDF from GMM, the results should be very similar to 'c-tf-idf'. 'npmi' uses normalized pointwise information between clusters and words. 'linear' calculates most predictive directions in embedding space and projects words onto them. 'fighting-words' calculates word importances based on the Fighting Words algorithm from Monroe et al.</p> <code>'soft-c-tf-idf'</code> <code>n_reduce_to</code> <code>Optional[int]</code> <p>Number of topics to reduce topics to. The specified reduction method will be used to merge them. By default, topics are not merged.</p> <code>None</code> <code>reduction_method</code> <code>LinkageMethod</code> <p>Method used for hierarchically merging topics. Could be \"smallest\", which is Top2Vec's default merging strategy, or any of the linkage methods listed in SciPy's documentation</p> <code>'average'</code> <code>reduction_distance_metric</code> <code>DistanceMetric</code> <p>Distance metric to use for hierarchical topic reduction.</p> <code>'cosine'</code> <code>reduction_topic_representation</code> <code>TopicRepresentation</code> <p>Topic representation used for hierarchical clustering. If 'component' the topic-word importance scores will be used as topic vectors, (this is how it's done in BERTopic) if 'centroid' the centroid vectors of clusters will be used as topic vectors (Top2Vec).</p> <code>'component'</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state to use so that results are exactly reproducible.</p> <code>None</code> Source code in <code>turftopic/models/cluster.py</code> <pre><code>class ClusteringTopicModel(\n    ContextualModel, ClusterMixin, DynamicTopicModel, MultimodalModel\n):\n    \"\"\"Topic models, which assume topics to be clusters of documents\n    in semantic space.\n    Models also include a dimensionality reduction step to aid clustering.\n\n    ```python\n    from turftopic import ClusteringTopicModel\n    from sklearn.cluster import HDBSCAN\n    import umap\n\n    corpus: list[str] = [\"some text\", \"more text\", ...]\n\n    # Construct a Top2Vec-like model\n    model = ClusteringTopicModel(\n        dimensionality_reduction=umap.UMAP(5),\n        clustering=HDBSCAN(),\n        feature_importance=\"centroid\"\n    ).fit(corpus)\n    model.print_topics()\n    ```\n\n    Parameters\n    ----------\n    encoder: str or SentenceTransformer\n        Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n    vectorizer: CountVectorizer, default None\n        Vectorizer used for term extraction.\n        Can be used to prune or filter the vocabulary.\n    dimensionality_reduction: TransformerMixin, default None\n        Dimensionality reduction step to run before clustering.\n        Defaults to TSNE with cosine distance.\n        To imitate the behavior of BERTopic or Top2Vec you should use UMAP.\n    clustering: ClusterMixin, default None\n        Clustering method to use for finding topics.\n        Defaults to OPTICS with 25 minimum cluster size.\n        To imitate the behavior of BERTopic or Top2Vec you should use HDBSCAN.\n    feature_importance: WordImportance, default 'soft-c-tf-idf'\n        Method for estimating term importances.\n        'centroid' uses distances from cluster centroid similarly\n        to Top2Vec.\n        'c-tf-idf' uses BERTopic's c-tf-idf.\n        'soft-c-tf-idf' uses Soft c-TF-IDF from GMM, the results should\n        be very similar to 'c-tf-idf'.\n        'npmi' uses normalized pointwise information between clusters and words.\n        'linear' calculates most predictive directions in embedding space and projects\n        words onto them.\n        'fighting-words' calculates word importances based on the Fighting Words\n        algorithm from Monroe et al.\n    n_reduce_to: int, default None\n        Number of topics to reduce topics to.\n        The specified reduction method will be used to merge them.\n        By default, topics are not merged.\n    reduction_method: LinkageMethod, default 'average'\n        Method used for hierarchically merging topics.\n        Could be \"smallest\", which is Top2Vec's default merging strategy, or\n        any of the linkage methods listed in [SciPy's documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)\n    reduction_distance_metric: DistanceMetric, default 'cosine'\n        Distance metric to use for hierarchical topic reduction.\n    reduction_topic_representation: {'component', 'centroid'}, default 'component'\n        Topic representation used for hierarchical clustering.\n        If 'component' the topic-word importance scores will be used as topic vectors, (this is how it's done in BERTopic)\n        if 'centroid' the centroid vectors of clusters will be used as topic vectors (Top2Vec).\n    random_state: int, default None\n        Random state to use so that results are exactly reproducible.\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: Union[\n            Encoder, str, MultimodalEncoder\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        dimensionality_reduction: Optional[TransformerMixin] = None,\n        clustering: Optional[ClusterMixin] = None,\n        feature_importance: WordImportance = \"soft-c-tf-idf\",\n        n_reduce_to: Optional[int] = None,\n        reduction_method: LinkageMethod = \"average\",\n        reduction_distance_metric: DistanceMetric = \"cosine\",\n        reduction_topic_representation: TopicRepresentation = \"component\",\n        random_state: Optional[int] = None,\n    ):\n        self.encoder = encoder\n        self.random_state = random_state\n        if feature_importance not in VALID_WORD_IMPORTANCE:\n            raise ValueError(\n                f\"feature_importance must be one of {VALID_WORD_IMPORTANCE} got {feature_importance} instead.\"\n            )\n        if reduction_method not in VALID_LINKAGE_METHODS:\n            raise ValueError(\n                f\"Topic reduction method has to be one of: {VALID_LINKAGE_METHODS}, but got {reduction_method} instead.\"\n            )\n        if reduction_distance_metric not in VALID_DISTANCE_METRICS:\n            raise ValueError(\n                f\"Distance metric should be one of: {VALID_DISTANCE_METRICS}, but got {reduction_distance_metric} instead.\"\n            )\n        if reduction_topic_representation not in VALID_TOPIC_REPRESENTATIONS:\n            raise ValueError(\n                f\"Topic representation should be one of: {VALID_TOPIC_REPRESENTATIONS}, but got {reduction_topic_representation} instead.\"\n            )\n        if isinstance(encoder, int):\n            raise TypeError(integer_message)\n        if isinstance(encoder, str):\n            self.encoder_ = SentenceTransformer(encoder)\n        else:\n            self.encoder_ = encoder\n        self.validate_encoder()\n        if vectorizer is None:\n            self.vectorizer = default_vectorizer()\n        else:\n            self.vectorizer = vectorizer\n        if clustering is None:\n            self.clustering = HDBSCAN(\n                min_samples=10,\n                min_cluster_size=25,\n            )\n        else:\n            self.clustering = clustering\n        if dimensionality_reduction is None:\n            self.dimensionality_reduction = TSNE(\n                n_components=2,\n                metric=\"cosine\",\n                perplexity=15,\n                random_state=random_state,\n            )\n        else:\n            self.dimensionality_reduction = dimensionality_reduction\n        self.feature_importance = feature_importance\n        self.reduction_distance_metric = reduction_distance_metric\n        self.reduction_topic_representation = reduction_topic_representation\n        self.n_reduce_to = n_reduce_to\n        self.reduction_method = reduction_method\n\n    @property\n    def topic_representations(self) -&gt; np.ndarray:\n        if self.reduction_topic_representation == \"component\":\n            return self.components_\n        else:\n            return self._calculate_topic_vectors()\n\n    def _calculate_topic_vectors(\n        self,\n        is_in_slice: Optional[np.ndarray] = None,\n        classes: Optional[np.ndarray] = None,\n        embeddings: Optional[np.ndarray] = None,\n        labels: Optional[np.ndarray] = None,\n    ) -&gt; np.ndarray:\n        if classes is None:\n            classes = self.classes_\n        if embeddings is None:\n            embeddings = self.embeddings\n        if labels is None:\n            labels = self.labels_\n        label_to_idx = {label: idx for idx, label in enumerate(classes)}\n        n_topics = len(classes)\n        n_dims = embeddings.shape[1]\n        topic_vectors = np.full((n_topics, n_dims), np.nan)\n        for label in np.unique(labels):\n            doc_idx = labels == label\n            if is_in_slice is not None:\n                doc_idx = doc_idx &amp; is_in_slice\n            topic_vectors[label_to_idx[label], :] = np.mean(\n                embeddings[doc_idx], axis=0\n            )\n        return topic_vectors\n\n    def estimate_components(\n        self, feature_importance: Optional[WordImportance] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Estimates feature importances based on a fitted clustering.\n\n        Parameters\n        ----------\n        feature_importance: WordImportance, default None\n            Method for estimating term importances.\n            'centroid' uses distances from cluster centroid similarly\n            to Top2Vec.\n            'c-tf-idf' uses BERTopic's c-tf-idf.\n            'soft-c-tf-idf' uses Soft c-TF-IDF from GMM, the results should\n            be very similar to 'c-tf-idf'.\n            'npmi' uses normalized pointwise mutual information between clusters and words.\n            'linear' calculates most predictive directions in embedding space and projects\n            words onto them.\n            'fighting-words' calculates word importances based on the Fighting Words\n            algorithm from Monroe et al.\n\n        Returns\n        -------\n        ndarray of shape (n_components, n_vocab)\n            Topic-term matrix.\n        \"\"\"\n        if feature_importance is not None:\n            if feature_importance not in VALID_WORD_IMPORTANCE:\n                raise ValueError(\n                    f\"feature_importance must be one of {VALID_WORD_IMPORTANCE} got {feature_importance} instead.\"\n                )\n            self.feature_importance = feature_importance\n        self.hierarchy.estimate_components()\n        doc_topic_matrix = safe_binarize(self.labels_, classes=self.classes_)\n        if feature_importance == \"c-tf-idf\":\n            _, self._idf_diag = ctf_idf(\n                doc_topic_matrix,\n                self.doc_term_matrix,\n                return_idf=True,\n            )\n        if feature_importance == \"soft-c-tf-idf\":\n            _, self._idf_diag = soft_ctf_idf(\n                doc_topic_matrix,\n                self.doc_term_matrix,\n                return_idf=True,\n            )\n        return self.components_\n\n    def reduce_topics(\n        self,\n        n_reduce_to: int,\n        reduction_method: Optional[LinkageMethod] = None,\n        metric: Optional[DistanceMetric] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Reduces the clustering to the desired amount with the given method.\n\n        Parameters\n        ----------\n        n_reduce_to: int, default None\n            Number of topics to reduce topics to.\n            The specified reduction method will be used to merge them.\n            By default, topics are not merged.\n        reduction_method: LinkageMethod, default None\n            Method used for hierarchically merging topics.\n            Could be \"smallest\", which is Top2Vec's default merging strategy, or\n            any of the linkage methods listed in [SciPy's documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)\n        reduction_distance_metric: DistanceMetric, default None\n            Distance metric to use for hierarchical topic reduction.\n\n        Returns\n        -------\n        ndarray of shape (n_documents)\n            New cluster labels for documents.\n        \"\"\"\n        if not hasattr(self, \"original_labels_\"):\n            self.original_labels_ = self.labels_\n            self.original_names_ = self.topic_names\n            self.original_classes_ = self.classes_\n        if reduction_method is None:\n            reduction_method = self.reduction_method\n        if metric is None:\n            metric = self.reduction_distance_metric\n        self.hierarchy.reduce_topics(\n            n_reduce_to, method=reduction_method, metric=metric\n        )\n        return self.labels_\n\n    def reset_topics(self):\n        \"\"\"Resets topics to the original cllustering.\"\"\"\n        original_labels = getattr(self, \"original_labels_\", None)\n        if original_labels is None:\n            warnings.warn(\"Topics have never been reduced, nothing to reset.\")\n        else:\n            self.hierarchy = ClusterNode.create_root(\n                self, labels=self.original_labels_\n            )\n            self.topic_names_ = self.original_names_\n\n    @property\n    def classes_(self):\n        try:\n            return self.hierarchy.classes_\n        except AttributeError as e:\n            raise AttributeError(\n                \"Model has not been fitted yet, and doesn't have classes_\"\n            ) from e\n\n    @property\n    def components_(self):\n        try:\n            return self.hierarchy.components_\n        except AttributeError as e:\n            raise AttributeError(\n                \"Model has not been fitted yet, and doesn't have components_\"\n            ) from e\n\n    @property\n    def labels_(self):\n        try:\n            return self.hierarchy.labels_\n        except AttributeError as e:\n            raise AttributeError(\n                \"Model has not been fitted yet, and doesn't have labels_\"\n            ) from e\n\n    @property\n    def document_topic_matrix(self):\n        return safe_binarize(self.labels_, classes=self.classes_)\n\n    def join_topics(\n        self, to_join: Sequence[int], joint_id: Optional[int] = None\n    ):\n        \"\"\"Joins the given topics in the cluster hierarchy to a single topic.\n\n        Parameters\n        ----------\n        to_join: Sequence of int\n            Topics to join together by ID.\n        joint_id: int, default None\n            New ID for the joint cluster.\n            Default is the smallest ID of the topics to join.\n        \"\"\"\n        self.hierarchy.join_topics(to_join, joint_id=joint_id)\n\n    def fit_predict(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Fits model and predicts cluster labels for all given documents.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        y: None\n            Ignored, when the dimensionality reduction is TSNE (the default),\n            in case of a dimensionality reduction that can utilize labels,\n            you can pass labels to the model to inform the clustering process.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n\n        Returns\n        -------\n        ndarray of shape (n_documents)\n            Cluster label for all documents (-1 for outliers)\n        \"\"\"\n        console = Console()\n        with console.status(\"Fitting model\") as status:\n            if embeddings is None:\n                status.update(\"Encoding documents\")\n                embeddings = self.encode_documents(raw_documents)\n                console.log(\"Encoding done.\")\n            self.embeddings = embeddings\n            status.update(\"Extracting terms\")\n            self.doc_term_matrix = self.vectorizer.fit_transform(raw_documents)\n            console.log(\"Term extraction done.\")\n            status.update(\"Reducing Dimensionality\")\n            # If y is specified, we pass it to the dimensionality\n            # reduction method as supervisory signal\n            if y is not None:\n                y = factorize_labels(y)\n            self.reduced_embeddings = (\n                self.dimensionality_reduction.fit_transform(embeddings, y=y)\n            )\n            console.log(\"Dimensionality reduction done.\")\n            status.update(\"Clustering documents\")\n            labels = self.clustering.fit_predict(self.reduced_embeddings)\n            console.log(\"Clustering done.\")\n            status.update(\"Estimating parameters.\")\n            # Initializing hierarchy\n            self.hierarchy = ClusterNode.create_root(self, labels=labels)\n            doc_topic_matrix = safe_binarize(\n                self.labels_, classes=self.classes_\n            )\n            if self.feature_importance == \"c-tf-idf\":\n                _, self._idf_diag = ctf_idf(\n                    doc_topic_matrix,\n                    self.doc_term_matrix,\n                    return_idf=True,\n                )\n            if self.feature_importance == \"soft-c-tf-idf\":\n                _, self._idf_diag = soft_ctf_idf(\n                    doc_topic_matrix,\n                    self.doc_term_matrix,\n                    return_idf=True,\n                )\n            console.log(\"Parameter estimation done.\")\n            if self.n_reduce_to is not None:\n                n_topics = self.classes_.shape[0]\n                status.update(\n                    f\"Reducing topics from {n_topics} to {self.n_reduce_to}\"\n                )\n                self.reduce_topics(\n                    self.n_reduce_to,\n                    self.reduction_method,\n                    self.reduction_distance_metric,\n                )\n                console.log(\n                    f\"Topic reduction done from {n_topics} to {self.n_reduce_to}.\"\n                )\n        console.log(\"Model fitting done.\")\n        self.top_documents = self.get_top_documents(\n            raw_documents=raw_documents,\n            document_topic_matrix=self.transform(\n                raw_documents, embeddings=self.embeddings\n            ),\n        )\n        return self.labels_\n\n    def fit_transform(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ):\n        self.fit_predict(raw_documents, y, embeddings)\n        embeddings = (\n            embeddings\n            if embeddings is not None\n            else getattr(self, \"embeddings\", None)\n        )\n        document_topic_matrix = self.transform(\n            raw_documents, embeddings=embeddings\n        )\n        return document_topic_matrix\n\n    def fit_transform_multimodal(\n        self,\n        raw_documents: list[str],\n        images: list[ImageRepr],\n        y=None,\n        embeddings: Optional[MultimodalEmbeddings] = None,\n    ) -&gt; np.ndarray:\n        self.validate_embeddings(embeddings)\n        self.multimodal_embeddings = embeddings\n        if self.multimodal_embeddings is None:\n            self.multimodal_embeddings = self.encode_multimodal(\n                raw_documents, images\n            )\n        doc_topic_matrix = self.fit_transform(\n            raw_documents,\n            embeddings=self.multimodal_embeddings[\"document_embeddings\"],\n            y=y,\n        )\n        self.image_topic_matrix = self.transform(\n            raw_documents,\n            embeddings=self.multimodal_embeddings[\"image_embeddings\"],\n        )\n        self.top_images: list[list[Image.Image]] = self.collect_top_images(\n            images, self.image_topic_matrix\n        )\n        return doc_topic_matrix\n\n    def estimate_temporal_components(\n        self,\n        time_labels,\n        time_bin_edges,\n        feature_importance: Optional[WordImportance] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Estimates temporal components based on a fitted topic model.\n\n        Parameters\n        ----------\n        feature_importance: WordImportance, default None\n            Method for estimating term importances.\n            'centroid' uses distances from cluster centroid similarly\n            to Top2Vec.\n            'c-tf-idf' uses BERTopic's c-tf-idf.\n            'soft-c-tf-idf' uses Soft c-TF-IDF from GMM, the results should\n            be very similar to 'c-tf-idf'.\n            'npmi' uses normalized pointwise information between clusters and words.\n            'linear' calculates most predictive directions in embedding space and projects\n            words onto them.\n\n        Returns\n        -------\n        ndarray of shape (n_time_bins, n_components, n_vocab)\n            Temporal topic-term matrix.\n        \"\"\"\n        if getattr(self, \"components_\", None) is None:\n            raise NotFittedError(\n                \"The model has not been fitted yet, please fit the model before estimating temporal components.\"\n            )\n        if feature_importance is None:\n            feature_importance = self.feature_importance\n        n_comp, n_vocab = self.components_.shape\n        self.time_bin_edges = time_bin_edges\n        n_bins = len(self.time_bin_edges) - 1\n        self.temporal_components_ = np.full(\n            (n_bins, n_comp, n_vocab),\n            np.nan,\n            dtype=self.components_.dtype,\n        )\n        self.temporal_importance_ = np.zeros((n_bins, n_comp))\n        for i_timebin in np.unique(time_labels):\n            topic_importances = self.document_topic_matrix[\n                time_labels == i_timebin\n            ].sum(axis=0)\n            if not topic_importances.sum() == 0:\n                topic_importances = topic_importances / topic_importances.sum()\n            self.temporal_importance_[i_timebin, :] = topic_importances\n            t_dtm = self.doc_term_matrix[time_labels == i_timebin]\n            t_doc_topic = self.document_topic_matrix[time_labels == i_timebin]\n            if feature_importance == \"c-tf-idf\":\n                self.temporal_components_[i_timebin], _ = ctf_idf(\n                    t_doc_topic, t_dtm, return_idf=True\n                )\n            elif feature_importance == \"soft-c-tf-idf\":\n                self.temporal_components_[i_timebin], _ = soft_ctf_idf(\n                    t_doc_topic, t_dtm, return_idf=True\n                )\n            elif feature_importance == \"npmi\":\n                self.temporal_components_[i_timebin] = npmi(t_doc_topic, t_dtm)\n            elif feature_importance == \"fighting-words\":\n                self.temporal_components_[i_timebin] = fighting_words(\n                    t_doc_topic, t_dtm\n                )\n            elif feature_importance in [\"centroid\", \"linear\"]:\n                t_topic_vectors = self._calculate_topic_vectors(\n                    is_in_slice=time_labels == i_timebin,\n                )\n                if feature_importance == \"centroid\":\n                    components = cluster_centroid_distance(\n                        t_topic_vectors,\n                        self.vocab_embeddings,\n                    )\n                    mask_terms = t_dtm.sum(axis=0).astype(np.float64)\n                    mask_terms = np.squeeze(np.asarray(mask_terms))\n                    components[:, mask_terms == 0] = np.nan\n                    self.temporal_components_[i_timebin] = components\n                else:\n                    self.temporal_components_[i_timebin] = linear_classifier(\n                        t_doc_topic,\n                        embeddings=self.embeddings,\n                        vocab_embedding=self.vocab_embeddings,\n                    )\n        return self.temporal_components_\n\n    def fit_transform_dynamic(\n        self,\n        raw_documents,\n        timestamps: list[datetime],\n        embeddings: Optional[np.ndarray] = None,\n        bins: Union[int, list[datetime]] = 10,\n    ):\n        time_labels, self.time_bin_edges = self.bin_timestamps(\n            timestamps, bins\n        )\n        if hasattr(self, \"components_\"):\n            doc_topic_matrix = safe_binarize(\n                self.labels_, classes=self.classes_\n            )\n        else:\n            doc_topic_matrix = self.fit_transform(\n                raw_documents, embeddings=embeddings\n            )\n        n_comp, n_vocab = self.components_.shape\n        n_bins = len(self.time_bin_edges) - 1\n        self.temporal_components_ = np.zeros(\n            (n_bins, n_comp, n_vocab), dtype=doc_topic_matrix.dtype\n        )\n        self.temporal_importance_ = np.zeros((n_bins, n_comp))\n        if embeddings is None:\n            embeddings = self.encode_documents(raw_documents)\n        self.embeddings = embeddings\n        self.estimate_temporal_components(\n            time_labels, self.time_bin_edges, self.feature_importance\n        )\n        return doc_topic_matrix\n\n    @staticmethod\n    def _labels_to_indices(labels, classes):\n        n_classes = len(classes)\n        class_to_index = dict(zip(classes, np.arange(n_classes)))\n        return np.array([class_to_index[label] for label in labels])\n\n    def plot_clusters_datamapplot(\n        self,\n        dimensions: tuple[int, int] = (0, 1),\n        hover_text: Optional[list[str]] = None,\n        **kwargs,\n    ):\n        \"\"\"Creates an interactive browser plot of the topics in your data using datamapplot.\n\n        Parameters\n        ----------\n        dimensions: tuple (x_coord, y_coord)\n            Indicates which dimensions of the reduced embeddings to use for displaying.\n            Defaults to the first two dimensions.\n        hover_text: list of str, optional\n            Text to show when hovering over a document.\n\n        Returns\n        -------\n        plot\n            Interactive datamap plot, you can call the `.show()` method to\n            display it in your default browser or save it as static HTML using `.write_html()`.\n        \"\"\"\n        coordinates = self.reduced_embeddings[:, dimensions]\n        plot = build_datamapplot(\n            coordinates=coordinates,\n            topic_names=self.topic_names,\n            labels=self.labels_,\n            classes=self.classes_,\n            top_words=self.get_top_words(),\n            hover_text=hover_text,\n            topic_descriptions=getattr(self, \"topic_descriptions\", None),\n            **kwargs,\n        )\n        return plot\n\n    def transform(\n        self, raw_documents, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        if getattr(self, \"components_\", None) is None:\n            raise NotFittedError(\n                \"You can only transform documents once the model has been fitted.\"\n            )\n        idf_diag = getattr(self, \"_idf_diag\", None)\n        if idf_diag is not None:\n            X = self.vectorizer.transform(raw_documents)\n            X = normalize(X, axis=1, norm=\"l1\", copy=False)\n            X = X * idf_diag\n            doc_topic_matrix = np.exp(cosine_similarity(X, self.components_))\n        elif self.feature_importance == \"centroid\":\n            if embeddings is None:\n                embeddings = self.encode_documents(raw_documents)\n            doc_topic_matrix = np.exp(\n                cosine_similarity(embeddings, self._calculate_topic_vectors())\n            )\n        else:\n            doc_topic_matrix = safe_binarize(\n                self.labels_, classes=self.classes_\n            )\n        return doc_topic_matrix\n</code></pre>"},{"location":"clustering/#turftopic.models.cluster.ClusteringTopicModel.estimate_components","title":"<code>estimate_components(feature_importance=None)</code>","text":"<p>Estimates feature importances based on a fitted clustering.</p> <p>Parameters:</p> Name Type Description Default <code>feature_importance</code> <code>Optional[WordImportance]</code> <p>Method for estimating term importances. 'centroid' uses distances from cluster centroid similarly to Top2Vec. 'c-tf-idf' uses BERTopic's c-tf-idf. 'soft-c-tf-idf' uses Soft c-TF-IDF from GMM, the results should be very similar to 'c-tf-idf'. 'npmi' uses normalized pointwise mutual information between clusters and words. 'linear' calculates most predictive directions in embedding space and projects words onto them. 'fighting-words' calculates word importances based on the Fighting Words algorithm from Monroe et al.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_components, n_vocab)</code> <p>Topic-term matrix.</p> Source code in <code>turftopic/models/cluster.py</code> <pre><code>def estimate_components(\n    self, feature_importance: Optional[WordImportance] = None\n) -&gt; np.ndarray:\n    \"\"\"Estimates feature importances based on a fitted clustering.\n\n    Parameters\n    ----------\n    feature_importance: WordImportance, default None\n        Method for estimating term importances.\n        'centroid' uses distances from cluster centroid similarly\n        to Top2Vec.\n        'c-tf-idf' uses BERTopic's c-tf-idf.\n        'soft-c-tf-idf' uses Soft c-TF-IDF from GMM, the results should\n        be very similar to 'c-tf-idf'.\n        'npmi' uses normalized pointwise mutual information between clusters and words.\n        'linear' calculates most predictive directions in embedding space and projects\n        words onto them.\n        'fighting-words' calculates word importances based on the Fighting Words\n        algorithm from Monroe et al.\n\n    Returns\n    -------\n    ndarray of shape (n_components, n_vocab)\n        Topic-term matrix.\n    \"\"\"\n    if feature_importance is not None:\n        if feature_importance not in VALID_WORD_IMPORTANCE:\n            raise ValueError(\n                f\"feature_importance must be one of {VALID_WORD_IMPORTANCE} got {feature_importance} instead.\"\n            )\n        self.feature_importance = feature_importance\n    self.hierarchy.estimate_components()\n    doc_topic_matrix = safe_binarize(self.labels_, classes=self.classes_)\n    if feature_importance == \"c-tf-idf\":\n        _, self._idf_diag = ctf_idf(\n            doc_topic_matrix,\n            self.doc_term_matrix,\n            return_idf=True,\n        )\n    if feature_importance == \"soft-c-tf-idf\":\n        _, self._idf_diag = soft_ctf_idf(\n            doc_topic_matrix,\n            self.doc_term_matrix,\n            return_idf=True,\n        )\n    return self.components_\n</code></pre>"},{"location":"clustering/#turftopic.models.cluster.ClusteringTopicModel.estimate_temporal_components","title":"<code>estimate_temporal_components(time_labels, time_bin_edges, feature_importance=None)</code>","text":"<p>Estimates temporal components based on a fitted topic model.</p> <p>Parameters:</p> Name Type Description Default <code>feature_importance</code> <code>Optional[WordImportance]</code> <p>Method for estimating term importances. 'centroid' uses distances from cluster centroid similarly to Top2Vec. 'c-tf-idf' uses BERTopic's c-tf-idf. 'soft-c-tf-idf' uses Soft c-TF-IDF from GMM, the results should be very similar to 'c-tf-idf'. 'npmi' uses normalized pointwise information between clusters and words. 'linear' calculates most predictive directions in embedding space and projects words onto them.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_time_bins, n_components, n_vocab)</code> <p>Temporal topic-term matrix.</p> Source code in <code>turftopic/models/cluster.py</code> <pre><code>def estimate_temporal_components(\n    self,\n    time_labels,\n    time_bin_edges,\n    feature_importance: Optional[WordImportance] = None,\n) -&gt; np.ndarray:\n    \"\"\"Estimates temporal components based on a fitted topic model.\n\n    Parameters\n    ----------\n    feature_importance: WordImportance, default None\n        Method for estimating term importances.\n        'centroid' uses distances from cluster centroid similarly\n        to Top2Vec.\n        'c-tf-idf' uses BERTopic's c-tf-idf.\n        'soft-c-tf-idf' uses Soft c-TF-IDF from GMM, the results should\n        be very similar to 'c-tf-idf'.\n        'npmi' uses normalized pointwise information between clusters and words.\n        'linear' calculates most predictive directions in embedding space and projects\n        words onto them.\n\n    Returns\n    -------\n    ndarray of shape (n_time_bins, n_components, n_vocab)\n        Temporal topic-term matrix.\n    \"\"\"\n    if getattr(self, \"components_\", None) is None:\n        raise NotFittedError(\n            \"The model has not been fitted yet, please fit the model before estimating temporal components.\"\n        )\n    if feature_importance is None:\n        feature_importance = self.feature_importance\n    n_comp, n_vocab = self.components_.shape\n    self.time_bin_edges = time_bin_edges\n    n_bins = len(self.time_bin_edges) - 1\n    self.temporal_components_ = np.full(\n        (n_bins, n_comp, n_vocab),\n        np.nan,\n        dtype=self.components_.dtype,\n    )\n    self.temporal_importance_ = np.zeros((n_bins, n_comp))\n    for i_timebin in np.unique(time_labels):\n        topic_importances = self.document_topic_matrix[\n            time_labels == i_timebin\n        ].sum(axis=0)\n        if not topic_importances.sum() == 0:\n            topic_importances = topic_importances / topic_importances.sum()\n        self.temporal_importance_[i_timebin, :] = topic_importances\n        t_dtm = self.doc_term_matrix[time_labels == i_timebin]\n        t_doc_topic = self.document_topic_matrix[time_labels == i_timebin]\n        if feature_importance == \"c-tf-idf\":\n            self.temporal_components_[i_timebin], _ = ctf_idf(\n                t_doc_topic, t_dtm, return_idf=True\n            )\n        elif feature_importance == \"soft-c-tf-idf\":\n            self.temporal_components_[i_timebin], _ = soft_ctf_idf(\n                t_doc_topic, t_dtm, return_idf=True\n            )\n        elif feature_importance == \"npmi\":\n            self.temporal_components_[i_timebin] = npmi(t_doc_topic, t_dtm)\n        elif feature_importance == \"fighting-words\":\n            self.temporal_components_[i_timebin] = fighting_words(\n                t_doc_topic, t_dtm\n            )\n        elif feature_importance in [\"centroid\", \"linear\"]:\n            t_topic_vectors = self._calculate_topic_vectors(\n                is_in_slice=time_labels == i_timebin,\n            )\n            if feature_importance == \"centroid\":\n                components = cluster_centroid_distance(\n                    t_topic_vectors,\n                    self.vocab_embeddings,\n                )\n                mask_terms = t_dtm.sum(axis=0).astype(np.float64)\n                mask_terms = np.squeeze(np.asarray(mask_terms))\n                components[:, mask_terms == 0] = np.nan\n                self.temporal_components_[i_timebin] = components\n            else:\n                self.temporal_components_[i_timebin] = linear_classifier(\n                    t_doc_topic,\n                    embeddings=self.embeddings,\n                    vocab_embedding=self.vocab_embeddings,\n                )\n    return self.temporal_components_\n</code></pre>"},{"location":"clustering/#turftopic.models.cluster.ClusteringTopicModel.fit_predict","title":"<code>fit_predict(raw_documents, y=None, embeddings=None)</code>","text":"<p>Fits model and predicts cluster labels for all given documents.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> required <code>y</code> <p>Ignored, when the dimensionality reduction is TSNE (the default), in case of a dimensionality reduction that can utilize labels, you can pass labels to the model to inform the clustering process.</p> <code>None</code> <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_documents)</code> <p>Cluster label for all documents (-1 for outliers)</p> Source code in <code>turftopic/models/cluster.py</code> <pre><code>def fit_predict(\n    self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n) -&gt; np.ndarray:\n    \"\"\"Fits model and predicts cluster labels for all given documents.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    y: None\n        Ignored, when the dimensionality reduction is TSNE (the default),\n        in case of a dimensionality reduction that can utilize labels,\n        you can pass labels to the model to inform the clustering process.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n\n    Returns\n    -------\n    ndarray of shape (n_documents)\n        Cluster label for all documents (-1 for outliers)\n    \"\"\"\n    console = Console()\n    with console.status(\"Fitting model\") as status:\n        if embeddings is None:\n            status.update(\"Encoding documents\")\n            embeddings = self.encode_documents(raw_documents)\n            console.log(\"Encoding done.\")\n        self.embeddings = embeddings\n        status.update(\"Extracting terms\")\n        self.doc_term_matrix = self.vectorizer.fit_transform(raw_documents)\n        console.log(\"Term extraction done.\")\n        status.update(\"Reducing Dimensionality\")\n        # If y is specified, we pass it to the dimensionality\n        # reduction method as supervisory signal\n        if y is not None:\n            y = factorize_labels(y)\n        self.reduced_embeddings = (\n            self.dimensionality_reduction.fit_transform(embeddings, y=y)\n        )\n        console.log(\"Dimensionality reduction done.\")\n        status.update(\"Clustering documents\")\n        labels = self.clustering.fit_predict(self.reduced_embeddings)\n        console.log(\"Clustering done.\")\n        status.update(\"Estimating parameters.\")\n        # Initializing hierarchy\n        self.hierarchy = ClusterNode.create_root(self, labels=labels)\n        doc_topic_matrix = safe_binarize(\n            self.labels_, classes=self.classes_\n        )\n        if self.feature_importance == \"c-tf-idf\":\n            _, self._idf_diag = ctf_idf(\n                doc_topic_matrix,\n                self.doc_term_matrix,\n                return_idf=True,\n            )\n        if self.feature_importance == \"soft-c-tf-idf\":\n            _, self._idf_diag = soft_ctf_idf(\n                doc_topic_matrix,\n                self.doc_term_matrix,\n                return_idf=True,\n            )\n        console.log(\"Parameter estimation done.\")\n        if self.n_reduce_to is not None:\n            n_topics = self.classes_.shape[0]\n            status.update(\n                f\"Reducing topics from {n_topics} to {self.n_reduce_to}\"\n            )\n            self.reduce_topics(\n                self.n_reduce_to,\n                self.reduction_method,\n                self.reduction_distance_metric,\n            )\n            console.log(\n                f\"Topic reduction done from {n_topics} to {self.n_reduce_to}.\"\n            )\n    console.log(\"Model fitting done.\")\n    self.top_documents = self.get_top_documents(\n        raw_documents=raw_documents,\n        document_topic_matrix=self.transform(\n            raw_documents, embeddings=self.embeddings\n        ),\n    )\n    return self.labels_\n</code></pre>"},{"location":"clustering/#turftopic.models.cluster.ClusteringTopicModel.join_topics","title":"<code>join_topics(to_join, joint_id=None)</code>","text":"<p>Joins the given topics in the cluster hierarchy to a single topic.</p> <p>Parameters:</p> Name Type Description Default <code>to_join</code> <code>Sequence[int]</code> <p>Topics to join together by ID.</p> required <code>joint_id</code> <code>Optional[int]</code> <p>New ID for the joint cluster. Default is the smallest ID of the topics to join.</p> <code>None</code> Source code in <code>turftopic/models/cluster.py</code> <pre><code>def join_topics(\n    self, to_join: Sequence[int], joint_id: Optional[int] = None\n):\n    \"\"\"Joins the given topics in the cluster hierarchy to a single topic.\n\n    Parameters\n    ----------\n    to_join: Sequence of int\n        Topics to join together by ID.\n    joint_id: int, default None\n        New ID for the joint cluster.\n        Default is the smallest ID of the topics to join.\n    \"\"\"\n    self.hierarchy.join_topics(to_join, joint_id=joint_id)\n</code></pre>"},{"location":"clustering/#turftopic.models.cluster.ClusteringTopicModel.plot_clusters_datamapplot","title":"<code>plot_clusters_datamapplot(dimensions=(0, 1), hover_text=None, **kwargs)</code>","text":"<p>Creates an interactive browser plot of the topics in your data using datamapplot.</p> <p>Parameters:</p> Name Type Description Default <code>dimensions</code> <code>tuple[int, int]</code> <p>Indicates which dimensions of the reduced embeddings to use for displaying. Defaults to the first two dimensions.</p> <code>(0, 1)</code> <code>hover_text</code> <code>Optional[list[str]]</code> <p>Text to show when hovering over a document.</p> <code>None</code> <p>Returns:</p> Type Description <code>plot</code> <p>Interactive datamap plot, you can call the <code>.show()</code> method to display it in your default browser or save it as static HTML using <code>.write_html()</code>.</p> Source code in <code>turftopic/models/cluster.py</code> <pre><code>def plot_clusters_datamapplot(\n    self,\n    dimensions: tuple[int, int] = (0, 1),\n    hover_text: Optional[list[str]] = None,\n    **kwargs,\n):\n    \"\"\"Creates an interactive browser plot of the topics in your data using datamapplot.\n\n    Parameters\n    ----------\n    dimensions: tuple (x_coord, y_coord)\n        Indicates which dimensions of the reduced embeddings to use for displaying.\n        Defaults to the first two dimensions.\n    hover_text: list of str, optional\n        Text to show when hovering over a document.\n\n    Returns\n    -------\n    plot\n        Interactive datamap plot, you can call the `.show()` method to\n        display it in your default browser or save it as static HTML using `.write_html()`.\n    \"\"\"\n    coordinates = self.reduced_embeddings[:, dimensions]\n    plot = build_datamapplot(\n        coordinates=coordinates,\n        topic_names=self.topic_names,\n        labels=self.labels_,\n        classes=self.classes_,\n        top_words=self.get_top_words(),\n        hover_text=hover_text,\n        topic_descriptions=getattr(self, \"topic_descriptions\", None),\n        **kwargs,\n    )\n    return plot\n</code></pre>"},{"location":"clustering/#turftopic.models.cluster.ClusteringTopicModel.reduce_topics","title":"<code>reduce_topics(n_reduce_to, reduction_method=None, metric=None)</code>","text":"<p>Reduces the clustering to the desired amount with the given method.</p> <p>Parameters:</p> Name Type Description Default <code>n_reduce_to</code> <code>int</code> <p>Number of topics to reduce topics to. The specified reduction method will be used to merge them. By default, topics are not merged.</p> required <code>reduction_method</code> <code>Optional[LinkageMethod]</code> <p>Method used for hierarchically merging topics. Could be \"smallest\", which is Top2Vec's default merging strategy, or any of the linkage methods listed in SciPy's documentation</p> <code>None</code> <code>reduction_distance_metric</code> <p>Distance metric to use for hierarchical topic reduction.</p> required <p>Returns:</p> Type Description <code>ndarray of shape (n_documents)</code> <p>New cluster labels for documents.</p> Source code in <code>turftopic/models/cluster.py</code> <pre><code>def reduce_topics(\n    self,\n    n_reduce_to: int,\n    reduction_method: Optional[LinkageMethod] = None,\n    metric: Optional[DistanceMetric] = None,\n) -&gt; np.ndarray:\n    \"\"\"Reduces the clustering to the desired amount with the given method.\n\n    Parameters\n    ----------\n    n_reduce_to: int, default None\n        Number of topics to reduce topics to.\n        The specified reduction method will be used to merge them.\n        By default, topics are not merged.\n    reduction_method: LinkageMethod, default None\n        Method used for hierarchically merging topics.\n        Could be \"smallest\", which is Top2Vec's default merging strategy, or\n        any of the linkage methods listed in [SciPy's documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.cluster.hierarchy.linkage.html)\n    reduction_distance_metric: DistanceMetric, default None\n        Distance metric to use for hierarchical topic reduction.\n\n    Returns\n    -------\n    ndarray of shape (n_documents)\n        New cluster labels for documents.\n    \"\"\"\n    if not hasattr(self, \"original_labels_\"):\n        self.original_labels_ = self.labels_\n        self.original_names_ = self.topic_names\n        self.original_classes_ = self.classes_\n    if reduction_method is None:\n        reduction_method = self.reduction_method\n    if metric is None:\n        metric = self.reduction_distance_metric\n    self.hierarchy.reduce_topics(\n        n_reduce_to, method=reduction_method, metric=metric\n    )\n    return self.labels_\n</code></pre>"},{"location":"clustering/#turftopic.models.cluster.ClusteringTopicModel.reset_topics","title":"<code>reset_topics()</code>","text":"<p>Resets topics to the original cllustering.</p> Source code in <code>turftopic/models/cluster.py</code> <pre><code>def reset_topics(self):\n    \"\"\"Resets topics to the original cllustering.\"\"\"\n    original_labels = getattr(self, \"original_labels_\", None)\n    if original_labels is None:\n        warnings.warn(\"Topics have never been reduced, nothing to reset.\")\n    else:\n        self.hierarchy = ClusterNode.create_root(\n            self, labels=self.original_labels_\n        )\n        self.topic_names_ = self.original_names_\n</code></pre>"},{"location":"clustering/#turftopic.models.cluster.BERTopic","title":"<code>turftopic.models.cluster.BERTopic</code>","text":"<p>             Bases: <code>ClusteringTopicModel</code></p> <p>Convenience function to construct a BERTopic model in Turftopic. The model is essentially just a ClusteringTopicModel with BERTopic's defaults (UMAP -&gt; HDBSCAN -&gt; C-TF-IDF).</p> <pre><code>pip install turftopic[umap-learn]\n</code></pre> <pre><code>from turftopic import BERTopic\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\nmodel = BERTopic().fit(corpus)\nmodel.print_topics()\n</code></pre> Source code in <code>turftopic/models/cluster.py</code> <pre><code>class BERTopic(ClusteringTopicModel):\n    \"\"\"Convenience function to construct a BERTopic model in Turftopic.\n    The model is essentially just a ClusteringTopicModel\n    with BERTopic's defaults (UMAP -&gt; HDBSCAN -&gt; C-TF-IDF).\n\n    ```bash\n    pip install turftopic[umap-learn]\n    ```\n\n    ```python\n    from turftopic import BERTopic\n\n    corpus: list[str] = [\"some text\", \"more text\", ...]\n\n    model = BERTopic().fit(corpus)\n    model.print_topics()\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: Union[\n            Encoder, str, MultimodalEncoder\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        dimensionality_reduction: Optional[TransformerMixin] = None,\n        clustering: Optional[ClusterMixin] = None,\n        feature_importance: WordImportance = \"c-tf-idf\",\n        n_reduce_to: Optional[int] = None,\n        reduction_method: LinkageMethod = \"average\",\n        reduction_distance_metric: DistanceMetric = \"cosine\",\n        reduction_topic_representation: TopicRepresentation = \"component\",\n        random_state: Optional[int] = None,\n    ):\n        if dimensionality_reduction is None:\n            try:\n                from umap import UMAP\n            except ModuleNotFoundError as e:\n                raise ModuleNotFoundError(\n                    \"UMAP is not installed in your environment, but BERTopic requires it.\"\n                ) from e\n            dimensionality_reduction = UMAP(\n                n_neighbors=15,\n                n_components=5,\n                min_dist=0.0,\n                metric=\"cosine\",\n                random_state=random_state,\n            )\n        if clustering is None:\n            clustering = HDBSCAN(\n                min_cluster_size=10,\n                metric=\"euclidean\",\n                cluster_selection_method=\"eom\",\n            )\n        super().__init__(\n            encoder=encoder,\n            vectorizer=vectorizer,\n            dimensionality_reduction=dimensionality_reduction,\n            clustering=clustering,\n            n_reduce_to=n_reduce_to,\n            random_state=random_state,\n            feature_importance=feature_importance,\n            reduction_method=reduction_method,\n            reduction_distance_metric=reduction_distance_metric,\n            reduction_topic_representation=reduction_topic_representation,\n        )\n</code></pre>"},{"location":"clustering/#turftopic.models.cluster.Top2Vec","title":"<code>turftopic.models.cluster.Top2Vec</code>","text":"<p>             Bases: <code>ClusteringTopicModel</code></p> <p>Convenience function to construct a Top2Vec model in Turftopic. The model is essentially the same as ClusteringTopicModel with defaults that resemble Top2Vec (UMAP -&gt; HDBSCAN -&gt; Centroid term importance).</p> <pre><code>pip install turftopic[umap-learn]\n</code></pre> <pre><code>from turftopic import Top2Vec\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\nmodel = Top2Vec().fit(corpus)\nmodel.print_topics()\n</code></pre> Source code in <code>turftopic/models/cluster.py</code> <pre><code>class Top2Vec(ClusteringTopicModel):\n    \"\"\"Convenience function to construct a Top2Vec model in Turftopic.\n    The model is essentially the same as ClusteringTopicModel\n    with defaults that resemble Top2Vec (UMAP -&gt; HDBSCAN -&gt; Centroid term importance).\n\n    ```bash\n    pip install turftopic[umap-learn]\n    ```\n\n    ```python\n    from turftopic import Top2Vec\n\n    corpus: list[str] = [\"some text\", \"more text\", ...]\n\n    model = Top2Vec().fit(corpus)\n    model.print_topics()\n    ```\n    \"\"\"\n\n    def __init__(\n        self,\n        encoder: Union[\n            Encoder, str, MultimodalEncoder\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        dimensionality_reduction: Optional[TransformerMixin] = None,\n        clustering: Optional[ClusterMixin] = None,\n        feature_importance: WordImportance = \"centroid\",\n        n_reduce_to: Optional[int] = None,\n        reduction_method: LinkageMethod = \"smallest\",\n        reduction_distance_metric: DistanceMetric = \"cosine\",\n        reduction_topic_representation: TopicRepresentation = \"centroid\",\n        random_state: Optional[int] = None,\n    ):\n        if dimensionality_reduction is None:\n            try:\n                from umap import UMAP\n            except ModuleNotFoundError as e:\n                raise ModuleNotFoundError(\n                    \"UMAP is not installed in your environment, but Top2Vec requires it.\"\n                ) from e\n            dimensionality_reduction = UMAP(\n                n_neighbors=15,\n                n_components=5,\n                min_dist=0.0,\n                metric=\"cosine\",\n                random_state=random_state,\n            )\n        if clustering is None:\n            clustering = HDBSCAN(\n                min_cluster_size=15,\n                metric=\"euclidean\",\n                cluster_selection_method=\"eom\",\n            )\n        super().__init__(\n            encoder=encoder,\n            vectorizer=vectorizer,\n            dimensionality_reduction=dimensionality_reduction,\n            clustering=clustering,\n            n_reduce_to=n_reduce_to,\n            random_state=random_state,\n            feature_importance=feature_importance,\n            reduction_method=reduction_method,\n            reduction_distance_metric=reduction_distance_metric,\n            reduction_topic_representation=reduction_topic_representation,\n        )\n</code></pre>"},{"location":"concept_induction/","title":"Concept Induction (BETA)","text":"<p>Concept induction is the idea that higher-level concepts can be discovered and described in detail in corpora using the power of Large Language Models (Lam et al. 2024). These high-level concepts in corpora can also be discovered from particular angles, using seeds. The original study, and the Lloom package uses LLMs all the way, and therefore requires excessive computational resources, and aggressive down-sampling of the original corpus.</p> <p>In order to account for this scalability issue, we use a seeded topic model (KeyNMF) to discover the concepts, and only use LLMs to describe and use them. This allows us to get similar results to Lloom with a fraction of the costs.</p> <p>In addition, we allow users to generate a Concept Browser programmatically, with which these concepts and their related documents can be explored.</p>  Figure 1: Concepts discovered on the political ideologies dataset."},{"location":"concept_induction/#example-usage","title":"Example Usage","text":"<p>The example bellow uses a synthetically generated political ideologies dataset, that we examine from the following angles:</p> <ul> <li>Taxation</li> <li>Stance on immigration</li> <li>Environmental policy</li> </ul> <p>We use an OpenAI analyzer and KeyNMF, with the <code>paraphrase-MiniLM-L12-v2</code> embedding model. The code runs in about ten minutes.</p> <p>Install dependencies and set API Key:</p> <pre><code>pip install turftopic[openai] datasets\nexport OPENAI_API_KEY=\"sk-&lt;your API key here&gt;\"\n</code></pre> <pre><code>import numpy as np\nfrom datasets import load_dataset\nfrom sentence_transformers import SentenceTransformer\n\nfrom turftopic import KeyNMF, create_concept_browser\nfrom turftopic.analyzers import OpenAIAnalyzer\n\n# Loading the dataset from huggingface\nds = load_dataset(\"JyotiNayak/political_ideologies\", split=\"train\")\ncorpus = list(ds[\"statement\"])\n\n# Embedding all documents in the corpus\nencoder = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\nembeddings = encoder.encode(corpus, show_progress_bar=True)\n\n# Running separate seeded KeyNMF models for each tab and saving them\nseeds = [\"Taxation\", \"Stance on immigration\", \"Environmental policy\"]\nmodels = []\ndoc_topic = []\nfor seed in seeds:\n    model = KeyNMF(\n        3, encoder=encoder, seed_phrase=seed, seed_exponent=2, random_state=42\n    )\n    doc_topic_matrix = model.fit_transform(corpus, embeddings=embeddings)\n    doc_topic.append(doc_topic_matrix)\n    models.append(model)\n\n# Calculating topic sizes\nsizes = []\ntop_documents = []\ntopic_sizes = []\nfor doc_topic_matrix in doc_topic:\n    # We say that if a document has at least five percent of the max importance\n    # then it contains the topic\n    rescaled = doc_topic_matrix / doc_topic_matrix.max()\n    sizes = (rescaled &gt;= 0.05).sum(axis=0)\n    topic_sizes.append(sizes)\n    # Finding representative documents for each topic\n    docs = []\n    for doc_t in rescaled.T:\n        # Extracting top 10 documents for each topic\n        top = np.argsort(-doc_t)[:10]\n        # Making sure only those documents get in,\n        # that we have marked to contain the topic\n        top = top[doc_t[top] &gt;= 0.05]\n        docs.append([corpus[i] for i in top])\n    top_documents.append(docs)\ntopic_sizes = np.stack(topic_sizes)\n\n# Running topic analysis on all models using GPT-5-Nano\nanalyzer = OpenAIAnalyzer()\nanalysis_results = []\nfor model, docs in zip(models, top_documents):\n    res = analyzer.analyze_topics(\n        keywords=model.get_top_words(), documents=docs\n    )\n    analysis_results.append(res)\n\n# Creating the concept browser:\nbrowser = create_concept_browser(\n    seeds=seeds,\n    topic_names=[res.topic_names for res in analysis_results],\n    keywords=[model.get_top_words() for model in models],\n    topic_descriptions=[res.topic_descriptions for res in analysis_results],\n    topic_sizes=topic_sizes,\n    top_documents=top_documents,\n)\nbrowser.show()\n</code></pre> <p>See Figure 1 for the results</p>"},{"location":"concept_induction/#api-reference","title":"API reference","text":""},{"location":"concept_induction/#turftopic._concept_browser.create_browser","title":"<code>turftopic._concept_browser.create_browser(seeds, topic_names, keywords, topic_descriptions, topic_sizes, top_documents)</code>","text":"<p>Creates a concept browser figure with which you can investigate concepts related to different seeds.</p> <p>Parameters:</p> Name Type Description Default <code>seeds</code> <code>list[str]</code> <p>Seed phrases used for the analysis.</p> required <code>topic_names</code> <code>list[list[str]]</code> <p>Names of the topics for each of the seed phrases.</p> required <code>keywords</code> <code>list[list[list[str]]]</code> <p>Keywords for each of the topics for each seed.</p> required <code>topic_descriptions</code> <code>list[list[str]]</code> <p>Descriptions of the topics for each of the seed phrases.</p> required <code>topic_sizes</code> <code>ndarray</code> <p>Sizes of the topics for each seed, preferably number of documents.</p> required <code>top_documents</code> <code>list[list[str]]</code> <p>Top documents for each of the topics for each seed.</p> required <p>Returns:</p> Type Description <code>HTMLFigure</code> <p>Interactive HTML figure that you can either display or save.</p> Source code in <code>turftopic/_concept_browser.py</code> <pre><code>def create_browser(\n    seeds: list[str],\n    topic_names: list[list[str]],\n    keywords: list[list[list[str]]],\n    topic_descriptions: list[list[str]],\n    topic_sizes: np.ndarray,\n    top_documents: list[list[str]],\n) -&gt; HTMLFigure:\n    \"\"\"Creates a concept browser figure with which you can investigate concepts related to different seeds.\n\n    Parameters\n    ----------\n    seeds: list[str]\n        Seed phrases used for the analysis.\n    topic_names: list[list[str]]\n        Names of the topics for each of the seed phrases.\n    keywords: list[list[list[str]]]\n        Keywords for each of the topics for each seed.\n    topic_descriptions: list[list[str]]\n        Descriptions of the topics for each of the seed phrases.\n    topic_sizes: np.ndarray\n        Sizes of the topics for each seed, preferably number of documents.\n    top_documents: list[list[str]]\n        Top documents for each of the topics for each seed.\n\n    Returns\n    -------\n    HTMLFigure\n        Interactive HTML figure that you can either display or save.\n    \"\"\"\n    html = HTML_WRAPPER.format(\n        style=STYLE,\n        body_content=render_widget(\n            seeds,\n            topic_names,\n            keywords,\n            topic_descriptions,\n            topic_sizes,\n            top_documents,\n        ),\n    )\n    return HTMLFigure(html)\n</code></pre>"},{"location":"cross_lingual/","title":"Cross-lingual Topic Modeling","text":"<p>Under certain circumstances you might want to run a topic model on a multilingual corpus, where you do not want the model to capture language-differences. In these cases we recommend that you turn to cross-lingual topic modeling.</p>"},{"location":"cross_lingual/#natively-multilingual-models","title":"Natively multilingual models","text":"<p>Some topic models in Turftopic support cross-lingual modeling by default. The only difference is that you will have to choose a multilingual encoder model to produce document embeddings (consult MTEB(Multilingual) to find an encoder for your use case).</p> <code>SemanticSignalSeparation</code><code>ClusteringTopicModel</code><code>AutoEncodingTopicModel(combined=False)</code><code>GMM</code> <pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(10, encoder=\"paraphrase-multilingual-MiniLM-L12-v2\")\n</code></pre> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(encoder=\"paraphrase-multilingual-MiniLM-L12-v2\")\n</code></pre> <pre><code>from turftopic import AutoEncodingTopicModel\n\nmodel = AutoEncodingTopicModel(combined=False, encoder=\"paraphrase-multilingual-MiniLM-L12-v2\")\n</code></pre> <pre><code>from turftopic import GMM\n\nmodel = GMM(encoder=\"paraphrase-multilingual-MiniLM-L12-v2\")\n</code></pre>"},{"location":"cross_lingual/#term-matching","title":"Term Matching","text":"<p>Other models do not support cross-lingual use out of the box, and therefore need assistance to be applicable in a multilingual context.</p> <p>KeyNMF can use a trick called term-matching, in which terms that are highly similar get merged into the same term, thereby allowing for one term representing the same word in multiple languages:</p> <p>Note</p> <p>Term matching is an experimental feature in Turftopic, and might be improved or extended to more models in the future.</p> <pre><code>from datasets import load_dataset\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom turftopic import KeyNMF\n\n# Loading a parallel corpus\nds = load_dataset(\n    \"aiana94/polynews-parallel\", \"deu_Latn-eng_Latn\", split=\"train\"\n)\n# Subsampling\nds = ds.train_test_split(test_size=1000)[\"test\"]\ncorpus = ds[\"src\"] + ds[\"tgt\"]\n\nmodel = KeyNMF(\n    10,\n    cross_lingual=True,\n    encoder=\"paraphrase-multilingual-MiniLM-L12-v2\",\n    vectorizer=CountVectorizer()\n)\nmodel.fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking ... 15 africa-afrikanisch-african, media-medien-medienwirksam, schwarzwald-negroe-schwarzer, apartheid, difficulties-complicated-problems, kontinents-continent-kontinent, \u00e4thiopien-ethiopia, investitionen-investiert-investierenden, satire-satirical, hundred-100-1001 16 lawmaker-judges-gesetzliche, schutz-sicherheit-gesch\u00fctzt, an-success-eintreten, australian-australien-australischen, appeal-appealing-appeals, lawyer-lawyers-attorney, regeln-rule-rules, \u00f6ffentlichkeit-\u00f6ffentliche-publicly, terrorism-terroristischer-terrorismus, convicted 17 israels-israel-israeli, pal\u00e4stinensischen-palestinians-palestine, gay-lgbtq-gays, david, blockaden-blockades-blockade, stars-star-stelle, aviv, bombardieren-bombenexplosion-bombing, milit\u00e4rischer-army-military, kampfflugzeuge-warplanes 18 russischer-russlands-russischen, facebookbeitrag-facebook-facebooks, soziale-gesellschaftliche-sozialbauten, internetnutzer-internet, activism-aktivisten-activists, webseiten-web-site, isis, netzwerken-networks-netzwerk, vkontakte, media-medien-medienwirksam 19 bundesstaates-regierenden-regiert, chinesischen-chinesische-chinesisch, pr\u00e4sidentschaft-presidential-president, regions-region-regionen, demokratien-democratic-democracy, kapitalismus-capitalist-capitalism, staatsb\u00fcrgerin-citizens-b\u00fcrger, jemen-jemenitische-yemen, angolanischen-angola, media-medien-medienwirksam"},{"location":"ctm/","title":"Variational Autoencoding Topic Models","text":"<p>Topic models based on Variational Autoencoding are generative models based on ProdLDA (citation) enhanced with contextual representations.</p> Pseudo-Plate Notation of Autoencoding Topic Models <p>You will also hear people refer to these models as CTMs or Contextualized Topic Models. This is confusing, as technically all of the models in Turftopic are contextualized, but most of them do not use autoencoding variational inference. We will therefore stick to calling these models Autoencoding topic models.</p> <p>You will need to install Turftopic with Pyro to be able to use these models:</p> <pre><code>pip install turftopic[pyro-ppl]\n</code></pre>"},{"location":"ctm/#the-model","title":"The Model","text":"<p>Autoencoding Topic Models are generative models over word content in documents, similarly to classical generative topic models, such as Latent Dirichlet Allocation. This means that we have a probabilistic description of how words in documents are generated based on latent representations (topic proportions).</p> <p>Where these models differ from LDA is that they:</p> <ol> <li>Use a Logistic Normal distribution for topic proportions instead of a Dirichlet.</li> <li>Words in a document are determined by a product of experts, rather than drawing a topic label for each word in a document.</li> <li>Use Amortized Variational Inference:   A mapping between parameters of the topic proportions and input representations is learned by an artificial neural network (encoder network), instead of sampling the posterior.</li> </ol> <p>Note that term importance estimation is built into the model, instead of </p> <p>Depending on what the input of the encoder network is, we are either talking about a ZeroShotTM or a CombinedTM. ZeroShotTM(default) only uses the contextual embeddings as the input, while CombinedTM concatenates these to Bag-of-Words representations.</p> <p>You can choose either, by modifying the <code>combined</code> parameter of the model:</p> <pre><code>from turftopic import AutoEncodingTopicModel\n\nzeroshot_tm = AutoEncodingTopicModel(10, combined=False)\n\ncombined_tm = AutoEncodingTopicModel(10, combined=True)\n</code></pre>"},{"location":"ctm/#comparison-with-the-ctm-package","title":"Comparison with the CTM Package","text":"<p>The main difference is in the implementation. CTM implements inference from scratch in Torch, whereas Turftopic uses a 3rd party inference engine (and probabilistic programming language) called Pyro. This has a number of implications, most notably:</p> <ul> <li>Default hyperparameters are different, as such you might get different results with the two packages.</li> <li>Turftopic's inference is more stable, and is less likely to fail due to issues with numerical stability.   This is simply because Pyro is a very well tested and widely used engine, and is a more reliable choice than writing inference by hand.</li> <li>Inference in CTM might be faster, as it uses a specific implementation that does not need to be universal in opposition to Pyro.</li> </ul> <p>Turftopic, similarly to Clustering models might not contain some model specific utilites, that CTM boasts.</p>"},{"location":"ctm/#api-reference","title":"API Reference","text":""},{"location":"ctm/#turftopic.models.ctm.AutoEncodingTopicModel","title":"<code>turftopic.models.ctm.AutoEncodingTopicModel</code>","text":"<p>             Bases: <code>ContextualModel</code>, <code>MultimodalModel</code></p> <p>Variational autoencoding topic models with contextualized representations (CTM). Uses amortized variational inference with neural networks to estimate posterior for ProdLDA.</p> <pre><code>from turftopic import AutoEncodingTopicModel\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\nmodel = AutoEncodingTopicModel(10, combined=False).fit(corpus)\nmodel.print_topics()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of topics.</p> required <code>encoder</code> <code>Union[Encoder, SentenceTransformer, MultimodalEncoder]</code> <p>Model to encode documents/terms, all-MiniLM-L6-v2 is the default.</p> <code>'sentence-transformers/all-MiniLM-L6-v2'</code> <code>vectorizer</code> <code>Optional[CountVectorizer]</code> <p>Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.</p> <code>None</code> <code>combined</code> <code>bool</code> <p>Indicates whether encoder inputs should be combined with bow representations. When False the model is equivalent to ZeroShotTM, when True it is CombinedTM.</p> <code>False</code> <code>dropout_rate</code> <code>float</code> <p>Dropout in the encoder layers.</p> <code>0.1</code> <code>hidden</code> <code>int</code> <p>Size of hidden layers in the encoder network.</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Batch size when training the network.</p> <code>42</code> <code>learning_rate</code> <code>float</code> <p>Learning rate for the optimizer.</p> <code>0.01</code> <code>n_epochs</code> <code>int</code> <p>Number of epochs to run during training.</p> <code>50</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state to use so that results are exactly reproducible.</p> <code>None</code> Source code in <code>turftopic/models/ctm.py</code> <pre><code>class AutoEncodingTopicModel(ContextualModel, MultimodalModel):\n    \"\"\"Variational autoencoding topic models\n    with contextualized representations (CTM).\n    Uses amortized variational inference with neural networks\n    to estimate posterior for ProdLDA.\n\n    ```python\n    from turftopic import AutoEncodingTopicModel\n\n    corpus: list[str] = [\"some text\", \"more text\", ...]\n\n    model = AutoEncodingTopicModel(10, combined=False).fit(corpus)\n    model.print_topics()\n    ```\n\n    Parameters\n    ----------\n    n_components: int\n        Number of topics.\n    encoder: str or SentenceTransformer\n        Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n    vectorizer: CountVectorizer, default None\n        Vectorizer used for term extraction.\n        Can be used to prune or filter the vocabulary.\n    combined: bool, default False\n        Indicates whether encoder inputs should be combined\n        with bow representations.\n        When False the model is equivalent to ZeroShotTM,\n        when True it is CombinedTM.\n    dropout_rate: float, default 0.1\n        Dropout in the encoder layers.\n    hidden: int, default 100\n        Size of hidden layers in the encoder network.\n    batch_size: int, default 42\n        Batch size when training the network.\n    learning_rate: float, default 1e-2\n        Learning rate for the optimizer.\n    n_epochs: int, default 50\n        Number of epochs to run during training.\n    random_state: int, default None\n        Random state to use so that results are exactly reproducible.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: int,\n        encoder: Union[\n            Encoder, SentenceTransformer, MultimodalEncoder\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        combined: bool = False,\n        dropout_rate: float = 0.1,\n        hidden: int = 100,\n        batch_size: int = 42,\n        learning_rate: float = 1e-2,\n        n_epochs: int = 50,\n        random_state: Optional[int] = None,\n    ):\n        self.n_components = n_components\n        self.random_state = random_state\n        self.encoder = encoder\n        if isinstance(encoder, str):\n            self.encoder_ = SentenceTransformer(encoder)\n        else:\n            self.encoder_ = encoder\n        self.validate_encoder()\n        if vectorizer is None:\n            self.vectorizer = default_vectorizer()\n        else:\n            self.vectorizer = vectorizer\n        self.combined = combined\n        self.dropout_rate = dropout_rate\n        self.batch_size = batch_size\n        self.n_epochs = n_epochs\n        self.learning_rate = learning_rate\n        self.hidden = hidden\n\n    def transform(\n        self, raw_documents, embeddings: Optional[np.ndarray] = None\n    ):\n        \"\"\"Infers topic importances for new documents based on a fitted model.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n\n        Returns\n        -------\n        ndarray of shape (n_dimensions, n_topics)\n            Document-topic matrix.\n        \"\"\"\n        if embeddings is None:\n            embeddings = self.encoder_.encode(raw_documents)\n        if self.combined:\n            bow = self.vectorizer.fit_transform(raw_documents)\n            contextual_embeddings = np.concatenate(\n                (embeddings, bow.toarray()), axis=1\n            )\n        else:\n            contextual_embeddings = embeddings\n        contextual_embeddings = torch.tensor(contextual_embeddings).float()\n        loc, scale = self.model.encoder(contextual_embeddings)\n        prob = torch.softmax(loc, dim=-1)\n        return prob.cpu().data.numpy()\n\n    def fit(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ):\n        self.fit_transform(raw_documents, y, embeddings=embeddings)\n        return self\n\n    def fit_transform(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        console = Console()\n        with console.status(\"Fitting model\") as status:\n            if embeddings is None:\n                status.update(\"Encoding documents\")\n                embeddings = self.encoder_.encode(raw_documents)\n                console.log(\"Documents encoded.\")\n            status.update(\"Extracting terms.\")\n            document_term_matrix = self.vectorizer.fit_transform(raw_documents)\n            console.log(\"Term extraction done.\")\n            seed = self.random_state or random.randint(0, 10_000)\n            torch.manual_seed(seed)\n            pyro.set_rng_seed(seed)\n            device = torch.device(\"cpu\")\n            pyro.clear_param_store()\n            contextualized_size = embeddings.shape[1]\n            if self.combined:\n                contextualized_size = (\n                    contextualized_size + document_term_matrix.shape[1]\n                )\n            self.model = Model(\n                vocab_size=document_term_matrix.shape[1],\n                contextualized_size=contextualized_size,\n                num_topics=self.n_components,\n                hidden=self.hidden,\n                dropout=self.dropout_rate,\n            )\n            self.model.to(device)\n            optimizer = pyro.optim.Adam({\"lr\": self.learning_rate})\n            svi = SVI(\n                self.model.model,\n                self.model.guide,\n                optimizer,\n                loss=TraceMeanField_ELBO(),\n            )\n            num_batches = int(\n                math.ceil(document_term_matrix.shape[0] / self.batch_size)\n            )\n\n            status.update(f\"Fitting model. Epoch [0/{self.n_epochs}]\")\n            for epoch in range(self.n_epochs):\n                running_loss = 0.0\n                for i in range(num_batches):\n                    batch_bow = np.atleast_2d(\n                        document_term_matrix[\n                            i * self.batch_size : (i + 1) * self.batch_size, :\n                        ].toarray()\n                    )\n                    # Skipping batches that are smaller than 2\n                    if batch_bow.shape[0] &lt; 2:\n                        continue\n                    batch_contextualized = np.atleast_2d(\n                        embeddings[\n                            i * self.batch_size : (i + 1) * self.batch_size, :\n                        ]\n                    )\n                    if self.combined:\n                        batch_contextualized = np.concatenate(\n                            (batch_contextualized, batch_bow), axis=1\n                        )\n                    batch_contextualized = (\n                        torch.tensor(batch_contextualized).float().to(device)\n                    )\n                    batch_bow = torch.tensor(batch_bow).float().to(device)\n                    loss = svi.step(batch_bow, batch_contextualized)\n                    running_loss += loss / batch_bow.size(0)\n                status.update(\n                    f\"Fitting model. Epoch [{epoch}/{self.n_epochs}], Loss [{running_loss}]\"\n                )\n            self.components_ = np.array(self.model.beta())\n            console.log(\"Model fitting done.\")\n            document_topic_matrix = self.transform(\n                raw_documents, embeddings=embeddings\n            )\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=document_topic_matrix\n            )\n            return document_topic_matrix\n\n    def fit_transform_multimodal(\n        self,\n        raw_documents: list[str],\n        images: list[ImageRepr],\n        y=None,\n        embeddings: Optional[MultimodalEmbeddings] = None,\n    ) -&gt; np.ndarray:\n        self.validate_embeddings(embeddings)\n        console = Console()\n        self.multimodal_embeddings = embeddings\n        with console.status(\"Fitting model\") as status:\n            if self.multimodal_embeddings is None:\n                status.update(\"Encoding documents\")\n                self.multimodal_embeddings = self.encode_multimodal(\n                    raw_documents, images\n                )\n                console.log(\"Documents encoded.\")\n            status.update(\"Extracting terms.\")\n            document_term_matrix = self.vectorizer.fit_transform(raw_documents)\n            console.log(\"Term extraction done.\")\n            seed = self.random_state or random.randint(0, 10_000)\n            torch.manual_seed(seed)\n            pyro.set_rng_seed(seed)\n            device = torch.device(\"cpu\")\n            pyro.clear_param_store()\n            contextualized_size = self.multimodal_embeddings[\n                \"document_embeddings\"\n            ].shape[1]\n            if self.combined:\n                contextualized_size = (\n                    contextualized_size + document_term_matrix.shape[1]\n                )\n            self.model = Model(\n                vocab_size=document_term_matrix.shape[1],\n                contextualized_size=contextualized_size,\n                num_topics=self.n_components,\n                hidden=self.hidden,\n                dropout=self.dropout_rate,\n            )\n            self.model.to(device)\n            optimizer = pyro.optim.Adam({\"lr\": self.learning_rate})\n            svi = SVI(\n                self.model.model,\n                self.model.guide,\n                optimizer,\n                loss=TraceMeanField_ELBO(),\n            )\n            num_batches = int(\n                math.ceil(document_term_matrix.shape[0] / self.batch_size)\n            )\n\n            status.update(f\"Fitting model. Epoch [0/{self.n_epochs}]\")\n            for epoch in range(self.n_epochs):\n                running_loss = 0.0\n                for i in range(num_batches):\n                    batch_bow = np.atleast_2d(\n                        document_term_matrix[\n                            i * self.batch_size : (i + 1) * self.batch_size, :\n                        ].toarray()\n                    )\n                    # Skipping batches that are smaller than 2\n                    if batch_bow.shape[0] &lt; 2:\n                        continue\n                    batch_contextualized = np.atleast_2d(\n                        self.multimodal_embeddings[\"document_embeddings\"][\n                            i * self.batch_size : (i + 1) * self.batch_size, :\n                        ]\n                    )\n                    if self.combined:\n                        batch_contextualized = np.concatenate(\n                            (batch_contextualized, batch_bow), axis=1\n                        )\n                    batch_contextualized = (\n                        torch.tensor(batch_contextualized).float().to(device)\n                    )\n                    batch_bow = torch.tensor(batch_bow).float().to(device)\n                    loss = svi.step(batch_bow, batch_contextualized)\n                    running_loss += loss / batch_bow.size(0)\n                status.update(\n                    f\"Fitting model. Epoch [{epoch}/{self.n_epochs}], Loss [{running_loss}]\"\n                )\n            self.components_ = np.array(self.model.beta())\n            console.log(\"Model fitting done.\")\n            status.update(\"Transforming documents and images.\")\n            document_topic_matrix = self.transform(\n                raw_documents,\n                embeddings=self.multimodal_embeddings[\"document_embeddings\"],\n            )\n            self.image_topic_matrix = self.transform(\n                raw_documents,\n                embeddings=self.multimodal_embeddings[\"image_embeddings\"],\n            )\n            self.top_images: list[list[Image.Image]] = self.collect_top_images(\n                images, self.image_topic_matrix\n            )\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=document_topic_matrix\n            )\n            console.log(\"Transformation done.\")\n        return document_topic_matrix\n</code></pre>"},{"location":"ctm/#turftopic.models.ctm.AutoEncodingTopicModel.transform","title":"<code>transform(raw_documents, embeddings=None)</code>","text":"<p>Infers topic importances for new documents based on a fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> required <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_dimensions, n_topics)</code> <p>Document-topic matrix.</p> Source code in <code>turftopic/models/ctm.py</code> <pre><code>def transform(\n    self, raw_documents, embeddings: Optional[np.ndarray] = None\n):\n    \"\"\"Infers topic importances for new documents based on a fitted model.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n\n    Returns\n    -------\n    ndarray of shape (n_dimensions, n_topics)\n        Document-topic matrix.\n    \"\"\"\n    if embeddings is None:\n        embeddings = self.encoder_.encode(raw_documents)\n    if self.combined:\n        bow = self.vectorizer.fit_transform(raw_documents)\n        contextual_embeddings = np.concatenate(\n            (embeddings, bow.toarray()), axis=1\n        )\n    else:\n        contextual_embeddings = embeddings\n    contextual_embeddings = torch.tensor(contextual_embeddings).float()\n    loc, scale = self.model.encoder(contextual_embeddings)\n    prob = torch.softmax(loc, dim=-1)\n    return prob.cpu().data.numpy()\n</code></pre>"},{"location":"dynamic/","title":"Dynamic Topic Modeling","text":"<p>If you want to examine the evolution of topics over time, you will need a dynamic topic model.</p> <p>You will need to install Plotly for plotting to work.</p> <pre><code>pip install plotly\n</code></pre> <p>You can currently use three different topic models for modeling topics over time:</p> <ol> <li>ClusteringTopicModel, where an overall model is fitted on the whole corpus, and then term importances are estimated over time slices.</li> <li>GMM, similarly to clustering models, term importances are reestimated per time slice</li> <li>KeyNMF, an overall decomposition is done, then using coordinate descent, topic-term-matrices are recalculated based on document-topic importances in the given time slice.</li> <li>SemanticSignalSeparation, a global model is fitted and then local models are inferred using linear regression from embeddings and document-topic signals in a given time-slice.</li> </ol>"},{"location":"dynamic/#usage","title":"Usage","text":"<p>Dynamic topic models in Turftopic have a unified interface. To fit a dynamic topic model you will need a corpus, that has been annotated with timestamps. The timestamps need to be Python <code>datetime</code> objects, but pandas <code>Timestamp</code> object are also supported.</p> <p>Models that have dynamic modeling capabilities (<code>KeyNMF</code>, <code>GMM</code>, <code>SemanticSignalSeparation</code> and <code>ClusteringTopicModel</code>) have a <code>fit_transform_dynamic()</code> method, that fits the model on the corpus over time.</p> <pre><code>from datetime import datetime\n\nfrom turftopic import KeyNMF\n\ncorpus: list[str] = []\ntimestamps: list[datetime] = []\n\nmodel = KeyNMF(5, top_n=5, random_state=42)\ndocument_topic_matrix = model.fit_transform_dynamic(\n    corpus, timestamps=timestamps, bins=10\n)\n# or alternatively:\ntopic_data = model.prepare_dynamic_topic_data(corpus, timestamps=timestamps, bins=10)\n</code></pre> <p>Interpret Topics over Time</p> Interactive PlotOver-time Table <pre><code>model.plot_topics_over_time()\n# or\ntopic_data.plot_topics_over_time()\n</code></pre> <p>  Topics over time in a Dynamic KeyNMF model. </p> <pre><code>model.print_topics_over_time()\n# or\ntopic_data.print_topics_over_time()\n</code></pre> Time Slice 0_olympics_tokyo_athletes_beijing 1_covid_vaccine_pandemic_coronavirus 2_olympic_athletes_ioc_athlete 3_djokovic_novak_tennis_federer 4_ronaldo_cristiano_messi_manchester 2012 12 06 - 2013 11 10 genocide, yugoslavia, karadzic, facts, cnn cnn, russia, chechnya, prince, merkel france, cnn, francois, hollande, bike tennis, tournament, wimbledon, grass, courts beckham, soccer, retired, david, learn 2013 11 10 - 2014 10 14 keith, stones, richards, musician, author georgia, russia, conflict, 2008, cnn civil, rights, hear, why, should cnn, kidneys, traffickers, organ, nepal ronaldo, cristiano, goalscorer, soccer, player 2014 10 14 - 2015 09 18 ethiopia, brew, coffee, birthplace, anderson climate, sutter, countries, snapchat, injustice women, guatemala, murder, country, worst cnn, climate, oklahoma, women, topics sweden, parental, dads, advantage, leave 2015 09 18 - 2016 08 22 snow, ice, winter, storm, pets climate, crisis, drought, outbreaks, syrian women, vulnerabilities, frontlines, countries, marcelas cnn, warming, climate, sutter, theresa sutter, band, paris, fans, crowd 2016 08 22 - 2017 07 26 derby, epsom, sporting, race, spectacle overdoses, heroin, deaths, macron, emmanuel fear, died, indigenous, people, arthur siblings, amnesia, palombo, racial, mh370 bobbi, measles, raped, camp, rape 2017 07 26 - 2018 06 30 her, percussionist, drums, she, deported novichok, hurricane, hospital, deaths, breathing women, day, celebrate, taliban, international abuse, harassment, cnn, women, pilgrimage maradona, argentina, history, jadon, rape 2018 06 30 - 2019 06 03 athletes, teammates, celtics, white, racism pope, archbishop, francis, vigano, resignation racism, athletes, teammates, celtics, white golf, iceland, volcanoes, atlantic, ocean rape, sudanese, racist, women, soldiers 2019 06 03 - 2020 05 07 esports, climate, ice, racers, culver esports, coronavirus, pandemic, football, teams racers, women, compete, zone, bery serena, stadium, sasha, final, naomi kobe, bryant, greatest, basketball, influence 2020 05 07 - 2021 04 10 olympics, beijing, xinjiang, ioc, boycott covid, vaccine, coronavirus, pandemic, vaccination olympic, japan, medalist, canceled, tokyo djokovic, novak, tennis, federer, masterclass ronaldo, cristiano, messi, juventus, barcelona 2021 04 10 - 2022 03 16 olympics, tokyo, athletes, beijing, medal covid, pandemic, vaccine, vaccinated, coronavirus olympic, athletes, ioc, medal, athlete djokovic, novak, tennis, wimbledon, federer ronaldo, cristiano, messi, manchester, scored"},{"location":"dynamic/#api-reference","title":"API reference","text":"<p>All dynamic topic models have a <code>temporal_components_</code> attribute, which contains the topic-term matrices for each time slice, along with a <code>temporal_importance_</code> attribute, which contains the importance of each topic in each time slice.</p>"},{"location":"dynamic/#turftopic.dynamic.DynamicTopicModel","title":"<code>turftopic.dynamic.DynamicTopicModel</code>","text":"<p>             Bases: <code>ABC</code></p> Source code in <code>turftopic/dynamic.py</code> <pre><code>class DynamicTopicModel(ABC):\n    @staticmethod\n    def bin_timestamps(\n        timestamps: list[datetime], bins: Union[int, list[datetime]] = 10\n    ) -&gt; tuple[np.ndarray, list[datetime]]:\n        \"\"\"Bins timestamps based on given bins.\n\n        Parameters\n        ----------\n        timestamps: list[datetime]\n            List of timestamps for documents.\n        bins: int or list[datetime], default 10\n            Time bins to use.\n            If the bins are an int (N), N equally sized bins are used.\n            Otherwise they should be bin edges, including the last and first edge.\n            Bins are inclusive at the lower end and exclusive at the upper (lower &lt;= timestamp &lt; upper).\n\n        Returns\n        -------\n        time_labels: ndarray of int\n            Labels for time slice in each document.\n        bin_edges: list[datetime]\n            List of edges for time bins.\n        \"\"\"\n        return bin_timestamps(timestamps, bins)\n\n    @abstractmethod\n    def fit_transform_dynamic(\n        self,\n        raw_documents,\n        timestamps: list[datetime],\n        embeddings: Optional[np.ndarray] = None,\n        bins: Union[int, list[datetime]] = 10,\n    ) -&gt; np.ndarray:\n        \"\"\"Fits a dynamic topic model on the corpus and returns document-topic-importances.\n\n        Parameters\n        ----------\n        raw_documents\n            Documents to fit the model on.\n        timestamps: list[datetime]\n            Timestamp for each document in `datetime` format.\n        embeddings: np.ndarray, default None\n            Document embeddings produced by an embedding model.\n        bins: int or list[datetime], default 10\n            Specifies how to bin timestamps in to time slices.\n            When an `int`, the corpus will be divided into N equal time slices.\n            When a list, it describes the edges of each time slice including the starting\n            and final edges of the slices.\n\n        Returns\n        -------\n        ndarray of shape (n_documents, n_topics)\n            Document-topic importance matrix.\n        \"\"\"\n        pass\n\n    def fit_dynamic(\n        self,\n        raw_documents,\n        timestamps: list[datetime],\n        embeddings: Optional[np.ndarray] = None,\n        bins: Union[int, list[datetime]] = 10,\n    ):\n        \"\"\"Fits a dynamic topic model on the corpus and returns document-topic-importances.\n\n        Parameters\n        ----------\n        raw_documents\n            Documents to fit the model on.\n        timestamps: list[datetime]\n            Timestamp for each document in `datetime` format.\n        embeddings: np.ndarray, default None\n            Document embeddings produced by an embedding model.\n        bins: int or list[datetime], default 10\n            Specifies how to bin timestamps in to time slices.\n            When an `int`, the corpus will be divided into N equal time slices.\n            When a list, it describes the edges of each time slice including the starting\n            and final edges of the slices.\n\n            Note: The final edge is not included. You might want to add one day to\n            the last bin edge if it equals the last timestamp.\n        \"\"\"\n        self.fit_transform_dynamic(raw_documents, timestamps, embeddings, bins)\n        return self\n\n    def prepare_dynamic_topic_data(\n        self,\n        corpus: list[str],\n        timestamps: list[datetime],\n        embeddings: Optional[np.ndarray] = None,\n        bins: Union[int, list[datetime]] = 10,\n    ):\n        \"\"\"Produces topic inference data for a given corpus, that can be then used and reused.\n        Exists to allow visualizations out of the box with topicwizard.\n\n        Parameters\n        ----------\n        corpus: list of str\n            Documents to infer topical content for.\n        timestamps: list[datetime]\n            Timestamp for each document in `datetime` format.\n        embeddings: ndarray of shape (n_documents, n_dimensions)\n            Embeddings of documents.\n        bins: int or list[datetime], default 10\n            Specifies how to bin timestamps in to time slices.\n            When an `int`, the corpus will be divided into N equal time slices.\n            When a list, it describes the edges of each time slice including the starting\n            and final edges of the slices.\n\n            Note: The final edge is not included. You might want to add one day to\n            the last bin edge if it equals the last timestamp.\n\n        Returns\n        -------\n        TopicData\n            Information about topical inference in a dictionary.\n        \"\"\"\n        if embeddings is None:\n            embeddings = self.encode_documents(corpus)\n        if getattr(self, \"temporal_components_\", None) is not None:\n            try:\n                document_topic_matrix = self.transform(\n                    corpus, embeddings=embeddings\n                )\n            except (AttributeError, NotFittedError):\n                document_topic_matrix = self.fit_transform_dynamic(\n                    corpus,\n                    timestamps=timestamps,\n                    embeddings=embeddings,\n                    bins=bins,\n                )\n        else:\n            document_topic_matrix = self.fit_transform_dynamic(\n                corpus, timestamps=timestamps, embeddings=embeddings, bins=bins\n            )\n        dtm = self.vectorizer.transform(corpus)  # type: ignore\n        try:\n            classes = self.classes_\n        except AttributeError:\n            classes = list(range(self.components_.shape[0]))\n        res = TopicData(\n            corpus=corpus,\n            document_term_matrix=dtm,\n            vocab=self.get_vocab(),\n            document_topic_matrix=document_topic_matrix,\n            document_representation=embeddings,\n            topic_term_matrix=self.components_,  # type: ignore\n            transform=getattr(self, \"transform\", None),\n            topic_names=self.topic_names,\n            classes=classes,\n            temporal_components=self.temporal_components_,\n            temporal_importance=self.temporal_importance_,\n            time_bin_edges=self.time_bin_edges,\n        )\n        return res\n</code></pre>"},{"location":"dynamic/#turftopic.dynamic.DynamicTopicModel.bin_timestamps","title":"<code>bin_timestamps(timestamps, bins=10)</code>  <code>staticmethod</code>","text":"<p>Bins timestamps based on given bins.</p> <p>Parameters:</p> Name Type Description Default <code>timestamps</code> <code>list[datetime]</code> <p>List of timestamps for documents.</p> required <code>bins</code> <code>Union[int, list[datetime]]</code> <p>Time bins to use. If the bins are an int (N), N equally sized bins are used. Otherwise they should be bin edges, including the last and first edge. Bins are inclusive at the lower end and exclusive at the upper (lower &lt;= timestamp &lt; upper).</p> <code>10</code> <p>Returns:</p> Name Type Description <code>time_labels</code> <code>ndarray of int</code> <p>Labels for time slice in each document.</p> <code>bin_edges</code> <code>list[datetime]</code> <p>List of edges for time bins.</p> Source code in <code>turftopic/dynamic.py</code> <pre><code>@staticmethod\ndef bin_timestamps(\n    timestamps: list[datetime], bins: Union[int, list[datetime]] = 10\n) -&gt; tuple[np.ndarray, list[datetime]]:\n    \"\"\"Bins timestamps based on given bins.\n\n    Parameters\n    ----------\n    timestamps: list[datetime]\n        List of timestamps for documents.\n    bins: int or list[datetime], default 10\n        Time bins to use.\n        If the bins are an int (N), N equally sized bins are used.\n        Otherwise they should be bin edges, including the last and first edge.\n        Bins are inclusive at the lower end and exclusive at the upper (lower &lt;= timestamp &lt; upper).\n\n    Returns\n    -------\n    time_labels: ndarray of int\n        Labels for time slice in each document.\n    bin_edges: list[datetime]\n        List of edges for time bins.\n    \"\"\"\n    return bin_timestamps(timestamps, bins)\n</code></pre>"},{"location":"dynamic/#turftopic.dynamic.DynamicTopicModel.fit_dynamic","title":"<code>fit_dynamic(raw_documents, timestamps, embeddings=None, bins=10)</code>","text":"<p>Fits a dynamic topic model on the corpus and returns document-topic-importances.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> required <code>timestamps</code> <code>list[datetime]</code> <p>Timestamp for each document in <code>datetime</code> format.</p> required <code>embeddings</code> <code>Optional[ndarray]</code> <p>Document embeddings produced by an embedding model.</p> <code>None</code> <code>bins</code> <code>Union[int, list[datetime]]</code> <p>Specifies how to bin timestamps in to time slices. When an <code>int</code>, the corpus will be divided into N equal time slices. When a list, it describes the edges of each time slice including the starting and final edges of the slices.</p> <p>Note: The final edge is not included. You might want to add one day to the last bin edge if it equals the last timestamp.</p> <code>10</code> Source code in <code>turftopic/dynamic.py</code> <pre><code>def fit_dynamic(\n    self,\n    raw_documents,\n    timestamps: list[datetime],\n    embeddings: Optional[np.ndarray] = None,\n    bins: Union[int, list[datetime]] = 10,\n):\n    \"\"\"Fits a dynamic topic model on the corpus and returns document-topic-importances.\n\n    Parameters\n    ----------\n    raw_documents\n        Documents to fit the model on.\n    timestamps: list[datetime]\n        Timestamp for each document in `datetime` format.\n    embeddings: np.ndarray, default None\n        Document embeddings produced by an embedding model.\n    bins: int or list[datetime], default 10\n        Specifies how to bin timestamps in to time slices.\n        When an `int`, the corpus will be divided into N equal time slices.\n        When a list, it describes the edges of each time slice including the starting\n        and final edges of the slices.\n\n        Note: The final edge is not included. You might want to add one day to\n        the last bin edge if it equals the last timestamp.\n    \"\"\"\n    self.fit_transform_dynamic(raw_documents, timestamps, embeddings, bins)\n    return self\n</code></pre>"},{"location":"dynamic/#turftopic.dynamic.DynamicTopicModel.fit_transform_dynamic","title":"<code>fit_transform_dynamic(raw_documents, timestamps, embeddings=None, bins=10)</code>  <code>abstractmethod</code>","text":"<p>Fits a dynamic topic model on the corpus and returns document-topic-importances.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> required <code>timestamps</code> <code>list[datetime]</code> <p>Timestamp for each document in <code>datetime</code> format.</p> required <code>embeddings</code> <code>Optional[ndarray]</code> <p>Document embeddings produced by an embedding model.</p> <code>None</code> <code>bins</code> <code>Union[int, list[datetime]]</code> <p>Specifies how to bin timestamps in to time slices. When an <code>int</code>, the corpus will be divided into N equal time slices. When a list, it describes the edges of each time slice including the starting and final edges of the slices.</p> <code>10</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_documents, n_topics)</code> <p>Document-topic importance matrix.</p> Source code in <code>turftopic/dynamic.py</code> <pre><code>@abstractmethod\ndef fit_transform_dynamic(\n    self,\n    raw_documents,\n    timestamps: list[datetime],\n    embeddings: Optional[np.ndarray] = None,\n    bins: Union[int, list[datetime]] = 10,\n) -&gt; np.ndarray:\n    \"\"\"Fits a dynamic topic model on the corpus and returns document-topic-importances.\n\n    Parameters\n    ----------\n    raw_documents\n        Documents to fit the model on.\n    timestamps: list[datetime]\n        Timestamp for each document in `datetime` format.\n    embeddings: np.ndarray, default None\n        Document embeddings produced by an embedding model.\n    bins: int or list[datetime], default 10\n        Specifies how to bin timestamps in to time slices.\n        When an `int`, the corpus will be divided into N equal time slices.\n        When a list, it describes the edges of each time slice including the starting\n        and final edges of the slices.\n\n    Returns\n    -------\n    ndarray of shape (n_documents, n_topics)\n        Document-topic importance matrix.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"dynamic/#turftopic.dynamic.DynamicTopicModel.prepare_dynamic_topic_data","title":"<code>prepare_dynamic_topic_data(corpus, timestamps, embeddings=None, bins=10)</code>","text":"<p>Produces topic inference data for a given corpus, that can be then used and reused. Exists to allow visualizations out of the box with topicwizard.</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>list[str]</code> <p>Documents to infer topical content for.</p> required <code>timestamps</code> <code>list[datetime]</code> <p>Timestamp for each document in <code>datetime</code> format.</p> required <code>embeddings</code> <code>Optional[ndarray]</code> <p>Embeddings of documents.</p> <code>None</code> <code>bins</code> <code>Union[int, list[datetime]]</code> <p>Specifies how to bin timestamps in to time slices. When an <code>int</code>, the corpus will be divided into N equal time slices. When a list, it describes the edges of each time slice including the starting and final edges of the slices.</p> <p>Note: The final edge is not included. You might want to add one day to the last bin edge if it equals the last timestamp.</p> <code>10</code> <p>Returns:</p> Type Description <code>TopicData</code> <p>Information about topical inference in a dictionary.</p> Source code in <code>turftopic/dynamic.py</code> <pre><code>def prepare_dynamic_topic_data(\n    self,\n    corpus: list[str],\n    timestamps: list[datetime],\n    embeddings: Optional[np.ndarray] = None,\n    bins: Union[int, list[datetime]] = 10,\n):\n    \"\"\"Produces topic inference data for a given corpus, that can be then used and reused.\n    Exists to allow visualizations out of the box with topicwizard.\n\n    Parameters\n    ----------\n    corpus: list of str\n        Documents to infer topical content for.\n    timestamps: list[datetime]\n        Timestamp for each document in `datetime` format.\n    embeddings: ndarray of shape (n_documents, n_dimensions)\n        Embeddings of documents.\n    bins: int or list[datetime], default 10\n        Specifies how to bin timestamps in to time slices.\n        When an `int`, the corpus will be divided into N equal time slices.\n        When a list, it describes the edges of each time slice including the starting\n        and final edges of the slices.\n\n        Note: The final edge is not included. You might want to add one day to\n        the last bin edge if it equals the last timestamp.\n\n    Returns\n    -------\n    TopicData\n        Information about topical inference in a dictionary.\n    \"\"\"\n    if embeddings is None:\n        embeddings = self.encode_documents(corpus)\n    if getattr(self, \"temporal_components_\", None) is not None:\n        try:\n            document_topic_matrix = self.transform(\n                corpus, embeddings=embeddings\n            )\n        except (AttributeError, NotFittedError):\n            document_topic_matrix = self.fit_transform_dynamic(\n                corpus,\n                timestamps=timestamps,\n                embeddings=embeddings,\n                bins=bins,\n            )\n    else:\n        document_topic_matrix = self.fit_transform_dynamic(\n            corpus, timestamps=timestamps, embeddings=embeddings, bins=bins\n        )\n    dtm = self.vectorizer.transform(corpus)  # type: ignore\n    try:\n        classes = self.classes_\n    except AttributeError:\n        classes = list(range(self.components_.shape[0]))\n    res = TopicData(\n        corpus=corpus,\n        document_term_matrix=dtm,\n        vocab=self.get_vocab(),\n        document_topic_matrix=document_topic_matrix,\n        document_representation=embeddings,\n        topic_term_matrix=self.components_,  # type: ignore\n        transform=getattr(self, \"transform\", None),\n        topic_names=self.topic_names,\n        classes=classes,\n        temporal_components=self.temporal_components_,\n        temporal_importance=self.temporal_importance_,\n        time_bin_edges=self.time_bin_edges,\n    )\n    return res\n</code></pre>"},{"location":"encoders/","title":"Encoders","text":"<p>Turftopic by default encodes documents using sentence transformers. You can always change the encoder model either by passing the name of a sentence transformer from the Huggingface Hub to a model, or by passing a <code>SentenceTransformer</code> instance.</p> <p>Here's an example of building a multilingual topic model by using multilingual embeddings:</p> <pre><code>from sentence_transformers import SentenceTransformer\nfrom turftopic import GMM\n\ntrf = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\n\nmodel = GMM(10, encoder=trf)\n\n# or\n\nmodel = GMM(10, encoder=\"paraphrase-multilingual-MiniLM-L12-v2\")\n</code></pre> <p>Different encoders have different performance and model sizes. To make an informed choice about which embedding model you should be using check out the Massive Text Embedding Benchmark.</p>"},{"location":"encoders/#asymmetric-and-instruction-tuned-embedding-models","title":"Asymmetric and Instruction-tuned Embedding Models","text":"<p>Some embedding models can be used together with prompting, or encode queries and passages differently. Microsoft's E5 models are, for instance, all prompted by default, and it would be detrimental to performance not to do so yourself.</p> <p>In these cases, you're better off NOT passing a string to Turftopic models, but explicitly loading the model using <code>sentence-transformers</code>.</p> <p>Here's an example of using instruct models for keyword retrieval with KeyNMF. In this case, documents will serve as the queries and words as the passages:</p> <pre><code>from turftopic import KeyNMF\nfrom sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\n    \"intfloat/multilingual-e5-large-instruct\",\n    prompts={\n        \"query\": \"Instruct: Retrieve relevant keywords from the given document. Query: \"\n        \"passage\": \"Passage: \"\n    },\n    # Make sure to set default prompt to query!\n    default_prompt_name=\"query\",\n)\nmodel = KeyNMF(10, encoder=encoder)\n</code></pre> <p>And a regular, asymmetric example:</p> <pre><code>encoder = SentenceTransformer(\n    \"intfloat/e5-large-v2\",\n    prompts={\n        \"query\": \"query: \"\n        \"passage\": \"passage: \"\n    },\n    # Make sure to set default prompt to query!\n    default_prompt_name=\"query\",\n)\nmodel = KeyNMF(10, encoder=encoder)\n</code></pre>"},{"location":"encoders/#performance-tips","title":"Performance tips","text":"<p>From <code>sentence-transformers</code> version <code>3.2.0</code> you can significantly speed up some models by using the <code>onnx</code> backend instead of regular torch.</p> <pre><code>pip install sentence-transformers[onnx, onnx-gpu]\n</code></pre> <pre><code>from turftopic import SemanticSignalSeparation\nfrom sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\"all-MiniLM-L6-v2\", backend=\"onnx\")\n\nmodel = SemanticSignalSeparation(10, encoder=encoder)\n</code></pre>"},{"location":"encoders/#external-embeddings","title":"External Embeddings","text":"<p>If you do not have the computational resources to run embedding models on your own infrastructure, you can also use high quality 3rd party embeddings. Turftopic currently supports OpenAI, Voyage and Cohere embeddings.</p>"},{"location":"encoders/#turftopic.encoders.base.ExternalEncoder","title":"<code>turftopic.encoders.base.ExternalEncoder</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for external encoder models.</p> Source code in <code>turftopic/encoders/base.py</code> <pre><code>class ExternalEncoder(ABC):\n    \"\"\"Base class for external encoder models.\"\"\"\n\n    @abstractmethod\n    def encode(self, sentences: Iterable[str]) -&gt; np.ndarray:\n        \"\"\"Encodes sentences into an embedding matrix.\n\n        Parameters\n        ----------\n        sentences: Iterable[str]\n            Sentences to get embeddings for.\n\n        Returns\n        -------\n        ndarray of shape (n_docs, n_dimensions)\n            Embedding matrix.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"encoders/#turftopic.encoders.base.ExternalEncoder.encode","title":"<code>encode(sentences)</code>  <code>abstractmethod</code>","text":"<p>Encodes sentences into an embedding matrix.</p> <p>Parameters:</p> Name Type Description Default <code>sentences</code> <code>Iterable[str]</code> <p>Sentences to get embeddings for.</p> required <p>Returns:</p> Type Description <code>ndarray of shape (n_docs, n_dimensions)</code> <p>Embedding matrix.</p> Source code in <code>turftopic/encoders/base.py</code> <pre><code>@abstractmethod\ndef encode(self, sentences: Iterable[str]) -&gt; np.ndarray:\n    \"\"\"Encodes sentences into an embedding matrix.\n\n    Parameters\n    ----------\n    sentences: Iterable[str]\n        Sentences to get embeddings for.\n\n    Returns\n    -------\n    ndarray of shape (n_docs, n_dimensions)\n        Embedding matrix.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"encoders/#turftopic.encoders.CohereEmbeddings","title":"<code>turftopic.encoders.CohereEmbeddings</code>","text":"<p>             Bases: <code>ExternalEncoder</code></p> <p>Encoder model using embeddings from Cohere.</p> <p>The available models are:</p> <ul> <li><code>embed-english-v3.0</code></li> <li><code>embed-multilingual-v3.0</code></li> <li><code>embed-english-light-v3.0</code></li> <li><code>embed-multilingual-light-v3.0</code></li> <li><code>embed-english-v2.0</code></li> <li><code>embed-english-light-v2.0</code></li> <li><code>embed-multilingual-v2.0</code></li> </ul> <pre><code>from turftopic.encoders import CohereEmbeddings\nfrom turftopic import GMM\n\nmodel = GMM(10, encoder=CohereEmbeddings())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Embedding model to use from Cohere.</p> <code>'embed-english-v3.0'</code> <code>input_type</code> <code>str</code> <p>Input type passed to the embedding model.</p> <code>'clustering'</code> <code>batch_size</code> <code>int</code> <p>Sizes of the batches that will be sent to Cohere's API.</p> <code>25</code> Source code in <code>turftopic/encoders/cohere.py</code> <pre><code>class CohereEmbeddings(ExternalEncoder):\n    \"\"\"Encoder model using embeddings from Cohere.\n\n    The available models are:\n\n     - `embed-english-v3.0`\n     - `embed-multilingual-v3.0`\n     - `embed-english-light-v3.0`\n     - `embed-multilingual-light-v3.0`\n     - `embed-english-v2.0`\n     - `embed-english-light-v2.0`\n     - `embed-multilingual-v2.0`\n\n    ```python\n    from turftopic.encoders import CohereEmbeddings\n    from turftopic import GMM\n\n    model = GMM(10, encoder=CohereEmbeddings())\n    ```\n\n    Parameters\n    ----------\n    model: str, default \"embed-english-v3.0\"\n        Embedding model to use from Cohere.\n\n    input_type: str, default \"clustering\"\n        Input type passed to the embedding model.\n\n    batch_size: int, default 25\n        Sizes of the batches that will be sent to Cohere's API.\n    \"\"\"\n\n    def __init__(\n        self,\n        model: str = \"embed-english-v3.0\",\n        input_type: str = \"clustering\",\n        batch_size: int = 25,\n    ):\n        import cohere\n\n        try:\n            self.client = cohere.Client(os.environ[\"COHERE_KEY\"])\n        except KeyError as e:\n            raise KeyError(\n                \"You have to set the COHERE_KEY environment\"\n                \" variable to use Cohere embeddings.\"\n            ) from e\n        self.model = model\n        self.input_type = input_type\n        self.batch_size = batch_size\n\n    def encode(self, sentences: Iterable[str]):\n        result = []\n        for b in batched(sentences, self.batch_size):\n            response = self.client.embed(b, input_type=self.input_type)\n            result.extend(response.embeddings)\n        return np.array(result)\n</code></pre>"},{"location":"encoders/#turftopic.encoders.OpenAIEmbeddings","title":"<code>turftopic.encoders.OpenAIEmbeddings</code>","text":"<p>             Bases: <code>ExternalEncoder</code></p> <p>Encoder model using embeddings from OpenAI.</p> <p>The available models are:</p> <ul> <li><code>text-embedding-3-large</code></li> <li><code>text-embedding-3-small</code></li> <li><code>text-embedding-ada-002</code></li> </ul> <pre><code>from turftopic.encoders import OpenAIEmbeddings\nfrom turftopic import GMM\n\nmodel = GMM(10, encoder=OpenAIEmbeddings())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Embedding model to use from OpenAI.</p> <code>'text-embedding-3-large'</code> <code>batch_size</code> <code>int</code> <p>Sizes of the batches that will be sent to OpenAI's API.</p> <code>25</code> Source code in <code>turftopic/encoders/openai.py</code> <pre><code>class OpenAIEmbeddings(ExternalEncoder):\n    \"\"\"Encoder model using embeddings from OpenAI.\n\n    The available models are:\n\n     - `text-embedding-3-large`\n     - `text-embedding-3-small`\n     - `text-embedding-ada-002`\n\n    ```python\n    from turftopic.encoders import OpenAIEmbeddings\n    from turftopic import GMM\n\n    model = GMM(10, encoder=OpenAIEmbeddings())\n    ```\n\n    Parameters\n    ----------\n    model: str, default \"text-embedding-3-large\"\n        Embedding model to use from OpenAI.\n\n    batch_size: int, default 25\n        Sizes of the batches that will be sent to OpenAI's API.\n\n    \"\"\"\n\n    def __init__(\n        self, model: str = \"text-embedding-3-large\", batch_size: int = 25\n    ):\n        import openai\n\n        try:\n            openai.api_key = os.environ[\"OPENAI_KEY\"]\n        except KeyError as e:\n            raise KeyError(\n                \"You have to set the OPENAI_KEY environment\"\n                \" variable to use OpenAI embeddings.\"\n            ) from e\n        openai.organization = os.getenv(\"OPENAI_ORG\")\n        self.model = model\n        self.batch_size = batch_size\n\n    def encode(self, sentences: Iterable[str]):\n        import openai\n\n        result = []\n        for b in batched(sentences, self.batch_size):\n            resp = openai.embeddings.create(\n                input=b, model=self.model\n            )  # fmt: off\n            result.extend([_.embedding for _ in resp.data])\n        return np.array(result)\n</code></pre>"},{"location":"encoders/#turftopic.encoders.VoyageEmbeddings","title":"<code>turftopic.encoders.VoyageEmbeddings</code>","text":"<p>             Bases: <code>ExternalEncoder</code></p> <p>Encoder model using embeddings from VoyageAI.</p> <p>The available models are:</p> <ul> <li><code>voyage-2</code></li> <li><code>voyage-lite-2-instruct</code></li> </ul> <pre><code>from turftopic.encoders import VoyageEmbeddings\nfrom turftopic import GMM\n\nmodel = GMM(10, encoder=VoyageEmbeddings())\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Embedding model to use from Voyage.</p> <code>'voyage-lite-2-instruct'</code> <code>batch_size</code> <code>int</code> <p>Sizes of the batches that will be sent to Voyage's API.</p> <code>25</code> Source code in <code>turftopic/encoders/voyage.py</code> <pre><code>class VoyageEmbeddings(ExternalEncoder):\n    \"\"\"Encoder model using embeddings from VoyageAI.\n\n    The available models are:\n\n     - `voyage-2`\n     - `voyage-lite-2-instruct`\n\n    ```python\n    from turftopic.encoders import VoyageEmbeddings\n    from turftopic import GMM\n\n    model = GMM(10, encoder=VoyageEmbeddings())\n    ```\n\n    Parameters\n    ----------\n    model: str, default \"voyage-lite-2-instruct\"\n        Embedding model to use from Voyage.\n\n    batch_size: int, default 25\n        Sizes of the batches that will be sent to Voyage's API.\n\n    \"\"\"\n\n    def __init__(\n        self, model: str = \"voyage-lite-2-instruct\", batch_size: int = 25\n    ):\n        import voyageai\n\n        try:\n            voyageai.api_key = os.environ[\"VOYAGE_KEY\"]\n        except KeyError as e:\n            raise KeyError(\n                \"You have to set the VOYAGE_KEY environment\"\n                \" variable to use Voyage embeddings.\"\n            ) from e\n        self.model = model\n        self.batch_size = batch_size\n\n    def encode(self, sentences: Iterable[str]):\n        from voyageai import get_embeddings\n\n        result = []\n        for b in batched(sentences, self.batch_size):\n            response = get_embeddings(b, self.model)\n            result.extend(response)\n        return np.array(result)\n</code></pre>"},{"location":"finetuning/","title":"Modifying and finetuning models","text":"<p>Some models in Turftopic can be flexibly modified after being fitted. This allows users to fit pretrained topic models to their specific use cases.</p>"},{"location":"finetuning/#namingrenaming-topics","title":"Naming/renaming topics","text":"<p>Topics can be freely renamed in all topic models. This can be beneficial when interpreting models, as it allows you to assign labels to the topics you've already looked at. </p> <pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(10).fit(corpus)\n\n# you can specify a dict mapping IDs to names\nmodel.rename_topics({0: \"New name for topic 0\", 5: \"New name for topic 5\"})\n# or a list of topic names\nmodel.rename_topics([f\"Topic {i}\" for i in range(10)])\n</code></pre> <p>You can also automatically name topics with an analyzer large language model.</p> <pre><code>from turftopic.analyzers import LLMAnalyzer\n\nanalyzer = LLMAnalyzer()\nmodel.rename_topics(analyzer, use_documents=False)\n</code></pre>"},{"location":"finetuning/#changing-the-number-of-topics","title":"Changing the number of topics","text":"<p>Multiple models allow you to change the number of topics in a model after fitting them.</p>"},{"location":"finetuning/#refitting-s3-with-different-number-of-topics","title":"Refitting \\(S^3\\) with different number of topics","text":"<p>\\(S^3\\) models store all information that is needed to refit them using a different number of topics, iterations or random seed. This process is incredibly fast and allows you to explore semantics in a corpora on multiple levels of detail. Moreover, any model you load from a third party can be refitted at will.</p> <pre><code>from turftopic import load_model\n\nmodel = load_model(\"hf_user/some_s3_model\")\n\nprint(type(model))\n# turftopic.models.decomp.SemanticSignalSeparation\n\nprint(len(model.topic_names))\n# 10\n\nmodel.refit(corpus, embeddings=embeddings, n_components=20, random_seed=42)\nprint(len(model.topic_names))\n# 20\n</code></pre>"},{"location":"finetuning/#merging-topics-in-clustering-models","title":"Merging topics in clustering models","text":"<p>Clustering models are very flexible in this regard, as they allow you to merge clusters after the model has been fitted.</p>"},{"location":"finetuning/#manual-topic-merging","title":"Manual topic merging","text":"<p>You can merge topics manually in a clustering model by using the <code>join_topics()</code> method:</p> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel().fit(corpus)\n\n# This will join topic 0, 5 and 4 into topic 0\nmodel.join_topics([0,5,4])\n</code></pre>"},{"location":"finetuning/#hierarchical-merging","title":"Hierarchical merging","text":"<p>You can also merge clusters automatically into a desired number of topics. This can be done with the <code>reduce_topics()</code> method:</p> <p>Info</p> <p>For more info on topic merging methods, check out this page</p> <pre><code>model = ClusteringTopicModel().fit(corpus)\nmodel.reduce_topics(n_reduce_to=20, reduction_method=\"smallest\")\n</code></pre>"},{"location":"finetuning/#finetuning-models-on-a-new-corpus","title":"Finetuning models on a new corpus.","text":"<p>Currently, you can only finetune KeyNMF to a new corpus. You can do this by using the <code>partial_fit()</code> method on texts the model hasn't seen before:</p> <pre><code>from turftopic import load_model\n\nmodel = load_model(\"pretrained_keynmf_model\")\n\nprint(type(model))\n# turftopic.models.keynmf.KeyNMF\n\nnew_corpus: list[str] = [...]\n# Finetune the model to the new corpus\nmodel.partial_fit(new_corpus)\n\nmodel.to_disk(\"finetuned_model/\")\n</code></pre>"},{"location":"finetuning/#re-estimating-word-importance","title":"Re-estimating word importance","text":"<p>Both \\(S^3\\) and Clustering models come with multiple ways of estimating the importance of words for topics. Since both of these models use post-hoc measures, these scores can be calculated without fitting a new model or refitting an old one. This allows you to play around with different types of feature importance estimation measures for the same model (same underlying clusters or axes).</p> <p>Here's an example with \\(S^3\\): <pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(5, feature_importance=\"combined\").fit(corpus)\nmodel.print_topics()\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Topic ID \u2503 Highest Ranking                                              \u2503 Lowest Ranking                                        \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502        0 \u2502 hypocrisy, hypocritical, fallacy, debated, skeptics          \u2502 xfree86, emulator, codes, 9600, cd300                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        1 \u2502 spectrometer, dblspace, statistically, nutritional, makefile \u2502 uh, um, yeah, hm, oh                                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        2 \u2502 bullpen, goaltenders, pitchers, goaltender, pitching         \u2502 intel, nsa, spying, encrypt, terrorism                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        3 \u2502 espionage, wiretapping, cia, fbi, wiretaps                   \u2502 agnosticism, agnostic, upgrading, affordable, cheaper \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        4 \u2502 affordable, dealers, warrants, handguns, dealership          \u2502 semitic, theologians, judaism, persecuted, pagan      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nmodel.estimate_components(\"angular\")\nmodel.print_topics()\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Topic ID \u2503 Highest Ranking                                              \u2503 Lowest Ranking                                           \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502        0 \u2502 hypocritical, debated, hypotheses, misconceptions, fallacy   \u2502 diagnostics, win31, modems, cd300, gd3004                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        1 \u2502 spectrometer, dblspace, statistically, makefile, nutritional \u2502 ye, sub, naked, experiences, uh                          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        2 \u2502 bullpen, puckett, hitters, clemens, jenks                    \u2502 encryption, encrypt, intel, cryptosystem, cryptosystems  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        3 \u2502 journalists, cdc, chlorine, npr, briefing                    \u2502 values, ratios, upgrading, calculations, inherit         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        4 \u2502 handguns, warrants, warranty, reliability, handgun           \u2502 nutritional, metabolism, deuteronomy, pathology, hormone \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre></p> <p>And one with clustering models:</p> <p>Info</p> <p>Remember, these are the same underlying clusters, just described in two different ways. For further details, check out this page</p> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(n_reduce_to=5, feature_importance=\"soft-c-tf-idf\").fit(corpus)\nmodel.print_topics()\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Topic ID \u2503 Highest Ranking                                                            \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502       -1 \u2502 like, just, don, use, does, know, time, good, people, edu                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        0 \u2502 people, said, god, president, mr, think, going, say, did, myers            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        1 \u2502 max, g9v, b8f, a86, pl, 00, 145, 1d9, dos, 34u                             \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        2 \u2502 msg, cancer, food, battery, water, candida, medical, vitamin, yeast, diet  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        3 \u2502 25, 55, pit, det, pts, la, bos, 03, 10, 11                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        4 \u2502 insurance, car, dog, radar, health, bike, helmet, private, detector, speed \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\nmodel.estimate_components(feature_importance=\"centroid\")\nmodel.print_topics()\n\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Topic ID \u2503 Highest Ranking                                                                                      \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502       -1 \u2502 documented, concerns, dubious, obsolete, concern, alternative, et4000, complaints, cx, discussed     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        0 \u2502 persecutions, persecution, condemning, condemnation, fundamentalists, persecuted, fundamentalism,    \u2502\n\u2502          \u2502 theology, advocating, fundamentalist                                                                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        1 \u2502 xfree86, pcx, emulation, microsoft, hardware, emulator, x11r5, netware, workstations, chipset        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        2 \u2502 contamination, fungal, precautions, harmful, poisoning, chemicals, treatments, toxicity, dangers,    \u2502\n\u2502          \u2502 prevention                                                                                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        3 \u2502 nhl, bullpen, goaltenders, standings, sabres, canucks, braves, mlb, flyers, playoffs                 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502        4 \u2502 automotive, vehicle, vehicles, speeding, automobile, automobiles, driving, motorcycling,             \u2502\n\u2502          \u2502 motorcycles, highways                                                                                \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500#\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"hierarchical/","title":"Hierarchical Topic Modeling","text":"<p>You might expect some topics in your corpus to belong to a hierarchy of topics. Some models in Turftopic allow you to investigate hierarchical relations and build a taxonomy of topics in a corpus.</p> <p>Models in Turftopic that can model hierarchical relations will have a <code>hierarchy</code> property, that you can manipulate and print/visualize:</p> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(n_reduce_to=10).fit(corpus)\n# We cut at level 3 for plotting, since the hierarchy is very deep\nmodel.hierarchy.cut(3).plot_tree()\n</code></pre> <p>Drag and click to zoom, hover to see word importance</p>"},{"location":"hierarchical/#1-divisivetop-down-hierarchical-modeling","title":"1. Divisive/Top-down Hierarchical Modeling","text":"<p>In divisive modeling, you start from larger structures, higher up in the hierarchy, and  divide topics into smaller sub-topics on-demand. This is how hierarchical modeling works in KeyNMF, which, by default does not discover a topic hierarchy, but you can divide topics to as many subtopics as you see fit.</p> <p>As a demonstration, let's load a corpus, that we know to have hierarchical themes.</p> <pre><code>from sklearn.datasets import fetch_20newsgroups\n\ncorpus = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n    categories=[\n        \"comp.os.ms-windows.misc\",\n        \"comp.sys.ibm.pc.hardware\",\n        \"talk.religion.misc\",\n        \"alt.atheism\",\n    ],\n).data\n</code></pre> <p>In this case, we have two base themes, which are computers, and religion. Let us fit a KeyNMF model with two topics to see if the model finds these.</p> <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(2, top_n=15, random_state=42).fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking 0 windows, dos, os, disk, card, drivers, file, pc, files, microsoft 1 atheism, atheist, atheists, religion, christians, religious, belief, christian, god, beliefs <p>The results conform our intuition. Topic 0 seems to revolve around IT, while Topic 1 around atheism and religion. We can already suspect, however that more granular topics could be discovered in this corpus. For instance Topic 0 contains terms related to operating systems, like windows and dos, but also components, like disk and card.</p> <p>We can access the hierarchy of topics in the model at the current stage, with the model's <code>hierarchy</code> property.</p> <pre><code>print(model.hierarchy)\n</code></pre> Root  \u251c\u2500\u2500 0: windows, dos, os, disk, card, drivers, file, pc, files, microsoft  \u2514\u2500\u2500 1: atheism, atheist, atheists, religion, christians, religious, belief, christian, god, beliefs  <p>There isn't much to see yet, the model contains a flat hierarchy of the two topics we discovered and we are at root level. We can dissect these topics, by adding a level to the hierarchy.</p> <p>Let us add 3 subtopics to each topic on the root level.</p> <pre><code>model.hierarchy.divide_children(n_subtopics=3)\n</code></pre> Root  \u251c\u2500\u2500 0: windows, dos, os, disk, card, drivers, file, pc, files, microsoft  \u2502   \u251c\u2500\u2500 0.0: dos, file, disk, files, program, windows, disks, shareware, norton, memory  \u2502   \u251c\u2500\u2500 0.1: os, unix, windows, microsoft, apps, nt, ibm, ms, os2, platform  \u2502   \u2514\u2500\u2500 0.2: card, drivers, monitor, driver, vga, ram, motherboard, cards, graphics, ati  ...  <p>As you can see, the model managed to identify meaningful subtopics of the two larger topics we found earlier. Topic 0 got divided into a topic mostly concerned with dos and windows, a topic on operating systems in general, and one about hardware.</p> <p>You can also divide individual topics to a number of subtopics, by using the <code>divide()</code> method. Let us divide Topic 0.0 to 5 subtopics.</p> <pre><code>model.hierarchy[0][0].divide(5)\nmodel.hierarchy\n</code></pre> Root  \u251c\u2500\u2500 0: windows, dos, os, disk, card, drivers, file, pc, files, microsoft  \u2502   \u251c\u2500\u2500 0.0: dos, file, disk, files, program, windows, disks, shareware, norton, memory  \u2502   \u2502   \u251c\u2500\u2500 0.0.1: file, files, ftp, bmp, program, windows, shareware, directory, bitmap, zip  \u2502   \u2502   \u251c\u2500\u2500 0.0.2: os, windows, unix, microsoft, crash, apps, crashes, nt, pc, operating  ..."},{"location":"hierarchical/#2-agglomerativebottom-up-hierarchical-modeling","title":"2. Agglomerative/Bottom-up Hierarchical Modeling","text":"<p>In other models, hierarchies arise from starting from smaller, more specific topics, and then merging them together based on their similarity until a desired number of top-level topics are obtained.</p> <p>This is how it is done in clustering topic models like BERTopic and Top2Vec. Clustering models typically find a lot of topics, and it can help with interpretation to merge topics until you gain 10-20 top-level topics.</p> <p>You can either do this by default on a clustering model by setting <code>n_reduce_to</code> on initialization or you can do it manually with <code>reduce_topics()</code>. For more details, check our guide on Clustering models.</p> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(\n    n_reduce_to=10,\n    feature_importance=\"centroid\",\n    reduction_method=\"smallest\",\n    reduction_topic_representation=\"centroid\",\n    reduction_distance_metric=\"cosine\",\n)\nmodel.fit(corpus)\n\nprint(model.hierarchy)\n</code></pre> Root:  \u251c\u2500\u2500 -1: documented, obsolete, et4000, concerns, dubious, embedded, hardware, xfree86, alternative, seeking \u251c\u2500\u2500 20: hitter, pitching, batting, hitters, pitchers, fielder, shortstop, inning, baseman, pitcher \u251c\u2500\u2500 284: nhl, goaltenders, canucks, sabres, hockey, bruins, puck, oilers, canadiens, flyers \u2502   \u251c\u2500\u2500 242: sportschannel, espn, nbc, nhl, broadcasts, broadcasting, broadcast, mlb, cbs, cbc \u2502   \u2502   \u251c\u2500\u2500 171: stadium, tickets, mlb, ticket, sportschannel, mets, inning, nationals, schedule, cubs \u2502   \u2502   \u2502   \u2514\u2500\u2500 ... \u2502   \u2502   \u2514\u2500\u2500 21: sportschannel, nbc, espn, nhl, broadcasting, broadcasts, broadcast, hockey, cbc, cbs \u2502   \u2514\u2500\u2500 236: nhl, goaltenders, canucks, sabres, puck, oilers, andreychuk, bruins, goaltender, leafs ..."},{"location":"hierarchical/#api-reference","title":"API reference","text":""},{"location":"hierarchical/#turftopic.hierarchical.TopicNode","title":"<code>turftopic.hierarchical.TopicNode</code>  <code>dataclass</code>","text":"<p>Node for a topic in a topic hierarchy.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ContextualModel</code> <p>Underlying topic model, which the hierarchy is based on.</p> required <code>path</code> <code>tuple[int]</code> <p>Path that leads to this node from the root of the tree.</p> <code>()</code> <code>word_importance</code> <code>Optional[ndarray]</code> <p>Importance of each word in the vocabulary for given topic.</p> <code>None</code> <code>document_topic_vector</code> <code>Optional[ndarray]</code> <p>Importance of the topic in all documents in the corpus.</p> <code>None</code> <code>children</code> <code>Optional[list[TopicNode]]</code> <p>List of subtopics within this topic.</p> <code>None</code> Source code in <code>turftopic/hierarchical.py</code> <pre><code>@dataclass\nclass TopicNode:\n    \"\"\"Node for a topic in a topic hierarchy.\n\n    Parameters\n    ----------\n    model: ContextualModel\n        Underlying topic model, which the hierarchy is based on.\n    path: tuple[int], default ()\n        Path that leads to this node from the root of the tree.\n    word_importance: ndarray of shape (n_vocab), default None\n        Importance of each word in the vocabulary for given topic.\n    document_topic_vector: ndarray of shape (n_documents), default None\n        Importance of the topic in all documents in the corpus.\n    children: list[TopicNode], default None\n        List of subtopics within this topic.\n    \"\"\"\n\n    model: ContextualModel\n    path: tuple[int] = ()\n    word_importance: Optional[np.ndarray] = None\n    document_topic_vector: Optional[np.ndarray] = None\n    children: Optional[list[TopicNode]] = None\n\n    def _path_str(self):\n        return \".\".join([str(level_id) for level_id in self.path])\n\n    @property\n    def classes_(self):\n        if self.children is None:\n            raise AttributeError(\"TopicNode doesn't have children.\")\n        return np.array([child.path[-1] for child in self.children])\n\n    @property\n    def components_(self):\n        if self.children is None:\n            raise AttributeError(\"TopicNode doesn't have children.\")\n        return np.stack([child.word_importance for child in self.children])\n\n    @classmethod\n    def create_root(\n        cls,\n        model: ContextualModel,\n        components: np.ndarray,\n        document_topic_matrix: np.ndarray,\n    ) -&gt; TopicNode:\n        \"\"\"Creates root node from a topic models' components and topic importances in documents.\"\"\"\n        children = []\n        n_components = components.shape[0]\n        classes = getattr(model, \"classes_\", None)\n        if classes is None:\n            classes = np.arange(n_components)\n        for topic_id, comp, doc_top in zip(\n            classes, components, document_topic_matrix.T\n        ):\n            children.append(\n                cls(\n                    model,\n                    path=(topic_id,),\n                    word_importance=comp,\n                    document_topic_vector=doc_top,\n                    children=None,\n                )\n            )\n        return cls(\n            model,\n            path=(),\n            word_importance=None,\n            document_topic_vector=None,\n            children=children,\n        )\n\n    @property\n    def level(self) -&gt; int:\n        \"\"\"Indicates how deep down the hierarchy the topic is.\"\"\"\n        return len(self.path)\n\n    def get_words(self, top_k: int = 10) -&gt; list[tuple[str, float]]:\n        \"\"\"Returns top words and words importances for the topic.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of top words to return.\n\n        Returns\n        -------\n        list[tuple[str, float]]\n            List of word, importance pairs.\n        \"\"\"\n        if self.word_importance is None:\n            return []\n        vocab = self.model.get_vocab()\n        most_important = np.argsort(-self.word_importance)[:top_k]\n        words = vocab[most_important]\n        imp = self.word_importance[most_important]\n        return list(zip(words, imp))\n\n    @property\n    def description(self) -&gt; str:\n        \"\"\"Returns a high level description of the topic with its path in the tree\n        and top words.\"\"\"\n        if not len(self.path):\n            path = \"Root\"\n        else:\n            path = str(\n                self.path[-1]\n            )  # \".\".join([str(idx) for idx in self.path])\n        words = []\n        for word, imp in self.get_words(top_k=10):\n            words.append(word)\n        concat_words = \", \".join(words)\n        color = COLOR_PER_LEVEL[min(self.level, len(COLOR_PER_LEVEL) - 1)]\n        stylized = f\"[{color} bold]{path}[/]: [italic]{concat_words}[/]\"\n        console = Console()\n        with console.capture() as capture:\n            console.print(stylized, end=\"\")\n        return capture.get()\n\n    @property\n    def _simple_desc(self) -&gt; str:\n        if not len(self.path):\n            path = \"Root\"\n        else:\n            path = str(\n                self.path[-1]\n            )  # \".\".join([str(idx) for idx in self.path])\n        words = []\n        for word, imp in self.get_words(top_k=5):\n            words.append(word)\n        concat_words = \", \".join(words)\n        return f\"{path}: {concat_words}\"\n\n    def _build_tree(\n        self,\n        tree: Tree = None,\n        top_k: int = 10,\n        max_depth: Optional[int] = None,\n    ) -&gt; Tree:\n        if tree is None:\n            tree = Tree(self.description)\n        else:\n            tree = tree.add(self.description)\n        out_of_depth = (max_depth is not None) and (self.level &gt;= max_depth)\n        if out_of_depth:\n            if self.children is not None:\n                tree.add(\"...\")\n            return tree\n        if self.children is not None:\n            for child in self.children:\n                child._build_tree(tree, max_depth=max_depth)\n        return tree\n\n    def print_tree(\n        self,\n        top_k: int = 10,\n        max_depth: Optional[int] = None,\n    ) -&gt; None:\n        \"\"\"Print hierarchy in tree form.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of words to print for each topic.\n        max_depth: int, default None\n            Maximum depth at which topics should be printed in the hierarchy.\n            If None, the entire hierarchy is printed.\n        \"\"\"\n        tree = self._build_tree(top_k=top_k, max_depth=max_depth)\n        console = Console()\n        console.print(tree)\n\n    def __str__(self):\n        tree = self._build_tree(top_k=10, max_depth=3)\n        console = Console()\n        with console.capture() as capture:\n            console.print(tree)\n        return capture.get()\n\n    def __repr__(self):\n        return str(self)\n\n    def __getitem__(self, id_or_path: int):\n        if self.children is None:\n            raise IndexError(\n                \"Current node is a leaf and does not have children.\"\n            )\n        mapping = {\n            topic_class: i_topic\n            for i_topic, topic_class in enumerate(self.classes_)\n        }\n        return self.children[mapping[id_or_path]]\n\n    def __iter__(self):\n        return iter(self.children)\n\n    def plot_tree(self):\n        \"\"\"Plots hierarchy as an interactive tree in Plotly.\"\"\"\n        return _tree_plot(self)\n\n    def _append_path(self, path_prefix: int):\n        self.path = (path_prefix, *self.path)\n        if self.children is not None:\n            for child in self.children:\n                child._append_path(path_prefix)\n\n    def copy(self, deep: bool = True) -&gt; TopicNode:\n        \"\"\"Creates a copy of the given node.\n\n        Parameters\n        ----------\n        deep: bool, default True\n            Indicates whether the copy should be deep or shallow.\n            Deep copies are done recursively, while shallow copies only\n            contain references to the original children.\n\n        Returns\n        -------\n        Copy of original hierarchy.\n        \"\"\"\n        if (self.children is None) or (not deep):\n            return type(self)(\n                model=self.model,\n                path=self.path,\n                children=self.children,\n                word_importance=self.word_importance,\n                document_topic_vector=self.document_topic_vector,\n            )\n        else:\n            children = [child.copy(deep=True) for child in self.children]\n            return type(self)(\n                model=self.model,\n                path=self.path,\n                children=children,\n                word_importance=self.word_importance,\n                document_topic_vector=self.document_topic_vector,\n            )\n\n    def cut(self, max_depth: int) -&gt; TopicNode:\n        \"\"\"Cuts hierarchy at a given depth, returns copy of the hierarchy with levels beyond max_depth removed.\n\n        Parameters\n        ----------\n        max_depth: int\n            Maximum level of nodes to keep.\n\n        Returns\n        -------\n        TopicNode\n            Hierarchy cut at the given level.\n            Contains a deep copy of the original nodes.\n        \"\"\"\n        if (self.level &gt;= max_depth) or (not self.children):\n            return type(self)(\n                model=self.model,\n                path=self.path,\n                children=None,\n                word_importance=self.word_importance,\n                document_topic_vector=self.document_topic_vector,\n            )\n        else:\n            children = [child.cut(max_depth) for child in self.children]\n            return type(self)(\n                model=self.model,\n                path=self.path,\n                children=children,\n                word_importance=self.word_importance,\n                document_topic_vector=self.document_topic_vector,\n            )\n\n    def collect_leaves(self) -&gt; list[TopicNode]:\n        def _collect_leaves(node: TopicNode, leaves: list[TopicNode]):\n            if not node.children:\n                leaves.append(node.copy(deep=False))\n            else:\n                for child in node.children:\n                    _collect_leaves(child, leaves)\n\n        leaves = []\n        _collect_leaves(self, leaves)\n        return leaves\n\n    def flatten(self) -&gt; TopicNode:\n        \"\"\"Returns new hierarchy with only the leaves of the tree.\n\n        Returns\n        -------\n        TopicNode\n            Root node containing all leaves in a hierarchy.\n            Copies of the original nodes.\n        \"\"\"\n        leaves = self.collect_leaves()\n        ids = [leaf.path[-1] for leaf in leaves]\n        # If the IDs are not unique, we label them from 0 to N\n        if len(set(ids)) != len(ids):\n            current = 0\n            new_ids = []\n            for node_id in ids:\n                if node_id != -1:\n                    new_ids.append(current)\n                    current += 1\n                else:\n                    new_ids.append(-1)\n            ids = new_ids\n        for leaf_id, leaf in zip(ids, leaves):\n            leaf.path = (*self.path, leaf_id)\n        return type(self)(\n            model=self.model,\n            path=self.path,\n            word_importance=self.word_importance,\n            document_topic_vector=self.document_topic_vector,\n            children=leaves,\n        )\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.TopicNode.description","title":"<code>description: str</code>  <code>property</code>","text":"<p>Returns a high level description of the topic with its path in the tree and top words.</p>"},{"location":"hierarchical/#turftopic.hierarchical.TopicNode.level","title":"<code>level: int</code>  <code>property</code>","text":"<p>Indicates how deep down the hierarchy the topic is.</p>"},{"location":"hierarchical/#turftopic.hierarchical.TopicNode.copy","title":"<code>copy(deep=True)</code>","text":"<p>Creates a copy of the given node.</p> <p>Parameters:</p> Name Type Description Default <code>deep</code> <code>bool</code> <p>Indicates whether the copy should be deep or shallow. Deep copies are done recursively, while shallow copies only contain references to the original children.</p> <code>True</code> <p>Returns:</p> Type Description <code>Copy of original hierarchy.</code> Source code in <code>turftopic/hierarchical.py</code> <pre><code>def copy(self, deep: bool = True) -&gt; TopicNode:\n    \"\"\"Creates a copy of the given node.\n\n    Parameters\n    ----------\n    deep: bool, default True\n        Indicates whether the copy should be deep or shallow.\n        Deep copies are done recursively, while shallow copies only\n        contain references to the original children.\n\n    Returns\n    -------\n    Copy of original hierarchy.\n    \"\"\"\n    if (self.children is None) or (not deep):\n        return type(self)(\n            model=self.model,\n            path=self.path,\n            children=self.children,\n            word_importance=self.word_importance,\n            document_topic_vector=self.document_topic_vector,\n        )\n    else:\n        children = [child.copy(deep=True) for child in self.children]\n        return type(self)(\n            model=self.model,\n            path=self.path,\n            children=children,\n            word_importance=self.word_importance,\n            document_topic_vector=self.document_topic_vector,\n        )\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.TopicNode.create_root","title":"<code>create_root(model, components, document_topic_matrix)</code>  <code>classmethod</code>","text":"<p>Creates root node from a topic models' components and topic importances in documents.</p> Source code in <code>turftopic/hierarchical.py</code> <pre><code>@classmethod\ndef create_root(\n    cls,\n    model: ContextualModel,\n    components: np.ndarray,\n    document_topic_matrix: np.ndarray,\n) -&gt; TopicNode:\n    \"\"\"Creates root node from a topic models' components and topic importances in documents.\"\"\"\n    children = []\n    n_components = components.shape[0]\n    classes = getattr(model, \"classes_\", None)\n    if classes is None:\n        classes = np.arange(n_components)\n    for topic_id, comp, doc_top in zip(\n        classes, components, document_topic_matrix.T\n    ):\n        children.append(\n            cls(\n                model,\n                path=(topic_id,),\n                word_importance=comp,\n                document_topic_vector=doc_top,\n                children=None,\n            )\n        )\n    return cls(\n        model,\n        path=(),\n        word_importance=None,\n        document_topic_vector=None,\n        children=children,\n    )\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.TopicNode.cut","title":"<code>cut(max_depth)</code>","text":"<p>Cuts hierarchy at a given depth, returns copy of the hierarchy with levels beyond max_depth removed.</p> <p>Parameters:</p> Name Type Description Default <code>max_depth</code> <code>int</code> <p>Maximum level of nodes to keep.</p> required <p>Returns:</p> Type Description <code>TopicNode</code> <p>Hierarchy cut at the given level. Contains a deep copy of the original nodes.</p> Source code in <code>turftopic/hierarchical.py</code> <pre><code>def cut(self, max_depth: int) -&gt; TopicNode:\n    \"\"\"Cuts hierarchy at a given depth, returns copy of the hierarchy with levels beyond max_depth removed.\n\n    Parameters\n    ----------\n    max_depth: int\n        Maximum level of nodes to keep.\n\n    Returns\n    -------\n    TopicNode\n        Hierarchy cut at the given level.\n        Contains a deep copy of the original nodes.\n    \"\"\"\n    if (self.level &gt;= max_depth) or (not self.children):\n        return type(self)(\n            model=self.model,\n            path=self.path,\n            children=None,\n            word_importance=self.word_importance,\n            document_topic_vector=self.document_topic_vector,\n        )\n    else:\n        children = [child.cut(max_depth) for child in self.children]\n        return type(self)(\n            model=self.model,\n            path=self.path,\n            children=children,\n            word_importance=self.word_importance,\n            document_topic_vector=self.document_topic_vector,\n        )\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.TopicNode.flatten","title":"<code>flatten()</code>","text":"<p>Returns new hierarchy with only the leaves of the tree.</p> <p>Returns:</p> Type Description <code>TopicNode</code> <p>Root node containing all leaves in a hierarchy. Copies of the original nodes.</p> Source code in <code>turftopic/hierarchical.py</code> <pre><code>def flatten(self) -&gt; TopicNode:\n    \"\"\"Returns new hierarchy with only the leaves of the tree.\n\n    Returns\n    -------\n    TopicNode\n        Root node containing all leaves in a hierarchy.\n        Copies of the original nodes.\n    \"\"\"\n    leaves = self.collect_leaves()\n    ids = [leaf.path[-1] for leaf in leaves]\n    # If the IDs are not unique, we label them from 0 to N\n    if len(set(ids)) != len(ids):\n        current = 0\n        new_ids = []\n        for node_id in ids:\n            if node_id != -1:\n                new_ids.append(current)\n                current += 1\n            else:\n                new_ids.append(-1)\n        ids = new_ids\n    for leaf_id, leaf in zip(ids, leaves):\n        leaf.path = (*self.path, leaf_id)\n    return type(self)(\n        model=self.model,\n        path=self.path,\n        word_importance=self.word_importance,\n        document_topic_vector=self.document_topic_vector,\n        children=leaves,\n    )\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.TopicNode.get_words","title":"<code>get_words(top_k=10)</code>","text":"<p>Returns top words and words importances for the topic.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words to return.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple[str, float]]</code> <p>List of word, importance pairs.</p> Source code in <code>turftopic/hierarchical.py</code> <pre><code>def get_words(self, top_k: int = 10) -&gt; list[tuple[str, float]]:\n    \"\"\"Returns top words and words importances for the topic.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of top words to return.\n\n    Returns\n    -------\n    list[tuple[str, float]]\n        List of word, importance pairs.\n    \"\"\"\n    if self.word_importance is None:\n        return []\n    vocab = self.model.get_vocab()\n    most_important = np.argsort(-self.word_importance)[:top_k]\n    words = vocab[most_important]\n    imp = self.word_importance[most_important]\n    return list(zip(words, imp))\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.TopicNode.plot_tree","title":"<code>plot_tree()</code>","text":"<p>Plots hierarchy as an interactive tree in Plotly.</p> Source code in <code>turftopic/hierarchical.py</code> <pre><code>def plot_tree(self):\n    \"\"\"Plots hierarchy as an interactive tree in Plotly.\"\"\"\n    return _tree_plot(self)\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.TopicNode.print_tree","title":"<code>print_tree(top_k=10, max_depth=None)</code>","text":"<p>Print hierarchy in tree form.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of words to print for each topic.</p> <code>10</code> <code>max_depth</code> <code>Optional[int]</code> <p>Maximum depth at which topics should be printed in the hierarchy. If None, the entire hierarchy is printed.</p> <code>None</code> Source code in <code>turftopic/hierarchical.py</code> <pre><code>def print_tree(\n    self,\n    top_k: int = 10,\n    max_depth: Optional[int] = None,\n) -&gt; None:\n    \"\"\"Print hierarchy in tree form.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of words to print for each topic.\n    max_depth: int, default None\n        Maximum depth at which topics should be printed in the hierarchy.\n        If None, the entire hierarchy is printed.\n    \"\"\"\n    tree = self._build_tree(top_k=top_k, max_depth=max_depth)\n    console = Console()\n    console.print(tree)\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.DivisibleTopicNode","title":"<code>turftopic.hierarchical.DivisibleTopicNode</code>  <code>dataclass</code>","text":"<p>             Bases: <code>TopicNode</code></p> <p>Node for a topic in a topic hierarchy that can be subdivided.</p> Source code in <code>turftopic/hierarchical.py</code> <pre><code>@dataclass\nclass DivisibleTopicNode(TopicNode):\n    \"\"\"Node for a topic in a topic hierarchy that can be subdivided.\"\"\"\n\n    def clear(self):\n        \"\"\"Deletes children of the given node.\"\"\"\n        self.children = None\n        return self\n\n    def divide(self, n_subtopics: int, **kwargs):\n        \"\"\"Divides current node into smaller subtopics.\n        Only works when the underlying model is a divisive hierarchical model.\n\n        Parameters\n        ----------\n        n_subtopics: int\n            Number of topics to divide the topic into.\n        \"\"\"\n        try:\n            self.children = self.model.divide_topic(\n                node=self, n_subtopics=n_subtopics, **kwargs\n            )\n        except AttributeError as e:\n            raise AttributeError(\n                \"Looks like your model is not a divisive hierarchical model.\"\n            ) from e\n        return self\n\n    def divide_children(self, n_subtopics: int, **kwargs):\n        \"\"\"Divides all children of the current node to smaller topics.\n        Only works when the underlying model is a divisive hierarchical model.\n\n        Parameters\n        ----------\n        n_subtopics: int\n            Number of topics to divide the topics into.\n        \"\"\"\n        if self.children is None:\n            raise ValueError(\n                \"Current Node is a leaf, children can't be subdivided.\"\n            )\n        for child in self.children:\n            child.divide(n_subtopics, **kwargs)\n        return self\n\n    def __str__(self):\n        tree = self._build_tree(top_k=10, max_depth=3)\n        console = Console()\n        with console.capture() as capture:\n            console.print(tree)\n        return capture.get()\n\n    def __repr__(self):\n        return str(self)\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.DivisibleTopicNode.clear","title":"<code>clear()</code>","text":"<p>Deletes children of the given node.</p> Source code in <code>turftopic/hierarchical.py</code> <pre><code>def clear(self):\n    \"\"\"Deletes children of the given node.\"\"\"\n    self.children = None\n    return self\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.DivisibleTopicNode.divide","title":"<code>divide(n_subtopics, **kwargs)</code>","text":"<p>Divides current node into smaller subtopics. Only works when the underlying model is a divisive hierarchical model.</p> <p>Parameters:</p> Name Type Description Default <code>n_subtopics</code> <code>int</code> <p>Number of topics to divide the topic into.</p> required Source code in <code>turftopic/hierarchical.py</code> <pre><code>def divide(self, n_subtopics: int, **kwargs):\n    \"\"\"Divides current node into smaller subtopics.\n    Only works when the underlying model is a divisive hierarchical model.\n\n    Parameters\n    ----------\n    n_subtopics: int\n        Number of topics to divide the topic into.\n    \"\"\"\n    try:\n        self.children = self.model.divide_topic(\n            node=self, n_subtopics=n_subtopics, **kwargs\n        )\n    except AttributeError as e:\n        raise AttributeError(\n            \"Looks like your model is not a divisive hierarchical model.\"\n        ) from e\n    return self\n</code></pre>"},{"location":"hierarchical/#turftopic.hierarchical.DivisibleTopicNode.divide_children","title":"<code>divide_children(n_subtopics, **kwargs)</code>","text":"<p>Divides all children of the current node to smaller topics. Only works when the underlying model is a divisive hierarchical model.</p> <p>Parameters:</p> Name Type Description Default <code>n_subtopics</code> <code>int</code> <p>Number of topics to divide the topics into.</p> required Source code in <code>turftopic/hierarchical.py</code> <pre><code>def divide_children(self, n_subtopics: int, **kwargs):\n    \"\"\"Divides all children of the current node to smaller topics.\n    Only works when the underlying model is a divisive hierarchical model.\n\n    Parameters\n    ----------\n    n_subtopics: int\n        Number of topics to divide the topics into.\n    \"\"\"\n    if self.children is None:\n        raise ValueError(\n            \"Current Node is a leaf, children can't be subdivided.\"\n        )\n    for child in self.children:\n        child.divide(n_subtopics, **kwargs)\n    return self\n</code></pre>"},{"location":"hierarchical/#turftopic.models._hierarchical_clusters.ClusterNode","title":"<code>turftopic.models._hierarchical_clusters.ClusterNode</code>","text":"<p>             Bases: <code>TopicNode</code></p> <p>Hierarchical Topic Node for clustering models. Supports merging topics based on a hierarchical merging strategy.</p> Source code in <code>turftopic/models/_hierarchical_clusters.py</code> <pre><code>class ClusterNode(TopicNode):\n    \"\"\"Hierarchical Topic Node for clustering models.\n    Supports merging topics based on a hierarchical merging strategy.\"\"\"\n\n    @classmethod\n    def create_root(cls, model: ContextualModel, labels: np.ndarray):\n        \"\"\"Creates root node from a topic models' components and topic importances in documents.\"\"\"\n        classes = np.sort(np.unique(labels))\n        document_topic_matrix = safe_binarize(labels, classes=classes)\n        children = []\n        for topic_id, doc_top in zip(classes, document_topic_matrix.T):\n            children.append(\n                cls(\n                    model,\n                    path=(topic_id,),\n                    document_topic_vector=doc_top,\n                    children=None,\n                )\n            )\n        res = cls(\n            model,\n            path=(),\n            word_importance=None,\n            document_topic_vector=None,\n            children=children,\n        )\n        res.estimate_components()\n        return res\n\n    def join_topics(\n        self, to_join: Sequence[int], joint_id: Optional[int] = None\n    ):\n        \"\"\"Joins a number of topics into a new topic with a given ID.\n\n        Parameters\n        ----------\n        to_join: Sequence of int\n            Children in the hierarchy to join (IDs indicate the last element of the path).\n        joint_id: int, default None\n            ID to give to the joint topic. By default, this will be the topic with the smallest ID.\n        \"\"\"\n        if self.children is None:\n            raise TypeError(\"Node doesn't have children, can't merge.\")\n        if len(set(to_join)) &lt; len(to_join):\n            raise ValueError(\n                f\"You can't join a cluster with itself: {to_join}\"\n            )\n        if joint_id is None:\n            joint_id = min(to_join)\n        children = [self[i] for i in to_join]\n        joint_membership = np.stack(\n            [child.document_topic_vector for child in children]\n        )\n        joint_membership = np.sum(joint_membership, axis=0)\n        child_ids = [child.path[-1] for child in children]\n        joint_node = TopicNode(\n            model=self.model,\n            children=children,\n            document_topic_vector=joint_membership,\n            path=(*self.path, joint_id),\n        )\n        for child in joint_node:\n            child._append_path(joint_id)\n        self.children = [\n            child for child in self.children if child.path[-1] not in child_ids\n        ] + [joint_node]\n        component_map = self._estimate_children_components()\n        for child in self.children:\n            child.word_importance = component_map[child.path[-1]]\n\n    def estimate_components(self) -&gt; np.ndarray:\n        component_map = self._estimate_children_components()\n        for child in self.children:\n            child.word_importance = component_map[child.path[-1]]\n        return self.components_\n\n    @property\n    def labels_(self) -&gt; np.ndarray:\n        topic_document_membership = np.stack(\n            [child.document_topic_vector for child in self.children]\n        )\n        labels = np.argmax(topic_document_membership, axis=0)\n        strength = np.max(topic_document_membership, axis=0)\n        # documents that are not in this part of the hierarchy are treated as outliers\n        labels[strength == 0] = -1\n        return np.array(\n            [self.children[label].path[-1] for label in labels if label != -1]\n        )\n\n    def _estimate_children_components(self) -&gt; dict[int, np.ndarray]:\n        \"\"\"Estimates feature importances based on a fitted clustering.\"\"\"\n        clusters = np.unique(self.labels_)\n        classes = np.sort(clusters)\n        labels = self.labels_\n        topic_vectors = self.model._calculate_topic_vectors(\n            classes=classes, labels=labels\n        )\n        document_topic_matrix = safe_binarize(labels, classes=classes)\n        if self.model.feature_importance == \"soft-c-tf-idf\":\n            components = soft_ctf_idf(\n                document_topic_matrix, self.model.doc_term_matrix\n            )  # type: ignore\n        if self.model.feature_importance == \"fighting-words\":\n            components = fighting_words(\n                document_topic_matrix, self.model.doc_term_matrix\n            )  # type: ignore\n        elif self.model.feature_importance in [\"centroid\", \"linear\"]:\n            if not hasattr(self.model, \"vocab_embeddings\"):\n                self.model.vocab_embeddings = self.model.encode_documents(\n                    self.model.vectorizer.get_feature_names_out()\n                )  # type: ignore\n                if (\n                    self.model.vocab_embeddings.shape[1]\n                    != topic_vectors.shape[1]\n                ):\n                    raise ValueError(\n                        NOT_MATCHING_ERROR.format(\n                            n_dims=topic_vectors.shape[1],\n                            n_word_dims=self.model.vocab_embeddings.shape[1],\n                        )\n                    )\n            if self.model.feature_importance == \"centroid\":\n                components = cluster_centroid_distance(\n                    topic_vectors,\n                    self.model.vocab_embeddings,\n                )\n            else:\n                components = linear_classifier(\n                    document_topic_matrix,\n                    self.model.embeddings,\n                    self.model.vocab_embeddings,\n                )\n        elif self.model.feature_importance == \"npmi\":\n            components = npmi(\n                document_topic_matrix, self.model.doc_term_matrix\n            )\n        else:\n            components = ctf_idf(\n                document_topic_matrix, self.model.doc_term_matrix\n            )\n        return dict(zip(classes, components))\n\n    def _merge_clusters(self, linkage_matrix: np.ndarray):\n        classes = self.classes_\n        max_class = len(classes[classes != -1])\n        for i_cluster, (left, right, *_) in enumerate(linkage_matrix):\n            self.join_topics(\n                [int(left), int(right)], int(max_class + i_cluster)\n            )\n\n    def _calculate_linkage(\n        self, n_reduce_to: int, method: str = \"average\", metric: str = \"cosine\"\n    ) -&gt; np.ndarray:\n        if method not in VALID_LINKAGE_METHODS:\n            raise ValueError(\n                f\"Linkage method has to be one of: {VALID_LINKAGE_METHODS}, but got {method} instead.\"\n            )\n        classes = self.classes_\n        labels = self.labels_\n        topic_sizes = np.array([np.sum(labels == label) for label in classes])\n        topic_representations = self.model.topic_representations\n        if method == \"smallest\":\n            return smallest_linkage(\n                n_reduce_to=n_reduce_to,\n                topic_vectors=topic_representations,\n                topic_sizes=topic_sizes,\n                classes=classes,\n                metric=metric,\n            )\n        else:\n            n_classes = len(classes[classes != -1])\n            topic_vectors = topic_representations[classes != -1]\n            n_reductions = n_classes - n_reduce_to\n            cond_dist = pdist(topic_vectors, metric=metric)\n            # Making the algorithm more numerically stable\n            if metric == \"cosine\":\n                cond_dist[~np.isfinite(cond_dist)] = -1\n            return linkage(cond_dist, method=method)[:n_reductions]\n\n    def reduce_topics(\n        self, n_reduce_to: int, method: str = \"average\", metric: str = \"cosine\"\n    ):\n        n_topics = np.sum(self.classes_ != -1)\n        if n_topics &lt;= n_reduce_to:\n            warnings.warn(\n                f\"Number of clusters is already {n_topics} &lt;= {n_reduce_to}, nothing to do.\"\n            )\n            return\n        linkage_matrix = self._calculate_linkage(\n            n_reduce_to, method=method, metric=metric\n        )\n        self.linkage_matrix_ = linkage_matrix\n        self._merge_clusters(linkage_matrix)\n</code></pre>"},{"location":"hierarchical/#turftopic.models._hierarchical_clusters.ClusterNode.create_root","title":"<code>create_root(model, labels)</code>  <code>classmethod</code>","text":"<p>Creates root node from a topic models' components and topic importances in documents.</p> Source code in <code>turftopic/models/_hierarchical_clusters.py</code> <pre><code>@classmethod\ndef create_root(cls, model: ContextualModel, labels: np.ndarray):\n    \"\"\"Creates root node from a topic models' components and topic importances in documents.\"\"\"\n    classes = np.sort(np.unique(labels))\n    document_topic_matrix = safe_binarize(labels, classes=classes)\n    children = []\n    for topic_id, doc_top in zip(classes, document_topic_matrix.T):\n        children.append(\n            cls(\n                model,\n                path=(topic_id,),\n                document_topic_vector=doc_top,\n                children=None,\n            )\n        )\n    res = cls(\n        model,\n        path=(),\n        word_importance=None,\n        document_topic_vector=None,\n        children=children,\n    )\n    res.estimate_components()\n    return res\n</code></pre>"},{"location":"hierarchical/#turftopic.models._hierarchical_clusters.ClusterNode.join_topics","title":"<code>join_topics(to_join, joint_id=None)</code>","text":"<p>Joins a number of topics into a new topic with a given ID.</p> <p>Parameters:</p> Name Type Description Default <code>to_join</code> <code>Sequence[int]</code> <p>Children in the hierarchy to join (IDs indicate the last element of the path).</p> required <code>joint_id</code> <code>Optional[int]</code> <p>ID to give to the joint topic. By default, this will be the topic with the smallest ID.</p> <code>None</code> Source code in <code>turftopic/models/_hierarchical_clusters.py</code> <pre><code>def join_topics(\n    self, to_join: Sequence[int], joint_id: Optional[int] = None\n):\n    \"\"\"Joins a number of topics into a new topic with a given ID.\n\n    Parameters\n    ----------\n    to_join: Sequence of int\n        Children in the hierarchy to join (IDs indicate the last element of the path).\n    joint_id: int, default None\n        ID to give to the joint topic. By default, this will be the topic with the smallest ID.\n    \"\"\"\n    if self.children is None:\n        raise TypeError(\"Node doesn't have children, can't merge.\")\n    if len(set(to_join)) &lt; len(to_join):\n        raise ValueError(\n            f\"You can't join a cluster with itself: {to_join}\"\n        )\n    if joint_id is None:\n        joint_id = min(to_join)\n    children = [self[i] for i in to_join]\n    joint_membership = np.stack(\n        [child.document_topic_vector for child in children]\n    )\n    joint_membership = np.sum(joint_membership, axis=0)\n    child_ids = [child.path[-1] for child in children]\n    joint_node = TopicNode(\n        model=self.model,\n        children=children,\n        document_topic_vector=joint_membership,\n        path=(*self.path, joint_id),\n    )\n    for child in joint_node:\n        child._append_path(joint_id)\n    self.children = [\n        child for child in self.children if child.path[-1] not in child_ids\n    ] + [joint_node]\n    component_map = self._estimate_children_components()\n    for child in self.children:\n        child.word_importance = component_map[child.path[-1]]\n</code></pre>"},{"location":"keyphrase/","title":"Keyphrase-based Topic Modeling with KeyNMF","text":"<p>KeyNMF, in its original form extracts topic descriptions as lists of words. Sometimes, however it is desirable that one can use whole keyphrases instead of just single keywords, as these can be substantially more informative.</p> <p>We can utilize keyphrases in KeyNMF by using KeyphraseVectorizers:</p> <pre><code>pip install keyphrase-vectorizers\n</code></pre> <p>KeyphraseVectorizers extracts entire, grammatically correct noun phrases from text relying on POS-tag annotations from SpaCy.</p>"},{"location":"keyphrase/#data","title":"Data","text":"<p>For this demonstration, we will use a subset of 20 Newsgroups.</p> <pre><code>from sklearn.datasets import fetch_20newsgroups\n\ncorpus = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n    categories=[\n        \"comp.os.ms-windows.misc\",\n        \"comp.sys.ibm.pc.hardware\",\n        \"talk.religion.misc\",\n        \"alt.atheism\",\n    ],\n).data\n</code></pre>"},{"location":"keyphrase/#model-definition","title":"Model definition","text":"<p>We can define the model with a <code>KeyphraseCountVectorizer</code> as its vectorizer model.</p> <pre><code>from turftopic import KeyNMF\nfrom keyphrase_vectorizers import KeyphraseCountVectorizer\n\nmodel = KeyNMF(n_components=10, vectorizer=KeyphraseCountVectorizer())\nmodel.fit(corpus)\n\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking 0 windows, dos, os/2, file, files, os, microsoft, ms, program, unix 1 ., ?, -, !, thanks, anyone, ..., one, 's, ftp 2 atheism, atheist, atheists, alt.atheism, belief, religion, weak atheism, strong atheism, weak atheist, believe 3 disk, drive, drives, floppy, disks, dos, hard drive, ide, hard disk, bios 4 card, monitor, drivers, video card, vga, motherboard, ram, cards, driver, ati 5 morality, moral, objective, objective morality, morals, moral system, subjective, moral sense, natural morality, animals 6 scsi, scsi-2, scsi-1, scsi drive, scsi controller, scsi-2 controller, scsi-2 controller chip, fast scsi-1, scsi-2 chip, scsi-2 speeds 7 modem, port, serial, serial port, modems, ports, serial ports, com ports, null modem, dos 8 printer, print, printer driver, fonts, printing, driver, font, printers, hp, print manager 9 christians, christian, bible, christianity, god, religion, jesus, faith, believe, beliefs <p>As you can see most topics are of much higher quality than what you normally expect with KeyNMF. This, however comes at the price of slower modeling.</p>"},{"location":"model_definition_and_training/","title":"Defining and Training Topic Models","text":"<p>In order to start modeling your corpora, you will need to define a topic model. There are a wide array of available models in Turftopic that all have their unique behaviour. On the other hand all models will need to have certain components, and have attributes you can adjust to your needs. This page provides a guide on how to define models, train them, and use them for inference.</p>"},{"location":"model_definition_and_training/#defining-a-model","title":"Defining a Model","text":""},{"location":"model_definition_and_training/#1-topic-model","title":"1. Topic Model","text":"<p>In order to initialize a model, you will first need to make a choice about which topic model you'd like to use. You might want to have a look at the Models page in order to make an informed choice about the topic model you intend to train.</p> <p>Here are some examples of models you can load and use in the package:</p> KeyNMFClusteringTopicModelSemanticSignalSeparation <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(n_components=10, top_n=15)\n</code></pre> <pre><code>from turftopic import ClusteringTopicModel\n\nmodel = ClusteringTopicModel(n_reduce_to=10, feature_importance=\"centroid\")\n</code></pre> <pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(n_components=10, feature_importance=\"combined\")\n</code></pre>"},{"location":"model_definition_and_training/#2-vectorizer","title":"2. Vectorizer","text":"<p>In Turftopic, all Models have a vectorizer component, which is responsible for extracting word content from documents in the corpus. This means, that a vectorizer also determines which words will be part of the model's vocabulary. For a more detailed explanation, see the Vectorizers page</p> <p>The default is scikit-learn's CountVectorizer:</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\ndefault_vectorizer = CountVectorizer(min_df=10, stop_words=\"english\")\n</code></pre> <p>You can add a custom vectorizer to a topic model upon initializing it, thereby getting different behaviours. You can for instance use noun-phrases in your model instead of words by using <code>NounPhraseCountVectorizer</code> or estimate parameters for lemmas by using <code>LemmaCountVectorizer</code></p> Noun Phrase ExtractionLemma ExtractionMultilingual Tokenization (Arabic example) <pre><code>pip install turftopic[spacy]\npython -m spacy download \"en_core_web_sm\"\n</code></pre> <pre><code>from turftopic import KeyNMF\nfrom turftopic.vectorizers.spacy import NounPhraseCountVectorizer\n\nmodel = KeyNMF(10, vectorizer=NounPhraseCountVectorizer(\"en_core_web_sm\"))\nmodel.fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking ... 3 fanaticism, theism, fanatism, all fanatism, theists, strong theism, strong atheism, fanatics, precisely some theists, all theism 4 religion foundation darwin fish bumper stickers, darwin fish, atheism, 3d plastic fish, fish symbol, atheist books, atheist organizations, negative atheism, positive atheism, atheism index ... <pre><code>pip install turftopic[spacy]\npython -m spacy download \"en_core_web_sm\"\n</code></pre> <pre><code>from turftopic import KeyNMF\nfrom turftopic.vectorizers.spacy import LemmaCountVectorizer\n\nmodel = KeyNMF(10, vectorizer=LemmaCountVectorizer(\"en_core_web_sm\"))\nmodel.fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking 0 atheist, theist, belief, christians, agnostic, christian, mythology, asimov, abortion, read 1 morality, moral, immoral, objective, society, animal, natural, societal, murder, morally ... <pre><code>from turftopic import KeyNMF\nfrom turftopic.vectorizers.spacy import TokenCountVectorizer\n\n# CountVectorizer for Arabic\nvectorizer = TokenCountVectorizer(\"ar\", min_df=10)\n\nmodel = KeyNMF(\n    n_components=10,\n    vectorizer=vectorizer,\n    encoder=\"Omartificial-Intelligence-Space/Arabic-MiniLM-L12-v2-all-nli-triplet\"\n)\nmodel.fit(corpus)\n</code></pre>"},{"location":"model_definition_and_training/#3-encoder","title":"3. Encoder","text":"<p>Since all models in Turftopic rely on contextual embeddings, you will need to specify a contextual embedding model to use. The default is <code>all-MiniLM-L6-v2</code>, which is a very fast and reasonably performant embedding model for English. You might, however want to use custom embeddings, either because your corpus is not in English, or because you need higher speed or performance. See a detailed guide on Encoders here.</p> <p>Similar to a vectorizer, you can add an encoder to a topic model upon initializing it.</p> <pre><code>from turftopic import KeyNMF\nfrom sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\"parahprase-multilingual-MiniLM-L12-v2\")\nmodel = KeyNMF(10, encoder=encoder)\n</code></pre>"},{"location":"model_definition_and_training/#4-analyzer-optional","title":"4. Analyzer (optional)","text":"<p>Analyzers are large language models, that can be used to generate meaningful topic names and descriptions for a fitted topic model. Analyzers are technically not part of your topic model, and should be used after training. See a detailed guide here.</p> Local LLMChatGPT <pre><code>from turftopic.analyzers import LLMAnalyzer\n\n# We enable document summaries for topic analysis\nanalyzer = LLMAnalyzer(use_summaries=True)\n\nanalysis_res = model.analyze_topics(analyzer)\nprint(analysis_res.topic_names)\n</code></pre> <pre><code>pip install openai\nexport OPENAI_API_KEY=\"sk-&lt;your key goes here&gt;\"\n</code></pre> <pre><code>from turftopic.analyzers import OpenAIAnalyzer\n\n# We enable document summaries for topic analysis\nanalyzer = OpenAIAnalyzer(\"gpt-5-nano\", use_summaries=True)\n\nanalysis_res = model.analyze_topics(analyzer)\nmodel.print_topics()\n</code></pre> Topic ID Topic Name Highest Ranking 0 Operating Systems and Software windows, dos, os, ms, microsoft, unix, nt, memory, program, apps 1 Atheism and Belief Systems atheism, atheist, atheists, belief, religion, religious, theists, beliefs, believe, faith ..."},{"location":"model_definition_and_training/#training-and-inference","title":"Training and Inference","text":""},{"location":"model_definition_and_training/#model-training","title":"Model Training","text":"<p>All models in Turftopic follow a scikit-learn API for fitting topic models. Every model in the library can be trained by passing a set of documents (a corpus) to the model. This has to be an <code>Iterable</code> type object, that has to be reusable as models will typically do multiple passes on the corpus.</p> <pre><code>corpus: list[str] = [\"this is a a document\", \"this is yet another document\", ...]\n</code></pre> <p>Fit your topic model</p> <code>fit(raw_documents, embeddings=None)</code><code>fit_transform(raw_documents, embeddings=None)</code><code>prepare_topic_data(corpus, embeddings=None)</code> <p><code>fit()</code> simply fits the topic model and returns the same model object fitted. You can optionally pass a set of precomputed embeddings for the documents.</p> <pre><code>model.fit(corpus)\n# or\nmodel.fit(corpus, embeddings=embeddings)\n</code></pre> <p><code>fit_transform()</code> not only trains the model but also returns topic-proportions in all documents in the corpus. <pre><code>document_topic_matrix = model.fit_transform(corpus)\n# or \ndocument_topic_matrix = model.fit_transform(corpus, embeddings=embeddings)\nprint(document_topic_matrix.shape)\n# prints (n_documents, n_topics)\n</code></pre></p> <p><code>prepare_topic_data()</code> not only fits the model (only if not already fitted), but also saves other aspects of topic inference, which makes it easier to then use this object for pretty printing and visualizing your models (see Model Interpretation)</p> <p><pre><code>topic_data = model.prepare_topic_data(corpus)\n# print to see what attributes you can access.\nprint(topic_data)\n</code></pre> <pre><code>TopicData\n\u251c\u2500\u2500 corpus (1000)\n\u251c\u2500\u2500 vocab (1746,)\n\u251c\u2500\u2500 document_term_matrix (1000, 1746)\n\u251c\u2500\u2500 topic_term_matrix (10, 1746)\n\u251c\u2500\u2500 document_topic_matrix (1000, 10)\n\u251c\u2500\u2500 document_representation (1000, 384)\n\u251c\u2500\u2500 transform\n\u251c\u2500\u2500 topic_names (10)\n\u251c\u2500\u2500 has_negative_side\n\u2514\u2500\u2500 hierarchy\n</code></pre> See Using TopicData for more detail.</p>"},{"location":"model_definition_and_training/#precomputing-embeddings","title":"Precomputing Embeddings","text":"<p>In order to cut down on costs/computational load when fitting multiple models in a row, you might want to encode the documents before fitting a model. Encoding the corpus is the heaviest part of the process and you can spare yourself a lot of time by only doing it once. Some models have to encode the vocabulary as well, this cannot be done before inference, as the models learn the vocabulary itself from the corpus.</p> <p>The <code>fit()</code> method of all models takes and <code>embeddings</code> argument, that allows you to pass a precooked embedding matrix along to fitting. One thing to watch out for is that you have to pass the embedding model along to the model that was used for encoding the corpus. This is again, to ensure that the vocabulary gets encoded with the same embedding model as the documents.</p> <p>Here's a snippet of correct usage:</p> <pre><code>import numpy as np\nfrom sentence_transformers import SentenceTransformer\nfrom turftopic import GMM, ClusteringTopicModel\n\nencoder = SentenceTransformer(\"intfloat/e5-large-v2\", prompts={\"query\": \"query: \", \"passage\": \"passage: \"}, default_prompt_name=\"query\")\n\ncorpus: list[str] = [\"this is a a document\", \"this is yet another document\", ...]\nembeddings = np.asarray(encoder.encode(corpus))\n\ngmm = GMM(10, encoder=encoder).fit(corpus, embeddings=embeddings)\n\nclustering = ClusteringTopicModel(encoder=encoder).fit(corpus, embeddings=embeddings)\n</code></pre>"},{"location":"model_definition_and_training/#inference","title":"Inference","text":"<p>Some models in Turftopic are capable of estimating topic importance scores for documents in your corpus. In order to get the importance of each topic for the documents in the corpus, you might want to use <code>fit_transform()</code> instead of <code>fit()</code></p> <p>Warning</p> <p>Note that using <code>fit()</code> and <code>transform()</code> in succession is not the same as using <code>fit_transform()</code> and the later should be preferred under all circumstances. For one, not all models have a <code>transform()</code> method, but <code>fit_transform()</code> is also way more efficient, as documents don't have to be encoded twice. Some models have additional optimizations going on when using <code>fit_transform()</code>, and the <code>fit()</code> method typically uses <code>fit_transform()</code> in the background.</p> <pre><code>document_topic_matrix = model.fit_transform(corpus)\n</code></pre> <p>This will give you a matrix, where every row is a document and every column represents the importance of a given topic.</p> <p>You can infer topical content for new documents with a fitted model using the <code>transform()</code> method (beware that this only works with inductive methods):</p> <pre><code>document_topic_matrix = model.transform(new_documents, embeddings=None)\n</code></pre>"},{"location":"model_interpretation/","title":"Interpreting and Visualizing Models","text":"<p>Interpreting topic models can be challenging. Luckily Turftopic comes loaded with a bunch of utilities you can use for interpreting your topic models.</p> <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(10)\ntopic_data = model.prepare_topic_data(corpus)\n</code></pre>"},{"location":"model_interpretation/#topic-tables","title":"Topic Tables","text":"<p>The easiest way you can investigate topics in your fitted model is to use the built-in pretty printing utilities, that you can call on every fitted model or <code>TopicData</code> object.</p> <p>Interpret your models with topic tables</p> Relevant WordsRelevant DocumentsTopic Distributions <pre><code>model.print_topics()\n# or\ntopic_data.print_topics()\n</code></pre> Topic ID Top 10 Words 0 armenians, armenian, armenia, turks, turkish, genocide, azerbaijan, soviet, turkey, azerbaijani 1 sale, price, shipping, offer, sell, prices, interested, 00, games, selling 2 christians, christian, bible, christianity, church, god, scripture, faith, jesus, sin 3 encryption, chip, clipper, nsa, security, secure, privacy, encrypted, crypto, cryptography .... <pre><code># Print highest ranking documents for topic 0\nmodel.print_representative_documents(0, corpus, document_topic_matrix)\n\n# since topic_data already stores the corpus and the doc-topic-matrix, you only need to give a topic ID\ntopic_data.print_representative_documents(0)\n</code></pre> Document Score Poor 'Poly'. I see you're preparing the groundwork for yet another retreat from your... 0.40 Then you must be living in an alternate universe. Where were they? An Appeal to Mankind During the... 0.40 It is 'Serdar', 'kocaoglan'. Just love it. Well, it could be your head wasn't screwed on just right... 0.39 <pre><code>document = \"I think guns should definitely banned from all public institutions, such as schools.\"\n\nmodel.print_topic_distribution(document)\n# or \ntopic_data.print_topic_distribution(document)\n</code></pre> Topic name Score 7_gun_guns_firearms_weapons 0.05 17_mail_address_email_send 0.00 3_encryption_chip_clipper_nsa 0.00 19_baseball_pitching_pitcher_hitter 0.00 11_graphics_software_program_3d 0.00 <p>You can also export tables as pandas DataFrames by removing the <code>print_</code> prefix, and postfixing the method with <code>_df</code> or export tables in a given format, by using the <code>export_&lt;something&gt;</code> method instead of <code>print_&lt;something&gt;</code>.</p> <code>DataFrame</code>MarkdownLatexCSV <pre><code>model.topics_df()\nmodel.topic_distribution_df(\"something something\")\ntopic_data.representative_documents_df(5)\n</code></pre> <pre><code>model.export_topics(format=\"markdown\")\nmodel.export_topic_distribution(\"something something\", format=\"markdown\")\ntopic_data.export_representative_documents(5, format=\"markdown\")\n</code></pre> <pre><code>model.export_topics(format=\"latex\")\nmodel.export_topic_distribution(\"something something\", format=\"latex\")\ntopic_data.export_representative_documents(5, format=\"latex\")\n</code></pre> <pre><code>model.export_topics(format=\"csv\")\nmodel.export_topic_distribution(\"something something\", format=\"csv\")\ntopic_data.export_representative_documents(5, format=\"csv\")\n</code></pre>"},{"location":"model_interpretation/#visualization-with-topicwizard","title":"Visualization with topicwizard","text":"<p>Turftopic comes with a number of model-specific visualization utilities, which you can check out on the models page. We do provide a general overview here, as well as instructions on how to use topicwizard with Turftopic for interactive topic interpretation.</p> <p>To use topicwizard you will first have to install it: <pre><code>pip install topic-wizard\n</code></pre></p> <p></p>"},{"location":"model_interpretation/#web-app","title":"Web App","text":"<p>The easiest way to investigate any topic model interactively is to use the topicwizard web app. You can launch the app either using a <code>TopicData</code> or a model object and a representative sample of documents.</p> With <code>TopicData</code>With <code>model</code> <pre><code>topic_data.visualize_topicwizard()\n</code></pre> <pre><code>import topicwizard\n\ntopicwizard.visualize(corpus=documents, model=model)\n</code></pre> <p></p>"},{"location":"model_interpretation/#figures","title":"Figures","text":"<p>You can also produce individual interactive figures using the Figures API in topicwizard. Almost all figures in the Figures API can be called on the <code>figures</code> submodule of any <code>TopicData</code> object.</p> <p>Interpret your models using interactive figures</p> Topic MapTopic BarchartsWord MapWord CloudsDocument Map <pre><code>topic_data.figures.topic_map()\n</code></pre> <p> </p> <pre><code>topic_data.figures.topic_barcharts()\n</code></pre> <p> </p> <pre><code>topic_data.figures.word_map()\n</code></pre> <p> </p> <pre><code>topic_data.figures.topic_wordclouds()\n</code></pre> <p> </p> <pre><code>topic_data.figures.document_map()\n</code></pre> <p> </p>"},{"location":"model_interpretation/#visualizing-with-datamapplot","title":"Visualizing with Datamapplot","text":"<p>You can interactively explore clusters using datamapplot directly in Turftopic. We have made some customizations to datamapplot to allow for easier topic exploration. You will first have to install <code>datamapplot</code> for this to work:</p> <pre><code>pip install turftopic[datamapplot]\n</code></pre>"},{"location":"model_interpretation/#clustering-models","title":"Clustering Models","text":"<p>Datamapplot works natively with clustering topic models in Turftopic, which already reduce document embeddings to a lower number of displayable dimensions and assign cluster labels. You can run datamapplot for any clustering model like so:</p> <pre><code>from turftopic import ClusteringTopicModel\nfrom turftopic.analyzers import OpenAIAnalyzer\n\n# Also works with BERTopic and Top2Vec\nmodel = ClusteringTopicModel().fit(corpus)\n\nanalyzer = OpenAIAnalyzer(\"gpt-5-nano\")\nanalysis_res = model.analyze_topics(analyzer)\n\n# We make sure that the users can hover over points and see the underlying document.\nfig = model.plot_clusters_datamapplot(hover_text=corpus)\nfig.save(\"clusters_visualization.html\")\nfig\n</code></pre> <p>Info</p> <p>If you are not running Turftopic from a Jupyter notebook, make sure to call <code>fig.show()</code>. This will open up a new browser tab with the interactive figure.</p>  Interactive figure to explore cluster structure in a clustering topic model."},{"location":"model_interpretation/#custom-datamapplot","title":"Custom Datamapplot","text":"<p>You can now also use Turftopic's custom datamapplot to display information from other topic models, which do not reduce embeddings as part of their pipeline. This is not entirely automatized, since there are many choices you have to make about how to calculate positions and color documents.</p> <p>Here's an example with KeyNMF:</p> <pre><code>from sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nfrom turftopic import KeyNMF, build_datamapplot\n\nmodel = KeyNMF(10)\ndocument_topic_matrix = model.fit_transform(corpus)\n\n# We use the document-topic-proportions to project to 2D:\nscaled = StandardScaler().fit_transform(document_topic_matrix)\nprojected = TSNE(n_components=2).fit_transform(scaled)\n# We assign the most relevant topic label to each document\nlabels = np.argmax(document_topic_matrix, axis=1)\n\nfig = build_datamapplot(\n    coordinates=projected,\n    labels=labels,\n    topic_names=model.topic_names,\n    classes=np.arange(model.n_components),\n    # Boundaries are unlikely to be very clear\n    cluster_boundary_polygons=False,\n)\nfig.show()\n</code></pre>"},{"location":"model_interpretation/#api-reference","title":"API Reference","text":""},{"location":"model_interpretation/#turftopic.build_datamapplot","title":"<code>turftopic.build_datamapplot(coordinates, topic_names, labels, classes, top_words=None, topic_descriptions=None, font_family='Merriweather', enable_topic_tree=True, topic_tree_kwds={'color_bullets': True}, cluster_boundary_polygons=False, cluster_boundary_line_width=6, polygon_alpha=2, **kwargs)</code>","text":"<p>Builds a Turftopic interactive datamapplot.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates</code> <code>ndarray</code> <p>X and Y coordinates of datapoints.</p> required <code>topic_names</code> <code>list[str]</code> <p>Names of topics in the model.</p> required <code>labels</code> <code>ndarray</code> <p>Topic labels for each datapoint (topic_id for each point)</p> required <code>classes</code> <code>ndarray</code> <p>List of topic IDs in the model.</p> required <code>top_words</code> <code>Optional[list[list[str]]]</code> <p>List of top keywords for each topic.</p> <code>None</code> <code>topic_descriptions</code> <code>Optional[list[str]]</code> <p>List of descriptions for the given topics.</p> <code>None</code> Source code in <code>turftopic/_datamapplot.py</code> <pre><code>def build_datamapplot(\n    coordinates: np.ndarray,\n    topic_names: list[str],\n    labels: np.ndarray,\n    classes: np.ndarray,\n    top_words: Optional[list[list[str]]] = None,\n    topic_descriptions: Optional[list[str]] = None,\n    font_family: str = \"Merriweather\",\n    enable_topic_tree=True,\n    topic_tree_kwds={\n        \"color_bullets\": True,\n    },\n    cluster_boundary_polygons=False,\n    cluster_boundary_line_width=6,\n    polygon_alpha=2,\n    **kwargs,\n):\n    \"\"\"Builds a Turftopic interactive datamapplot.\n\n    Parameters\n    ----------\n    coordinates: np.ndarray\n        X and Y coordinates of datapoints.\n    topic_names: list[str]\n        Names of topics in the model.\n    labels: np.ndarray\n        Topic labels for each datapoint (topic_id for each point)\n    classes: np.ndarray\n        List of topic IDs in the model.\n    top_words: list[list[str]], optional\n        List of top keywords for each topic.\n    topic_descriptions: list[str], optional\n        List of descriptions for the given topics.\n    \"\"\"\n    try:\n        import datamapplot\n    except ModuleNotFoundError as e:\n        raise ModuleNotFoundError(\n            \"You need to install datamapplot to be able to use plot_clusters_datamapplot().\"\n        ) from e\n    coordinates = scale(coordinates) * 4\n    indices = _labels_to_indices(labels, classes)\n    labels = np.array(topic_names)[indices]\n    if -1 in classes:\n        i_outlier = np.where(classes == -1)[0][0]\n        kwargs[\"noise_label\"] = topic_names[i_outlier]\n    # Calculating how much of the corpus is made up of a topic\n    percentages = []\n    for label in topic_names:\n        percentages.append(100 * np.sum(labels == label) / len(labels))\n    # Sanitizing the names so they don't mess up the HTML\n    topic_names = [sanitize_for_html(name) for name in topic_names]\n    custom_js = \"\"\n    custom_js += \"const nameToPercent = new Map();\\n\"\n    for name, percent in zip(topic_names, percentages):\n        custom_js += 'nameToPercent.set(\"{name}\", {percent});\\n'.format(\n            name=name,\n            percent=percent,\n        )\n    custom_js += \"const nameToDesc = new Map();\\n\"\n    if topic_descriptions is not None:\n        topic_descriptions = [\n            sanitize_for_html(desc) for desc in topic_descriptions\n        ]\n        for topic_id, name, desc in zip(\n            classes, topic_names, topic_descriptions\n        ):\n            custom_js += 'nameToDesc.set(\"{name}\", \"{desc}\");\\n'.format(\n                name=name,\n                desc=desc,\n            )\n    custom_js += \"const nameToKeywords = new Map();\\n\"\n    if top_words is not None:\n        for name, words in zip(topic_names, top_words):\n            custom_js += (\n                'nameToKeywords.set(\"{name}\", \"{keywords}\");\\n'.format(\n                    name=name, keywords=\", \".join(words)\n                )\n            )\n    custom_html = \"\"\n    custom_html += \"\"\"\n&lt;div class=\"description\"&gt;\n    &lt;h3 id=\"topic-name\"&gt;{topic_name}&lt;/h3&gt;\n    &lt;p id=\"topic-keywords\"&gt;{topic_keywords}&lt;/p&gt;\n    &lt;progress id=\"topic-percent\" value=\"{percentage:.2f}\" max=\"100\"&gt;&lt;/progress&gt;\n    &lt;b id=\"percent-message\"&gt;{percentage:.2f}% of all documents&lt;/b&gt;\n    &lt;hr&gt;\n    &lt;p id=\"topic-description\"&gt;{topic_description}&lt;/p&gt;\n&lt;/div&gt;\n    \"\"\".format(\n        topic_name=topic_names[1],\n        topic_description=(\n            topic_descriptions[1] if topic_descriptions is not None else \"\"\n        ),\n        topic_keywords=(\n            \"Keywords: \"\n            + (\", \".join(top_words[1]) if top_words is not None else \"\")\n        ),\n        percentage=percentages[1],\n    )\n    custom_js += \"\"\"\nsetTimeout(function(){\n    const labelNodes = Array.from(document.getElementsByClassName('topic-tree-label'))\n    labelNodes.forEach(button =&gt; {\n        console.log(\"Found this button\")\n        button.addEventListener('click', function(event) {\n            const topicName = document.getElementById(\"topic-name\");\n            const topicDesc = document.getElementById(\"topic-description\");\n            const topicKeywords = document.getElementById(\"topic-keywords\");\n            const topicPercent = document.getElementById(\"topic-percent\");\n            const percentMessage = document.getElementById(\"percent-message\");\n            const name = button.textContent.replace(/[\\\\n\\\\r\\\\t]/gm, \" \");\n            topicName.textContent = name;\n            const percent = nameToPercent.get(name);\n            topicPercent.value = percent;\n            percentMessage.textContent = percent.toFixed(2) + \"% of all documents\";\n            const description = nameToDesc.get(name);\n            console.log(description)\n            if (description) {\n                topicDesc.textContent = description;\n            } else {\n                topicDesc.textContent = \"\";\n            }\n            const keywords = nameToKeywords.get(name);\n            console.log(keywords)\n            if (keywords) {\n                topicKeywords.textContent = \"Keywords: \" + keywords;\n            } else {\n                topicKeywords.textContent = \"\";\n            }\n        });\n    });\n}, 200);\n    \"\"\"\n    plot = datamapplot.create_interactive_plot(\n        coordinates,\n        labels,\n        font_family=font_family,\n        logo=\"https://x-tabdeveloping.github.io/turftopic/images/logo.svg\",\n        logo_width=80,\n        enable_topic_tree=enable_topic_tree,\n        topic_tree_kwds=topic_tree_kwds,\n        cluster_boundary_polygons=cluster_boundary_polygons,\n        cluster_boundary_line_width=cluster_boundary_line_width,\n        polygon_alpha=polygon_alpha,\n        custom_css=CUSTOM_CSS,\n        custom_html=custom_html,\n        custom_js=custom_js,\n        **kwargs,\n    )\n\n    def show_fig():\n        with tempfile.TemporaryDirectory() as temp_dir:\n            file_name = Path(temp_dir).joinpath(\"fig.html\")\n            plot.save(file_name)\n            webbrowser.open(\"file://\" + str(file_name.absolute()), new=2)\n            time.sleep(2)\n\n    plot.show = show_fig\n    plot.write_html = plot.save\n    return plot\n</code></pre>"},{"location":"model_interpretation/#analyzing-and-naming-topics-with-llms","title":"Analyzing and Naming Topics with LLMS","text":"<p>Analyzers are large language models, that can be used to generate meaningful topic names and descriptions for a fitted topic model. See a our detailed guide about Analyzers to learn how you can use LLMs to assign names to topics.</p> <p>You can also manually label topics if you wish.</p> <p>Examples</p> AutomatedManual <pre><code>from turftopic import KeyNMF\nfrom turftopic.namers import OpenAIAnalyzer\n\nanalyzer = OpenAIAnalyzer(\"gpt-5-nano\")\nanalysis_res = model.analyze_topics(analyzer)\n\nmodel.print_topics()\n</code></pre> Topic ID Topic Name Highest Ranking 0 Operating Systems and Software windows, dos, os, ms, microsoft, unix, nt, memory, program, apps 1 Atheism and Belief Systems atheism, atheist, atheists, belief, religion, religious, theists, beliefs, believe, faith 2 Computer Architecture and Performance motherboard, ram, memory, cpu, bios, isa, speed, 486, bus, performance ... <pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(10).fit(corpus)\nmodel.rename_topics({0: \"New name for topic 0\", 5: \"New name for topic 5\"})\n</code></pre>"},{"location":"model_interpretation/#api-reference_1","title":"API Reference","text":""},{"location":"model_interpretation/#turftopic.container.TopicContainer","title":"<code>turftopic.container.TopicContainer</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Base class for classes that contain topical information.</p> Source code in <code>turftopic/container.py</code> <pre><code>class TopicContainer(ABC):\n    \"\"\"Base class for classes that contain topical information.\"\"\"\n\n    @property\n    def has_negative_side(self) -&gt; bool:\n        return np.any(self.components_ &lt; 0)\n\n    def get_topics(\n        self, top_k: int = 10\n    ) -&gt; List[Tuple[Any, List[Tuple[str, float]]]]:\n        \"\"\"Returns high-level topic representations in form of the top K words\n        in each topic.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of top words to return for each topic.\n\n        Returns\n        -------\n        list[tuple]\n            List of topics. Each topic is a tuple of\n            topic ID and the top k words.\n            Top k words are a list of (word, word_importance) pairs.\n        \"\"\"\n        n_topics = self.components_.shape[0]\n        try:\n            classes = self.classes_\n        except AttributeError:\n            classes = list(range(n_topics))\n        highest = np.argpartition(-self.components_, top_k)[:, :top_k]\n        vocab = self.get_vocab()\n        top = []\n        score = []\n        for component, high in zip(self.components_, highest):\n            importance = component[high]\n            high = high[np.argsort(-importance)]\n            score.append(component[high])\n            top.append(vocab[high])\n        topics = []\n        for topic, words, scores in zip(classes, top, score):\n            topic_data = (topic, list(zip(words, scores)))\n            topics.append(topic_data)\n        return topics\n\n    def _top_terms(\n        self, top_k: int = 10, positive: bool = True\n    ) -&gt; list[list[str]]:\n        terms = []\n        vocab = self.get_vocab()\n        for component in self.components_:\n            lowest = np.argpartition(component, top_k)[:top_k]\n            lowest = lowest[np.argsort(component[lowest])]\n            highest = np.argpartition(-component, top_k)[:top_k]\n            highest = highest[np.argsort(-component[highest])]\n            if not positive:\n                terms.append(list(vocab[lowest]))\n            else:\n                terms.append(list(vocab[highest]))\n        return terms\n\n    def get_top_words(\n        self, top_k: int = 10, positive: bool = True\n    ) -&gt; list[list[str]]:\n        \"\"\"Returns list of top words for each topic.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of words to return.\n        positive: bool, default True\n            Indicates whether the highest\n            or lowest scoring terms should be returned.\n        \"\"\"\n        return self._top_terms(top_k, positive)\n\n    def get_top_documents(\n        self,\n        raw_documents=None,\n        document_topic_matrix=None,\n        top_k: int = 10,\n        positive: bool = True,\n    ) -&gt; list[list[str]]:\n        \"\"\"Returns list of top documents for each topic.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of documents to return per topic.\n        positive: bool, default True\n            Indicates whether the highest\n            or lowest scoring documents should be returned.\n        \"\"\"\n        if (\n            positive\n            and hasattr(self, \"top_documents\")\n            and len(self.top_documents[0]) &gt;= top_k\n        ):\n            return [docs[:top_k] for docs in self.top_documents]\n        if (\n            not positive\n            and hasattr(self, \"negative_documents\")\n            and len(self.negative_documents[0]) &gt;= top_k\n        ):\n            return [docs[:top_k] for docs in self.negative_documents]\n        docs = []\n        raw_documents = raw_documents or getattr(self, \"corpus\", None)\n        if raw_documents is None:\n            raise ValueError(\n                \"No corpus was passed, can't search for representative documents.\"\n            )\n        if document_topic_matrix is None:\n            document_topic_matrix = getattr(\n                self, \"document_topic_matrix\", None\n            )\n        if document_topic_matrix is None:\n            try:\n                document_topic_matrix = self.transform(raw_documents)\n            except AttributeError:\n                raise ValueError(\n                    \"Transductive methods cannot \"\n                    \"infer topical content in documents.\\n\"\n                    \"Please pass a document_topic_matrix.\"\n                )\n        for topic_doc_vec in document_topic_matrix.T:\n            if positive:\n                topic_doc_vec = -topic_doc_vec\n            highest = np.argsort(topic_doc_vec)[:top_k]\n            docs.append([raw_documents[i_doc] for i_doc in highest])\n        return docs\n\n    def get_top_images(self, top_k: int = True, positive: bool = True):\n        \"\"\"Returns list of top images for each topic.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of images to return.\n        positive: bool, default True\n            Indicates whether the highest\n            or lowest scoring images should be returned.\n        \"\"\"\n        if not hasattr(self, \"top_images\"):\n            raise ValueError(\n                \"Model either has not been fit or was fit without images. top_images property missing.\"\n            )\n        if (not positive) and not hasattr(self, \"negative_images\"):\n            raise ValueError(\n                \"Model either has not been fit or was fit without images. top_images property missing.\"\n            )\n        top_images = self.top_images if positive else self.negative_images\n        ims = []\n        for topic_images in top_images:\n            if len(topic_images) &lt; top_k:\n                warnings.warn(\n                    \"Number of images stored in the topic model is smaller than the specified top_k, returning all that the model has.\"\n                )\n            ims.append(topic_images[:top_k])\n        return ims\n\n    def _rename_automatic(\n        self, analyzer: Analyzer, use_documents: bool = True\n    ) -&gt; list[str]:\n        try:\n            documents = self.get_top_documents()\n        except ValueError as e:\n            warnings.warn(\n                f\"Couldn't get top documents, proceeding only with keywords: {e}\"\n            )\n            documents = None\n        if not use_documents:\n            documents = None\n        self.topic_names_ = analyzer.name_topics(\n            self._top_terms(), documents=documents\n        )\n        return self.topic_names_\n\n    def _topics_table(\n        self,\n        top_k: int = 10,\n        show_scores: bool = False,\n        show_negative: Optional[bool] = None,\n    ) -&gt; list[list[str]]:\n        if show_negative is None:\n            show_negative = self.has_negative_side\n        columns = [\"Topic ID\"]\n        if getattr(self, \"topic_names_\", None):\n            columns.append(\"Topic Name\")\n        if getattr(self, \"topic_descriptions\", None):\n            columns.append(\"Topic Descriptions\")\n        columns.append(\"Highest Ranking\")\n        if show_negative:\n            columns.append(\"Lowest Ranking\")\n        rows = []\n        try:\n            classes = self.classes_\n        except AttributeError:\n            classes = list(range(self.components_.shape[0]))\n        vocab = self.get_vocab()\n        for i_topic, (topic_id, component) in enumerate(\n            zip(classes, self.components_)\n        ):\n            highest = np.argpartition(-component, top_k)[:top_k]\n            highest = highest[np.argsort(-component[highest])]\n            lowest = np.argpartition(component, top_k)[:top_k]\n            lowest = lowest[np.argsort(component[lowest])]\n            if show_scores:\n                concat_positive = \", \".join(\n                    [\n                        f\"{word}({importance:.2f})\"\n                        for word, importance in zip(\n                            vocab[highest], component[highest]\n                        )\n                    ]\n                )\n                concat_negative = \", \".join(\n                    [\n                        f\"{word}({importance:.2f})\"\n                        for word, importance in zip(\n                            vocab[lowest], component[lowest]\n                        )\n                    ]\n                )\n            else:\n                concat_positive = \", \".join([word for word in vocab[highest]])\n                concat_negative = \", \".join([word for word in vocab[lowest]])\n            row = [f\"{topic_id}\"]\n            if getattr(self, \"topic_names_\", None):\n                row.append(self.topic_names_[i_topic])\n            if getattr(self, \"topic_descriptions\", None):\n                row.append(self.topic_descriptions[i_topic])\n            row.append(f\"{concat_positive}\")\n            if show_negative:\n                row.append(concat_negative)\n            rows.append(row)\n        return [columns, *rows]\n\n    def print_topics(\n        self,\n        top_k: int = 10,\n        show_scores: bool = False,\n        show_negative: Optional[bool] = None,\n    ):\n        \"\"\"Pretty prints topics in the model in a table.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of top words to return for each topic.\n        show_scores: bool, default False\n            Indicates whether to show importance scores for each word.\n        show_negative: bool, default False\n            Indicates whether the most negative terms should also be displayed.\n        \"\"\"\n        columns, *rows = self._topics_table(top_k, show_scores, show_negative)\n        table = Table(show_lines=True)\n        for column in columns:\n            if column == \"Highest Ranking\":\n                table.add_column(\n                    column, justify=\"left\", style=\"magenta\", max_width=100\n                )\n            elif column == \"Lowest Ranking\":\n                table.add_column(\n                    column, justify=\"left\", style=\"red\", max_width=100\n                )\n            elif column == \"Topic ID\":\n                table.add_column(column, style=\"blue\", justify=\"right\")\n            else:\n                table.add_column(column)\n        for row in rows:\n            table.add_row(*row)\n        console = Console()\n        console.print(table)\n\n    def export_topics(\n        self,\n        top_k: int = 10,\n        show_scores: bool = False,\n        show_negative: Optional[bool] = None,\n        format: str = \"csv\",\n    ) -&gt; str:\n        \"\"\"Exports top K words from topics in a table in a given format.\n        Returns table as a pure string.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of top words to return for each topic.\n        show_scores: bool, default False\n            Indicates whether to show importance scores for each word.\n        show_negative: bool, default False\n            Indicates whether the most negative terms should also be displayed.\n        format: 'csv', 'latex' or 'markdown'\n            Specifies which format should be used.\n            'csv', 'latex' and 'markdown' are supported.\n        \"\"\"\n        table = self._topics_table(\n            top_k, show_scores, show_negative=show_negative\n        )\n        return export_table(table, format=format)\n\n    def _representative_docs(\n        self,\n        topic_id,\n        raw_documents=None,\n        document_topic_matrix=None,\n        top_k=5,\n        show_negative: Optional[bool] = None,\n    ) -&gt; list[list[str]]:\n        if show_negative is None:\n            show_negative = self.has_negative_side\n        raw_documents = (\n            raw_documents\n            if raw_documents is not None\n            else getattr(self, \"corpus\", None)\n        )\n        if raw_documents is None:\n            raise ValueError(\n                \"No corpus was passed, can't search for representative documents.\"\n            )\n        document_topic_matrix = (\n            document_topic_matrix\n            if document_topic_matrix is not None\n            else getattr(self, \"document_topic_matrix\", None)\n        )\n        if document_topic_matrix is None:\n            try:\n                document_topic_matrix = self.transform(raw_documents)\n            except AttributeError:\n                raise ValueError(\n                    \"Transductive methods cannot \"\n                    \"infer topical content in documents.\\n\"\n                    \"Please pass a document_topic_matrix.\"\n                )\n        try:\n            topic_id = list(self.classes_).index(topic_id)\n        except AttributeError:\n            pass\n        kth = min(top_k, document_topic_matrix.shape[0] - 1)\n        highest = np.argpartition(-document_topic_matrix[:, topic_id], kth)[\n            :kth\n        ]\n        highest = highest[\n            np.argsort(-document_topic_matrix[highest, topic_id])\n        ]\n        scores = document_topic_matrix[highest, topic_id]\n        columns = []\n        columns.append(\"Document\")\n        columns.append(\"Score\")\n        rows = []\n        for document_id, score in zip(highest, scores):\n            doc = raw_documents[document_id]\n            doc = remove_whitespace(doc)\n            if len(doc) &gt; 300:\n                doc = doc[:300] + \"...\"\n            rows.append([doc, f\"{score:.2f}\"])\n        if show_negative:\n            rows.append([\"...\", \"\"])\n            lowest = np.argpartition(document_topic_matrix[:, topic_id], kth)[\n                :kth\n            ]\n            lowest = lowest[\n                np.argsort(document_topic_matrix[lowest, topic_id])\n            ]\n            lowest = lowest[::-1]\n            scores = document_topic_matrix[lowest, topic_id]\n            for document_id, score in zip(lowest, scores):\n                doc = raw_documents[document_id]\n                doc = remove_whitespace(doc)\n                if len(doc) &gt; 300:\n                    doc = doc[:300] + \"...\"\n                rows.append([doc, f\"{score:.2f}\"])\n        return [columns, *rows]\n\n    def print_representative_documents(\n        self,\n        topic_id,\n        raw_documents=None,\n        document_topic_matrix=None,\n        top_k=5,\n        show_negative: Optional[bool] = None,\n    ):\n        \"\"\"Pretty prints the highest ranking documents in a topic.\n\n        Parameters\n        ----------\n        topic_id: int\n            ID of the topic to display.\n        raw_documents: list of str\n            List of documents to consider.\n        document_topic_matrix: ndarray of shape (n_documents, n_topics), optional\n            Document topic matrix to use. This is useful for transductive methods,\n            as they cannot infer topics from text.\n        top_k: int, default 5\n            Top K documents to show.\n        show_negative: bool, default False\n            Indicates whether lowest ranking documents should also be shown.\n        \"\"\"\n        columns, *rows = self._representative_docs(\n            topic_id,\n            raw_documents,\n            document_topic_matrix,\n            top_k,\n            show_negative,\n        )\n        table = Table(show_lines=True)\n        table.add_column(\n            \"Document\", justify=\"left\", style=\"magenta\", max_width=100\n        )\n        table.add_column(\"Score\", style=\"blue\", justify=\"right\")\n        for row in rows:\n            table.add_row(*row)\n        console = Console()\n        console.print(table)\n\n    def export_representative_documents(\n        self,\n        topic_id,\n        raw_documents=None,\n        document_topic_matrix=None,\n        top_k=5,\n        show_negative: Optional[bool] = None,\n        format: str = \"csv\",\n    ):\n        \"\"\"Exports the highest ranking documents in a topic as a text table.\n\n        Parameters\n        ----------\n        topic_id: int\n            ID of the topic to display.\n        raw_documents: list of str\n            List of documents to consider.\n        document_topic_matrix: ndarray of shape (n_topics, n_topics), optional\n            Document topic matrix to use. This is useful for transductive methods,\n            as they cannot infer topics from text.\n        top_k: int, default 5\n            Top K documents to show.\n        show_negative: bool, default False\n            Indicates whether lowest ranking documents should also be shown.\n        format: 'csv', 'latex' or 'markdown'\n            Specifies which format should be used.\n            'csv', 'latex' and 'markdown' are supported.\n        \"\"\"\n        table = self._representative_docs(\n            topic_id,\n            raw_documents,\n            document_topic_matrix,\n            top_k,\n            show_negative,\n        )\n        return export_table(table, format=format)\n\n    @property\n    def topic_names(self) -&gt; list[str]:\n        \"\"\"Names of the topics based on the highest scoring 4 terms.\"\"\"\n        topic_names = getattr(self, \"topic_names_\", None)\n        if topic_names is not None:\n            return list(topic_names)\n        topic_desc = self.get_topics(top_k=4)\n        names = []\n        for topic_id, terms in topic_desc:\n            concat_words = \"_\".join([word for word, importance in terms])\n            names.append(f\"{topic_id}_{concat_words}\")\n        return names\n\n    def analyze_topics(\n        self,\n        analyzer: Analyzer,\n        use_documents: bool = True,\n        use_summaries: Optional[bool] = None,\n    ) -&gt; AnalysisResults:\n        \"\"\"Analyzes topics in a fitted topic model using an Analyzer.\n\n        Example\n        -------\n        ```python\n        from turftopic.analyzers import T5Analyzer\n\n        model = KeyNMF(10).fit(corpus)\n        analyzer = T5Analyzer()\n        res = model.analyze_topics(analyzer)\n        ```\n\n        Parameters\n        ----------\n        analyzer: Analyzer\n            Large language model to analyze the topics in your topic model.\n        use_documents: bool, default True\n            Indicates whether top documents should be involved in analyzing topics.\n        use_summaries: bool, optional\n            Indicates whether the analyzer should first summarize the most relevant documents.\n            This can be beneficial when your corpus includes very long documents.\n\n        Returns\n        -------\n        AnalysisResults\n            Analysis results. Dataclass containing `topic_names`, `topic_descriptions`\n            and `document_summaries` if relevant.\n        \"\"\"\n        try:\n            documents = self.get_top_documents()\n        except ValueError as e:\n            warnings.warn(\n                f\"Couldn't get top documents, proceeding only with keywords: {e}\"\n            )\n            documents = None\n        if not use_documents:\n            documents = None\n        res = analyzer.analyze_topics(\n            keywords=self._top_terms(),\n            documents=documents,\n            use_summaries=use_summaries,\n        )\n        self.topic_names_ = res.topic_names\n        if res.document_summaries is not None:\n            self.document_summaries = res.document_summaries\n        self.topic_descriptions = res.topic_descriptions\n        if -1 in getattr(self, \"classes_\", ()):\n            id_to_idx = dict(zip(self.classes_, range(len(self.classes_))))\n            self.topic_names_[id_to_idx[-1]] = \"Outliers\"\n            if self.topic_descriptions is not None:\n                self.topic_descriptions[id_to_idx[-1]] = (\n                    \"Topic containing outlier documents.\"\n                )\n        return res\n\n    def rename_topics(\n        self,\n        names: Union[list[str], dict[int, str], Analyzer],\n        use_documents: bool = True,\n    ) -&gt; None:\n        \"\"\"Rename topics in a model manually or automatically, using a namer.\n\n        Examples:\n        ```python\n        model.rename_topics([\"Automobiles\", \"Telephones\"])\n        # Or:\n        model.rename_topics({-1: \"Outliers\", 2: \"Christianity\"})\n        # Or:\n        namer = OpenAIAnalyzer()\n        model.rename_topics(namer)\n        ```\n\n        Parameters\n        ----------\n        names: list[str] or dict[int,str]\n            Should be a list of topic names, or a mapping of topic IDs to names.\n        use_documents: bool, default True\n            Indicates whether documents should be used when naming topics with an analyzer.\n        \"\"\"\n        if isinstance(names, Analyzer):\n            self._rename_automatic(names, use_documents=use_documents)\n        elif isinstance(names, dict):\n            topic_names = self.topic_names\n            for topic_id, topic_name in names.items():\n                try:\n                    topic_id = list(self.classes_).index(topic_id)\n                except AttributeError:\n                    pass\n                topic_names[topic_id] = topic_name\n            self.topic_names_ = topic_names\n        else:\n            names = list(names)\n            n_given = len(names)\n            n_topics = self.components_.shape[0]\n            if n_topics != n_given:\n                raise ValueError(\n                    f\"Number of topics ({n_topics}) doesn't match the length of the given topic name list ({n_given}).\"\n                )\n            self.topic_names_ = names\n\n    def _topic_distribution(\n        self, text=None, topic_dist=None, top_k: int = 10\n    ) -&gt; list[list[str]]:\n        if topic_dist is None:\n            if text is None:\n                raise ValueError(\n                    \"You should either pass a text or a distribution.\"\n                )\n            try:\n                topic_dist = self.transform([text])\n            except AttributeError:\n                raise ValueError(\n                    \"Transductive methods cannot \"\n                    \"infer topical content in documents.\\n\"\n                    \"Please pass a topic distribution.\"\n                )\n        topic_dist = np.squeeze(np.asarray(topic_dist))\n        highest = np.argsort(-topic_dist)[:top_k]\n        columns = []\n        columns.append(\"Topic name\")\n        columns.append(\"Score\")\n        rows = []\n        for ind in highest:\n            score = topic_dist[ind]\n            rows.append([self.topic_names[ind], f\"{score:.2f}\"])\n        return [columns, *rows]\n\n    def print_topic_distribution(\n        self, text=None, topic_dist=None, top_k: int = 10\n    ):\n        \"\"\"Pretty prints topic distribution in a document.\n\n        Parameters\n        ----------\n        text: str, optional\n            Text to infer topic distribution for.\n        topic_dist: ndarray of shape (n_topics), optional\n            Already inferred topic distribution for the text.\n            This is useful for transductive methods,\n            as they cannot infer topics from text.\n        top_k: int, default 10\n            Top K topics to show.\n        \"\"\"\n        columns, *rows = self._topic_distribution(text, topic_dist, top_k)\n        table = Table()\n        table.add_column(\"Topic name\", justify=\"left\", style=\"magenta\")\n        table.add_column(\"Score\", justify=\"right\", style=\"blue\")\n        for row in rows:\n            table.add_row(*row)\n        console = Console()\n        console.print(table)\n\n    def export_topic_distribution(\n        self, text=None, topic_dist=None, top_k: int = 10, format=\"csv\"\n    ) -&gt; str:\n        \"\"\"Exports topic distribution as a text table.\n\n        Parameters\n        ----------\n        text: str, optional\n            Text to infer topic distribution for.\n        topic_dist: ndarray of shape (n_topics), optional\n            Already inferred topic distribution for the text.\n            This is useful for transductive methods,\n            as they cannot infer topics from text.\n        top_k: int, default 10\n            Top K topics to show.\n        format: 'csv', 'latex' or 'markdown'\n            Specifies which format should be used.\n            'csv', 'latex' and 'markdown' are supported.\n        \"\"\"\n        table = self._topic_distribution(text, topic_dist, top_k)\n        return export_table(table, format=format)\n\n    def topics_df(\n        self,\n        top_k: int = 10,\n        show_scores: bool = False,\n        show_negative: Optional[bool] = None,\n    ):\n        \"\"\"Extracts topics into a pandas dataframe.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of top words to return for each topic.\n        show_scores: bool, default False\n            Indicates whether to show importance scores for each word.\n        show_negative: bool, default False\n            Indicates whether the most negative terms should also be displayed.\n        \"\"\"\n        try:\n            import pandas as pd\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"You need to pip install pandas to be able to use dataframes.\"\n            )\n        columns, *rows = self._topics_table(top_k, show_scores, show_negative)\n        return pd.DataFrame(rows, columns=columns)\n\n    def representative_documents_df(\n        self,\n        topic_id,\n        raw_documents=None,\n        document_topic_matrix=None,\n        top_k=5,\n        show_negative: Optional[bool] = None,\n    ):\n        \"\"\"Collects highest ranking documents in a topic to a dataframe.\n\n        Parameters\n        ----------\n        topic_id: int\n            ID of the topic to display.\n        raw_documents: list of str\n            List of documents to consider.\n        document_topic_matrix: ndarray of shape (n_documents, n_topics), optional\n            Document topic matrix to use. This is useful for transductive methods,\n            as they cannot infer topics from text.\n        top_k: int, default 5\n            Top K documents to show.\n        show_negative: bool, default False\n            Indicates whether lowest ranking documents should also be shown.\n        \"\"\"\n        try:\n            import pandas as pd\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"You need to pip install pandas to be able to use dataframes.\"\n            )\n        if show_negative is None:\n            show_negative = self.has_negative_side\n        raw_documents = raw_documents or getattr(self, \"corpus\", None)\n        if raw_documents is None:\n            raise ValueError(\n                \"No corpus was passed, can't search for representative documents.\"\n            )\n        document_topic_matrix = document_topic_matrix or getattr(\n            self, \"document_topic_matrix\", None\n        )\n        if document_topic_matrix is None:\n            try:\n                document_topic_matrix = self.transform(raw_documents)\n            except AttributeError:\n                raise ValueError(\n                    \"Transductive methods cannot \"\n                    \"infer topical content in documents.\\n\"\n                    \"Please pass a document_topic_matrix.\"\n                )\n        try:\n            topic_id = list(self.classes_).index(topic_id)\n        except AttributeError:\n            pass\n        kth = min(top_k, document_topic_matrix.shape[0] - 1)\n        highest = np.argpartition(-document_topic_matrix[:, topic_id], kth)[\n            :kth\n        ]\n        highest = highest[\n            np.argsort(-document_topic_matrix[highest, topic_id])\n        ]\n        scores = document_topic_matrix[highest, topic_id]\n        columns = [[\"Document\", \"Score\"]]\n        rows = []\n        for document_id, score in zip(highest, scores):\n            doc = raw_documents[document_id]\n            rows.append([doc, score])\n        if show_negative:\n            lowest = np.argpartition(document_topic_matrix[:, topic_id], kth)[\n                :kth\n            ]\n            lowest = lowest[\n                np.argsort(document_topic_matrix[lowest, topic_id])\n            ]\n            lowest = lowest[::-1]\n            scores = document_topic_matrix[lowest, topic_id]\n            for document_id, score in zip(lowest, scores):\n                doc = raw_documents[document_id]\n                rows.append([doc, score])\n        return pd.DataFrame(rows, columns=columns)\n\n    def topic_distribution_df(\n        self, text=None, topic_dist=None, top_k: int = 10\n    ):\n        \"\"\"Extracts topic distribution into a dataframe.\n\n        Parameters\n        ----------\n        text: str, optional\n            Text to infer topic distribution for.\n        topic_dist: ndarray of shape (n_topics), optional\n            Already inferred topic distribution for the text.\n            This is useful for transductive methods,\n            as they cannot infer topics from text.\n        top_k: int, default 10\n            Top K topics to show.\n        \"\"\"\n        try:\n            import pandas as pd\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"You need to pip install pandas to be able to use dataframes.\"\n            )\n        if topic_dist is None:\n            if text is None:\n                raise ValueError(\n                    \"You should either pass a text or a distribution.\"\n                )\n            try:\n                topic_dist = self.transform([text])\n            except AttributeError:\n                raise ValueError(\n                    \"Transductive methods cannot \"\n                    \"infer topical content in documents.\\n\"\n                    \"Please pass a topic distribution.\"\n                )\n        topic_dist = np.squeeze(np.asarray(topic_dist))\n        highest = np.argsort(-topic_dist)[:top_k]\n        columns = []\n        columns.append(\"Topic name\")\n        columns.append(\"Score\")\n        rows = []\n        for ind in highest:\n            score = topic_dist[ind]\n            rows.append([self.topic_names[ind], score])\n        return pd.DataFrame(rows, columns=columns)\n\n    def get_time_slices(self) -&gt; list[tuple[datetime, datetime]]:\n        \"\"\"Returns starting and ending datetime of\n        each timeslice in the model.\"\"\"\n        bins = getattr(self, \"time_bin_edges\", None)\n        if bins is None:\n            raise AttributeError(\n                \"Topic model is not dynamic, time_bin_edges attribute is missing.\"\n            )\n        res = []\n        for i_bin, slice_end in enumerate(bins[1:]):\n            res.append((bins[i_bin], slice_end))\n        return res\n\n    def get_topics_over_time(\n        self, top_k: int = 10\n    ) -&gt; list[list[tuple[Any, list[tuple[str, float]]]]]:\n        \"\"\"Returns high-level topic representations in form of the top K words\n        in each topic.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of top words to return for each topic.\n\n        Returns\n        -------\n        list[list[tuple]]\n            List of topics over each time slice in the dynamic model.\n            Each time slice is a list of topics.\n            Each topic is a tuple of topic ID and the top k words.\n            Top k words are a list of (word, word_importance) pairs.\n        \"\"\"\n        temporal_components = getattr(self, \"temporal_components_\", None)\n        if temporal_components is None:\n            raise AttributeError(\n                \"Topic model is not dynamic, temporal_components_ attribute is missing.\"\n            )\n        n_topics = temporal_components.shape[1]\n        try:\n            classes = self.classes_\n        except AttributeError:\n            classes = list(range(n_topics))\n        res = []\n        for components in temporal_components:\n            highest = np.argpartition(-components, top_k)[:, :top_k]\n            vocab = self.get_vocab()\n            top = []\n            score = []\n            for component, high in zip(components, highest):\n                importance = component[high]\n                high = high[np.argsort(-importance)]\n                score.append(component[high])\n                top.append(vocab[high])\n            topics = []\n            for topic, words, scores in zip(classes, top, score):\n                topic_data = (topic, list(zip(words, scores)))\n                topics.append(topic_data)\n            res.append(topics)\n        return res\n\n    def _topics_over_time(\n        self,\n        top_k: int = 5,\n        show_scores: bool = False,\n        date_format: str = \"%Y %m %d\",\n    ) -&gt; list[list[str]]:\n        temporal_components = getattr(self, \"temporal_components_\", None)\n        if temporal_components is None:\n            raise AttributeError(\n                \"Topic model is not dynamic, temporal_components_ attribute is missing.\"\n            )\n        temporal_importance = getattr(self, \"temporal_importance_\", None)\n        if temporal_components is None:\n            raise AttributeError(\n                \"Topic model is not dynamic, temporal_importance_ attribute is missing.\"\n            )\n        slices = self.get_time_slices()\n        slice_names = []\n        for start_dt, end_dt in slices:\n            start_str = start_dt.strftime(date_format)\n            end_str = end_dt.strftime(date_format)\n            slice_names.append(f\"{start_str} - {end_str}\")\n        n_topics = temporal_components.shape[1]\n        try:\n            topic_names = self.topic_names\n        except AttributeError:\n            topic_names = [f\"Topic {i}\" for i in range(n_topics)]\n        columns = []\n        rows = []\n        columns.append(\"Time Slice\")\n        for topic in topic_names:\n            columns.append(topic)\n        for slice_name, components, weights in zip(\n            slice_names, temporal_components, temporal_importance\n        ):\n            fields = []\n            fields.append(slice_name)\n            vocab = self.get_vocab()\n            for component, weight in zip(components, weights):\n                if np.all(component == 0) or np.all(np.isnan(component)):\n                    fields.append(\"Topic not present.\")\n                    continue\n                if weight &lt; 0:\n                    component = -component\n                top = np.argpartition(-component, top_k)[:top_k]\n                importance = component[top]\n                top = top[np.argsort(-importance)]\n                top = top[importance != 0]\n                scores = component[top]\n                words = vocab[top]\n                if show_scores:\n                    concat_words = \", \".join(\n                        [\n                            f\"{word}({importance:.2f})\"\n                            for word, importance in zip(words, scores)\n                        ]\n                    )\n                else:\n                    concat_words = \", \".join([word for word in words])\n                fields.append(concat_words)\n            rows.append(fields)\n        return [columns, *rows]\n\n    def print_topics_over_time(\n        self,\n        top_k: int = 5,\n        show_scores: bool = False,\n        date_format: str = \"%Y %m %d\",\n    ):\n        \"\"\"Pretty prints topics in the model in a table.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of top words to return for each topic.\n        show_scores: bool, default False\n            Indicates whether to show importance scores for each word.\n        \"\"\"\n        columns, *rows = self._topics_over_time(\n            top_k, show_scores, date_format\n        )\n        table = Table(show_lines=True)\n        for column in columns:\n            table.add_column(column)\n        for row in rows:\n            table.add_row(*row)\n        console = Console()\n        console.print(table)\n\n    def export_topics_over_time(\n        self,\n        top_k: int = 5,\n        show_scores: bool = False,\n        date_format: str = \"%Y %m %d\",\n        format=\"csv\",\n    ) -&gt; str:\n        \"\"\"Pretty prints topics in the model in a table.\n\n        Parameters\n        ----------\n        top_k: int, default 10\n            Number of top words to return for each topic.\n        show_scores: bool, default False\n            Indicates whether to show importance scores for each word.\n        format: 'csv', 'latex' or 'markdown'\n            Specifies which format should be used.\n            'csv', 'latex' and 'markdown' are supported.\n        \"\"\"\n        table = self._topics_over_time(top_k, show_scores, date_format)\n        return export_table(table, format=format)\n\n    def topics_over_time_df(\n        self,\n        top_k: int = 5,\n        show_scores: bool = False,\n        format=\"csv\",\n    ):\n        try:\n            import pandas as pd\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"You need to pip install pandas to be able to use dataframes.\"\n            )\n\n        def parse_time_slice(slice: str) -&gt; tuple[datetime, datetime]:\n            date_format = \"%Y %m %d\"\n            start_date, end_date = slice.split(\" - \")\n            return datetime.strptime(\n                start_date, date_format\n            ), datetime.strptime(end_date, date_format)\n\n        columns, *rows = self._topics_over_time(top_k, show_scores)\n        df = pd.DataFrame(rows, columns=columns)\n        df[\"Time Slice\"] = df[\"Time Slice\"].map(parse_time_slice)\n        return df\n\n    def plot_topics_over_time(\n        self,\n        top_k: int = 6,\n        color_discrete_sequence: Optional[Iterable[str]] = None,\n        color_discrete_map: Optional[dict[str, str]] = None,\n    ):\n        \"\"\"Displays topics over time in the fitted dynamic model on a dynamic HTML figure.\n\n        &gt; You will need to `pip install plotly` to use this method.\n\n        Parameters\n        ----------\n        top_k: int, default 6\n            Number of top words per topic to display on the figure.\n        color_discrete_sequence: Iterable[str], default None\n            Color palette to use in the plot.\n            Example:\n\n            ```python\n            import plotly.express as px\n            model.plot_topics_over_time(color_discrete_sequence=px.colors.qualitative.Light24)\n            ```\n\n        color_discrete_map: dict[str, str], default None\n            Topic names mapped to the colors that should\n            be associated with them.\n\n        Returns\n        -------\n        go.Figure\n            Plotly graph objects Figure, that can be displayed or exported as\n            HTML or static image.\n        \"\"\"\n        try:\n            import plotly.express as px\n            import plotly.graph_objects as go\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n        temporal_components = getattr(self, \"temporal_components_\", None)\n        if temporal_components is None:\n            raise AttributeError(\n                \"Topic model is not dynamic, temporal_components_ attribute is missing.\"\n            )\n        temporal_importance = getattr(self, \"temporal_importance_\", None)\n        if temporal_components is None:\n            raise AttributeError(\n                \"Topic model is not dynamic, temporal_importance_ attribute is missing.\"\n            )\n        if color_discrete_sequence is not None:\n            topic_colors = itertools.cycle(color_discrete_sequence)\n        elif color_discrete_map is not None:\n            topic_colors = [\n                color_discrete_map[topic_name]\n                for topic_name in self.topic_names\n            ]\n        else:\n            topic_colors = px.colors.qualitative.Dark24\n        fig = go.Figure()\n        vocab = self.get_vocab()\n        n_topics = temporal_components.shape[1]\n        try:\n            topic_names = self.topic_names\n        except AttributeError:\n            topic_names = [f\"Topic {i}\" for i in range(n_topics)]\n        for trace_color, (i_topic, topic_imp_t) in zip(\n            itertools.cycle(topic_colors), enumerate(temporal_importance.T)\n        ):\n            component_over_time = temporal_components[:, i_topic, :]\n            name_over_time = []\n            for component, importance in zip(component_over_time, topic_imp_t):\n                if importance &lt; 0:\n                    component = -component\n                top = np.argpartition(-component, top_k)[:top_k]\n                values = component[top]\n                if np.all(values == 0) or np.all(np.isnan(values)):\n                    name_over_time.append(\"&lt;not present&gt;\")\n                    continue\n                top = top[np.argsort(-values)]\n                name_over_time.append(\", \".join(vocab[top]))\n            times = self.time_bin_edges[:-1]\n            fig.add_trace(\n                go.Scatter(\n                    x=times,\n                    y=topic_imp_t,\n                    mode=\"markers+lines\",\n                    text=name_over_time,\n                    name=topic_names[i_topic],\n                    hovertemplate=\"&lt;b&gt;%{text}&lt;/b&gt;\",\n                    marker=dict(\n                        line=dict(width=2, color=\"black\"),\n                        size=14,\n                        color=trace_color,\n                    ),\n                    line=dict(width=3),\n                )\n            )\n        fig.update_layout(\n            template=\"plotly_white\",\n            hoverlabel=dict(font_size=16, bgcolor=\"white\"),\n            hovermode=\"x\",\n            font=dict(family=\"Roboto Mono\"),\n        )\n        fig.add_hline(y=0, line_dash=\"dash\", opacity=0.5)\n        fig.update_xaxes(title=\"Time Slice Start\")\n        fig.update_yaxes(title=\"Topic Importance\")\n        return fig\n\n    @staticmethod\n    def _image_grid(\n        images: list[Image.Image],\n        final_size=(1200, 1200),\n        grid_size: tuple[int, int] = (4, 4),\n    ):\n        grid_img = Image.new(\"RGB\", final_size, (255, 255, 255))\n        cell_width = final_size[0] // grid_size[0]\n        cell_height = final_size[1] // grid_size[1]\n        n_rows, n_cols = grid_size\n        for idx, img in enumerate(images[: n_rows * n_cols]):\n            img = img.resize(\n                (cell_width, cell_height), resample=Image.Resampling.LANCZOS\n            )\n            x_offset = (idx % grid_size[0]) * cell_width\n            y_offset = (idx // grid_size[1]) * cell_height\n            grid_img.paste(img, (x_offset, y_offset))\n        return grid_img\n\n    def plot_topics_with_images(self, n_cols: int = 3, grid_size: int = 4):\n        \"\"\"Plots the most important images for each topic, along with keywords.\n\n        Note that you will need to `pip install plotly` to use plots in Turftopic.\n\n        Parameters\n        ----------\n        n_cols: int, default 3\n            Number of columns you want to have in the grid of topics.\n        grid_size: int, default 4\n            The square root of the number of images you want to display for a given topic.\n            For instance if grid_size==4, all topics will have 16 images displayed,\n            since the joint image will have 4 columns and 4 rows.\n\n        Returns\n        -------\n        go.Figure\n            Plotly figure containing top images and keywords for topics.\n        \"\"\"\n        if not hasattr(self, \"top_images\"):\n            raise ValueError(\n                \"Model either has not been fit or was fit without images. top_images property missing.\"\n            )\n        try:\n            import plotly.graph_objects as go\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n        negative_images = getattr(self, \"negative_images\", None)\n        if negative_images is not None:\n            # If the model has negative images, it should display them side by side with the positive ones.\n            n_components = self.components_.shape[0]\n            fig = go.Figure()\n            width, height = 1200, 1200\n            scale_factor = 0.25\n            w, h = width * scale_factor, height * scale_factor\n            padding = 10\n            figure_height = (h + padding) * n_components\n            figure_width = (w + padding) * 2\n            fig = fig.add_trace(\n                go.Scatter(\n                    x=[0, figure_width],\n                    y=[0, figure_height],\n                    mode=\"markers\",\n                    marker_opacity=0,\n                )\n            )\n            vocab = self.get_vocab()\n            for i, component in enumerate(self.components_):\n                positive = vocab[np.argsort(-component)[:7]]\n                negative = vocab[np.argsort(component)[:7]]\n                pos_image = self._image_grid(\n                    self.top_images[i],\n                    (width, height),\n                    grid_size=(grid_size, grid_size),\n                )\n                neg_image = self._image_grid(\n                    self.negative_images[i],\n                    (width, height),\n                    grid_size=(grid_size, grid_size),\n                )\n                x0 = 0\n                y0 = (h + padding) * (n_components - i)\n                fig = fig.add_layout_image(\n                    dict(\n                        x=x0,\n                        sizex=w,\n                        y=y0,\n                        sizey=h,\n                        xref=\"x\",\n                        yref=\"y\",\n                        opacity=1.0,\n                        layer=\"below\",\n                        sizing=\"stretch\",\n                        source=pos_image,\n                    ),\n                )\n                fig.add_annotation(\n                    x=(w / 2),\n                    y=(h + padding) * (n_components - i) - (h / 2),\n                    text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(positive),\n                    font=dict(\n                        size=16,\n                        family=\"Roboto Mono\",\n                        color=\"white\",\n                    ),\n                    bgcolor=\"rgba(0,0,255, 0.5)\",\n                )\n                x0 = (w + padding) * 1\n                fig = fig.add_layout_image(\n                    dict(\n                        x=x0,\n                        sizex=w,\n                        y=y0,\n                        sizey=h,\n                        xref=\"x\",\n                        yref=\"y\",\n                        opacity=1.0,\n                        layer=\"below\",\n                        sizing=\"stretch\",\n                        source=neg_image,\n                    ),\n                )\n                fig.add_annotation(\n                    x=(w + padding) + (w / 2),\n                    y=(h + padding) * (n_components - i) - (h / 2),\n                    text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(negative),\n                    font=dict(\n                        size=16,\n                        family=\"Times New Roman\",\n                        color=\"white\",\n                    ),\n                    bgcolor=\"rgba(255,0,0, 0.5)\",\n                )\n            fig = fig.update_xaxes(visible=False, range=[0, figure_width])\n            fig = fig.update_yaxes(\n                visible=False,\n                range=[0, figure_height],\n                # the scaleanchor attribute ensures that the aspect ratio stays constant\n                scaleanchor=\"x\",\n            )\n            fig = fig.update_layout(\n                width=figure_width,\n                height=figure_height,\n                margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n            )\n            return fig\n        else:\n            fig = go.Figure()\n            width, height = 1200, 1200\n            scale_factor = 0.25\n            w, h = width * scale_factor, height * scale_factor\n            padding = 10\n            n_components = self.components_.shape[0]\n            n_rows = n_components // n_cols + int(bool(n_components % n_cols))\n            figure_height = (h + padding) * n_rows\n            figure_width = (w + padding) * n_cols\n            fig = fig.add_trace(\n                go.Scatter(\n                    x=[0, figure_width],\n                    y=[0, figure_height],\n                    mode=\"markers\",\n                    marker_opacity=0,\n                )\n            )\n            vocab = self.get_vocab()\n            for i, component in enumerate(self.components_):\n                col = i % n_cols\n                row = i // n_cols\n                top_7 = vocab[np.argsort(-component)[:7]]\n                images = self.top_images[i]\n                image = self._image_grid(\n                    images, (width, height), grid_size=(grid_size, grid_size)\n                )\n                x0 = (w + padding) * col\n                y0 = (h + padding) * (n_rows - row)\n                fig = fig.add_layout_image(\n                    dict(\n                        x=x0,\n                        sizex=w,\n                        y=y0,\n                        sizey=h,\n                        xref=\"x\",\n                        yref=\"y\",\n                        opacity=1.0,\n                        layer=\"below\",\n                        sizing=\"stretch\",\n                        source=image,\n                    ),\n                )\n                fig.add_annotation(\n                    x=(w + padding) * col + (w / 2),\n                    y=(h + padding) * (n_rows - row) - (h / 2),\n                    text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(top_7),\n                    font=dict(\n                        size=16,\n                        family=\"Times New Roman\",\n                        color=\"white\",\n                    ),\n                    bgcolor=\"rgba(0,0,0, 0.5)\",\n                )\n            fig = fig.update_xaxes(visible=False, range=[0, figure_width])\n            fig = fig.update_yaxes(\n                visible=False,\n                range=[0, figure_height],\n                # the scaleanchor attribute ensures that the aspect ratio stays constant\n                scaleanchor=\"x\",\n            )\n            fig = fig.update_layout(\n                width=figure_width,\n                height=figure_height,\n                margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n            )\n            return fig\n\n    def plot_multimodal_topics(\n        self,\n        top_k: int = 10,\n        grid_size: int = 4,\n        raw_documents=None,\n        document_topic_matrix=None,\n    ):\n        \"\"\"Plots all multimodal topics in a model along with top documents individually,\n        and provides a slider to switch between them.\n\n        Parameters\n        ----------\n        top_k: int = 10\n            Number of top words and documents to display.\n        grid_size: int, default 4\n            The square root of the number of images you want to display for a given topic.\n            For instance if grid_size==4, all topics will have 16 images displayed,\n            since the joint image will have 4 columns and 4 rows.\n        raw_documents: list of str, optional\n            List of documents to consider.\n        document_topic_matrix: ndarray of shape (n_documents, n_topics), optional\n            Document topic matrix to use. This is useful for transductive methods,\n            as they cannot infer topics from text.\n\n        \"\"\"\n        if not hasattr(self, \"top_images\"):\n            raise ValueError(\n                \"Model either has not been fit or was fit without images. top_images property missing.\"\n            )\n        try:\n            import plotly.express as px\n            import plotly.graph_objects as go\n            from plotly.subplots import make_subplots\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n        negative_images = getattr(self, \"negative_images\", None)\n        negative_topics = (\n            self.get_top_words(top_k=top_k, positive=False)\n            if negative_images is not None\n            else None\n        )\n        specs = [{\"type\": \"image\"}, {\"type\": \"table\"}]\n        if negative_images is not None:\n            specs.append({\"type\": \"image\"})\n        fig = make_subplots(\n            rows=1,\n            cols=2 if negative_images is None else 3,\n            specs=[specs],\n            shared_yaxes=True,\n            shared_xaxes=True,\n        )\n        width, height = 1200, 1200\n        topics = self.get_top_words(top_k=top_k)\n        n_topics = len(topics)\n        annotations = []\n        for i, topic in enumerate(topics):\n            images = self.top_images[i]\n            image = TopicContainer._image_grid(\n                images, (width, height), grid_size=(grid_size, grid_size)\n            )\n            trace = px.imshow(image).data[0]\n            trace.visible = False\n            fig.add_trace(trace, col=1, row=1)\n            annt = dict(\n                x=width / 2,\n                y=height / 2,\n                text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(topic),\n                font=dict(\n                    size=16,\n                    family=\"Roboto Mono\",\n                    color=\"white\",\n                ),\n                bgcolor=\"rgba(0,0,255, 0.5)\",\n                xref=\"x\",\n                yref=\"y\",\n            )\n            annotations.append(annt)\n        if negative_topics is not None:\n            for i, negative_topic in enumerate(negative_topics):\n                images = negative_images[i]\n                image = TopicContainer._image_grid(\n                    images, (width, height), grid_size=(grid_size, grid_size)\n                )\n                trace = px.imshow(image).data[0]\n                trace.visible = False\n                fig.add_trace(trace, col=3, row=1)\n                annotations.append(\n                    dict(\n                        x=width / 2,\n                        y=height / 2,\n                        text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(negative_topic),\n                        font=dict(\n                            size=16,\n                            family=\"Roboto Mono\",\n                            color=\"white\",\n                        ),\n                        bgcolor=\"rgba(255,0,0, 0.5)\",\n                        xref=\"x2\",\n                        yref=\"y2\",\n                    )\n                )\n        fig = fig.add_annotation(**annotations[0])\n        if negative_images is not None:\n            fig.add_annotation(**annotations[n_topics])\n        classes = getattr(self, \"classes_\", np.arange(n_topics))\n        for i, topic_id in enumerate(classes):\n            header, *cells = self._representative_docs(\n                topic_id,\n                raw_documents=raw_documents,\n                document_topic_matrix=document_topic_matrix,\n                top_k=top_k,\n                show_negative=negative_images is not None,\n            )\n            # Transposing cells\n            cells = [list(column) for column in zip(*cells)]\n            fig.add_trace(\n                go.Table(\n                    columnorder=[1, 2],\n                    columnwidth=[400, 80],\n                    header=dict(\n                        values=header,\n                        fill_color=\"white\",\n                        line=dict(color=\"black\", width=4),\n                        font=dict(\n                            family=\"Roboto Mono\", color=\"black\", size=20\n                        ),\n                    ),\n                    cells=dict(\n                        values=cells,\n                        fill_color=\"white\",\n                        align=\"left\",\n                        line=dict(color=\"black\", width=2),\n                        font=dict(\n                            family=\"Roboto Mono\", color=\"black\", size=16\n                        ),\n                        height=40,\n                    ),\n                    visible=False,\n                ),\n                col=2,\n                row=1,\n            )\n        fig.data[0].visible = True\n        fig.data[n_topics].visible = True\n        if negative_images is not None:\n            fig.data[n_topics * 2].visible = True\n        fig = fig.update_layout(\n            margin={\"l\": 0, \"r\": 0, \"t\": 40, \"b\": 20},\n            template=\"plotly_white\",\n            font=dict(family=\"Roboto Mono\"),\n        )\n        fig = fig.update_xaxes(visible=False)\n        fig = fig.update_yaxes(visible=False)\n        steps = []\n        n_traces = n_topics * 2 if negative_images is None else n_topics * 3\n        for i, name in enumerate(self.topic_names):\n            _annt = [annotations[i]]\n            if negative_topics is not None:\n                _annt.append(annotations[n_topics + i])\n            step = dict(\n                method=\"update\",\n                label=name,\n                args=[\n                    {\"visible\": [False] * n_traces},\n                    {\n                        \"title\": \"Topic: \" + name,\n                        \"annotations\": _annt,\n                    },\n                ],\n            )\n            step[\"args\"][0][\"visible\"][i] = True\n            step[\"args\"][0][\"visible\"][n_topics + i] = True\n            if negative_images is not None:\n                step[\"args\"][0][\"visible\"][n_topics * 2 + i] = True\n            steps.append(step)\n        sliders = [\n            dict(\n                active=0,\n                currentvalue={\"prefix\": \"Topic: \"},\n                pad={\"t\": 50, \"b\": 20, \"r\": 40, \"l\": 40},\n                steps=steps,\n            )\n        ]\n        fig = fig.update_layout(sliders=sliders)\n        return fig\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.topic_names","title":"<code>topic_names: list[str]</code>  <code>property</code>","text":"<p>Names of the topics based on the highest scoring 4 terms.</p>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.analyze_topics","title":"<code>analyze_topics(analyzer, use_documents=True, use_summaries=None)</code>","text":"<p>Analyzes topics in a fitted topic model using an Analyzer.</p> Example <pre><code>from turftopic.analyzers import T5Analyzer\n\nmodel = KeyNMF(10).fit(corpus)\nanalyzer = T5Analyzer()\nres = model.analyze_topics(analyzer)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>analyzer</code> <code>Analyzer</code> <p>Large language model to analyze the topics in your topic model.</p> required <code>use_documents</code> <code>bool</code> <p>Indicates whether top documents should be involved in analyzing topics.</p> <code>True</code> <code>use_summaries</code> <code>Optional[bool]</code> <p>Indicates whether the analyzer should first summarize the most relevant documents. This can be beneficial when your corpus includes very long documents.</p> <code>None</code> <p>Returns:</p> Type Description <code>AnalysisResults</code> <p>Analysis results. Dataclass containing <code>topic_names</code>, <code>topic_descriptions</code> and <code>document_summaries</code> if relevant.</p> Source code in <code>turftopic/container.py</code> <pre><code>def analyze_topics(\n    self,\n    analyzer: Analyzer,\n    use_documents: bool = True,\n    use_summaries: Optional[bool] = None,\n) -&gt; AnalysisResults:\n    \"\"\"Analyzes topics in a fitted topic model using an Analyzer.\n\n    Example\n    -------\n    ```python\n    from turftopic.analyzers import T5Analyzer\n\n    model = KeyNMF(10).fit(corpus)\n    analyzer = T5Analyzer()\n    res = model.analyze_topics(analyzer)\n    ```\n\n    Parameters\n    ----------\n    analyzer: Analyzer\n        Large language model to analyze the topics in your topic model.\n    use_documents: bool, default True\n        Indicates whether top documents should be involved in analyzing topics.\n    use_summaries: bool, optional\n        Indicates whether the analyzer should first summarize the most relevant documents.\n        This can be beneficial when your corpus includes very long documents.\n\n    Returns\n    -------\n    AnalysisResults\n        Analysis results. Dataclass containing `topic_names`, `topic_descriptions`\n        and `document_summaries` if relevant.\n    \"\"\"\n    try:\n        documents = self.get_top_documents()\n    except ValueError as e:\n        warnings.warn(\n            f\"Couldn't get top documents, proceeding only with keywords: {e}\"\n        )\n        documents = None\n    if not use_documents:\n        documents = None\n    res = analyzer.analyze_topics(\n        keywords=self._top_terms(),\n        documents=documents,\n        use_summaries=use_summaries,\n    )\n    self.topic_names_ = res.topic_names\n    if res.document_summaries is not None:\n        self.document_summaries = res.document_summaries\n    self.topic_descriptions = res.topic_descriptions\n    if -1 in getattr(self, \"classes_\", ()):\n        id_to_idx = dict(zip(self.classes_, range(len(self.classes_))))\n        self.topic_names_[id_to_idx[-1]] = \"Outliers\"\n        if self.topic_descriptions is not None:\n            self.topic_descriptions[id_to_idx[-1]] = (\n                \"Topic containing outlier documents.\"\n            )\n    return res\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.export_representative_documents","title":"<code>export_representative_documents(topic_id, raw_documents=None, document_topic_matrix=None, top_k=5, show_negative=None, format='csv')</code>","text":"<p>Exports the highest ranking documents in a topic as a text table.</p> <p>Parameters:</p> Name Type Description Default <code>topic_id</code> <p>ID of the topic to display.</p> required <code>raw_documents</code> <p>List of documents to consider.</p> <code>None</code> <code>document_topic_matrix</code> <p>Document topic matrix to use. This is useful for transductive methods, as they cannot infer topics from text.</p> <code>None</code> <code>top_k</code> <p>Top K documents to show.</p> <code>5</code> <code>show_negative</code> <code>Optional[bool]</code> <p>Indicates whether lowest ranking documents should also be shown.</p> <code>None</code> <code>format</code> <code>str</code> <p>Specifies which format should be used. 'csv', 'latex' and 'markdown' are supported.</p> <code>'csv'</code> Source code in <code>turftopic/container.py</code> <pre><code>def export_representative_documents(\n    self,\n    topic_id,\n    raw_documents=None,\n    document_topic_matrix=None,\n    top_k=5,\n    show_negative: Optional[bool] = None,\n    format: str = \"csv\",\n):\n    \"\"\"Exports the highest ranking documents in a topic as a text table.\n\n    Parameters\n    ----------\n    topic_id: int\n        ID of the topic to display.\n    raw_documents: list of str\n        List of documents to consider.\n    document_topic_matrix: ndarray of shape (n_topics, n_topics), optional\n        Document topic matrix to use. This is useful for transductive methods,\n        as they cannot infer topics from text.\n    top_k: int, default 5\n        Top K documents to show.\n    show_negative: bool, default False\n        Indicates whether lowest ranking documents should also be shown.\n    format: 'csv', 'latex' or 'markdown'\n        Specifies which format should be used.\n        'csv', 'latex' and 'markdown' are supported.\n    \"\"\"\n    table = self._representative_docs(\n        topic_id,\n        raw_documents,\n        document_topic_matrix,\n        top_k,\n        show_negative,\n    )\n    return export_table(table, format=format)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.export_topic_distribution","title":"<code>export_topic_distribution(text=None, topic_dist=None, top_k=10, format='csv')</code>","text":"<p>Exports topic distribution as a text table.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>Text to infer topic distribution for.</p> <code>None</code> <code>topic_dist</code> <p>Already inferred topic distribution for the text. This is useful for transductive methods, as they cannot infer topics from text.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>Top K topics to show.</p> <code>10</code> <code>format</code> <p>Specifies which format should be used. 'csv', 'latex' and 'markdown' are supported.</p> <code>'csv'</code> Source code in <code>turftopic/container.py</code> <pre><code>def export_topic_distribution(\n    self, text=None, topic_dist=None, top_k: int = 10, format=\"csv\"\n) -&gt; str:\n    \"\"\"Exports topic distribution as a text table.\n\n    Parameters\n    ----------\n    text: str, optional\n        Text to infer topic distribution for.\n    topic_dist: ndarray of shape (n_topics), optional\n        Already inferred topic distribution for the text.\n        This is useful for transductive methods,\n        as they cannot infer topics from text.\n    top_k: int, default 10\n        Top K topics to show.\n    format: 'csv', 'latex' or 'markdown'\n        Specifies which format should be used.\n        'csv', 'latex' and 'markdown' are supported.\n    \"\"\"\n    table = self._topic_distribution(text, topic_dist, top_k)\n    return export_table(table, format=format)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.export_topics","title":"<code>export_topics(top_k=10, show_scores=False, show_negative=None, format='csv')</code>","text":"<p>Exports top K words from topics in a table in a given format. Returns table as a pure string.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words to return for each topic.</p> <code>10</code> <code>show_scores</code> <code>bool</code> <p>Indicates whether to show importance scores for each word.</p> <code>False</code> <code>show_negative</code> <code>Optional[bool]</code> <p>Indicates whether the most negative terms should also be displayed.</p> <code>None</code> <code>format</code> <code>str</code> <p>Specifies which format should be used. 'csv', 'latex' and 'markdown' are supported.</p> <code>'csv'</code> Source code in <code>turftopic/container.py</code> <pre><code>def export_topics(\n    self,\n    top_k: int = 10,\n    show_scores: bool = False,\n    show_negative: Optional[bool] = None,\n    format: str = \"csv\",\n) -&gt; str:\n    \"\"\"Exports top K words from topics in a table in a given format.\n    Returns table as a pure string.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of top words to return for each topic.\n    show_scores: bool, default False\n        Indicates whether to show importance scores for each word.\n    show_negative: bool, default False\n        Indicates whether the most negative terms should also be displayed.\n    format: 'csv', 'latex' or 'markdown'\n        Specifies which format should be used.\n        'csv', 'latex' and 'markdown' are supported.\n    \"\"\"\n    table = self._topics_table(\n        top_k, show_scores, show_negative=show_negative\n    )\n    return export_table(table, format=format)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.export_topics_over_time","title":"<code>export_topics_over_time(top_k=5, show_scores=False, date_format='%Y %m %d', format='csv')</code>","text":"<p>Pretty prints topics in the model in a table.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words to return for each topic.</p> <code>5</code> <code>show_scores</code> <code>bool</code> <p>Indicates whether to show importance scores for each word.</p> <code>False</code> <code>format</code> <p>Specifies which format should be used. 'csv', 'latex' and 'markdown' are supported.</p> <code>'csv'</code> Source code in <code>turftopic/container.py</code> <pre><code>def export_topics_over_time(\n    self,\n    top_k: int = 5,\n    show_scores: bool = False,\n    date_format: str = \"%Y %m %d\",\n    format=\"csv\",\n) -&gt; str:\n    \"\"\"Pretty prints topics in the model in a table.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of top words to return for each topic.\n    show_scores: bool, default False\n        Indicates whether to show importance scores for each word.\n    format: 'csv', 'latex' or 'markdown'\n        Specifies which format should be used.\n        'csv', 'latex' and 'markdown' are supported.\n    \"\"\"\n    table = self._topics_over_time(top_k, show_scores, date_format)\n    return export_table(table, format=format)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.get_time_slices","title":"<code>get_time_slices()</code>","text":"<p>Returns starting and ending datetime of each timeslice in the model.</p> Source code in <code>turftopic/container.py</code> <pre><code>def get_time_slices(self) -&gt; list[tuple[datetime, datetime]]:\n    \"\"\"Returns starting and ending datetime of\n    each timeslice in the model.\"\"\"\n    bins = getattr(self, \"time_bin_edges\", None)\n    if bins is None:\n        raise AttributeError(\n            \"Topic model is not dynamic, time_bin_edges attribute is missing.\"\n        )\n    res = []\n    for i_bin, slice_end in enumerate(bins[1:]):\n        res.append((bins[i_bin], slice_end))\n    return res\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.get_top_documents","title":"<code>get_top_documents(raw_documents=None, document_topic_matrix=None, top_k=10, positive=True)</code>","text":"<p>Returns list of top documents for each topic.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of documents to return per topic.</p> <code>10</code> <code>positive</code> <code>bool</code> <p>Indicates whether the highest or lowest scoring documents should be returned.</p> <code>True</code> Source code in <code>turftopic/container.py</code> <pre><code>def get_top_documents(\n    self,\n    raw_documents=None,\n    document_topic_matrix=None,\n    top_k: int = 10,\n    positive: bool = True,\n) -&gt; list[list[str]]:\n    \"\"\"Returns list of top documents for each topic.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of documents to return per topic.\n    positive: bool, default True\n        Indicates whether the highest\n        or lowest scoring documents should be returned.\n    \"\"\"\n    if (\n        positive\n        and hasattr(self, \"top_documents\")\n        and len(self.top_documents[0]) &gt;= top_k\n    ):\n        return [docs[:top_k] for docs in self.top_documents]\n    if (\n        not positive\n        and hasattr(self, \"negative_documents\")\n        and len(self.negative_documents[0]) &gt;= top_k\n    ):\n        return [docs[:top_k] for docs in self.negative_documents]\n    docs = []\n    raw_documents = raw_documents or getattr(self, \"corpus\", None)\n    if raw_documents is None:\n        raise ValueError(\n            \"No corpus was passed, can't search for representative documents.\"\n        )\n    if document_topic_matrix is None:\n        document_topic_matrix = getattr(\n            self, \"document_topic_matrix\", None\n        )\n    if document_topic_matrix is None:\n        try:\n            document_topic_matrix = self.transform(raw_documents)\n        except AttributeError:\n            raise ValueError(\n                \"Transductive methods cannot \"\n                \"infer topical content in documents.\\n\"\n                \"Please pass a document_topic_matrix.\"\n            )\n    for topic_doc_vec in document_topic_matrix.T:\n        if positive:\n            topic_doc_vec = -topic_doc_vec\n        highest = np.argsort(topic_doc_vec)[:top_k]\n        docs.append([raw_documents[i_doc] for i_doc in highest])\n    return docs\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.get_top_images","title":"<code>get_top_images(top_k=True, positive=True)</code>","text":"<p>Returns list of top images for each topic.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of images to return.</p> <code>True</code> <code>positive</code> <code>bool</code> <p>Indicates whether the highest or lowest scoring images should be returned.</p> <code>True</code> Source code in <code>turftopic/container.py</code> <pre><code>def get_top_images(self, top_k: int = True, positive: bool = True):\n    \"\"\"Returns list of top images for each topic.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of images to return.\n    positive: bool, default True\n        Indicates whether the highest\n        or lowest scoring images should be returned.\n    \"\"\"\n    if not hasattr(self, \"top_images\"):\n        raise ValueError(\n            \"Model either has not been fit or was fit without images. top_images property missing.\"\n        )\n    if (not positive) and not hasattr(self, \"negative_images\"):\n        raise ValueError(\n            \"Model either has not been fit or was fit without images. top_images property missing.\"\n        )\n    top_images = self.top_images if positive else self.negative_images\n    ims = []\n    for topic_images in top_images:\n        if len(topic_images) &lt; top_k:\n            warnings.warn(\n                \"Number of images stored in the topic model is smaller than the specified top_k, returning all that the model has.\"\n            )\n        ims.append(topic_images[:top_k])\n    return ims\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.get_top_words","title":"<code>get_top_words(top_k=10, positive=True)</code>","text":"<p>Returns list of top words for each topic.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of words to return.</p> <code>10</code> <code>positive</code> <code>bool</code> <p>Indicates whether the highest or lowest scoring terms should be returned.</p> <code>True</code> Source code in <code>turftopic/container.py</code> <pre><code>def get_top_words(\n    self, top_k: int = 10, positive: bool = True\n) -&gt; list[list[str]]:\n    \"\"\"Returns list of top words for each topic.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of words to return.\n    positive: bool, default True\n        Indicates whether the highest\n        or lowest scoring terms should be returned.\n    \"\"\"\n    return self._top_terms(top_k, positive)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.get_topics","title":"<code>get_topics(top_k=10)</code>","text":"<p>Returns high-level topic representations in form of the top K words in each topic.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words to return for each topic.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[tuple]</code> <p>List of topics. Each topic is a tuple of topic ID and the top k words. Top k words are a list of (word, word_importance) pairs.</p> Source code in <code>turftopic/container.py</code> <pre><code>def get_topics(\n    self, top_k: int = 10\n) -&gt; List[Tuple[Any, List[Tuple[str, float]]]]:\n    \"\"\"Returns high-level topic representations in form of the top K words\n    in each topic.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of top words to return for each topic.\n\n    Returns\n    -------\n    list[tuple]\n        List of topics. Each topic is a tuple of\n        topic ID and the top k words.\n        Top k words are a list of (word, word_importance) pairs.\n    \"\"\"\n    n_topics = self.components_.shape[0]\n    try:\n        classes = self.classes_\n    except AttributeError:\n        classes = list(range(n_topics))\n    highest = np.argpartition(-self.components_, top_k)[:, :top_k]\n    vocab = self.get_vocab()\n    top = []\n    score = []\n    for component, high in zip(self.components_, highest):\n        importance = component[high]\n        high = high[np.argsort(-importance)]\n        score.append(component[high])\n        top.append(vocab[high])\n    topics = []\n    for topic, words, scores in zip(classes, top, score):\n        topic_data = (topic, list(zip(words, scores)))\n        topics.append(topic_data)\n    return topics\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.get_topics_over_time","title":"<code>get_topics_over_time(top_k=10)</code>","text":"<p>Returns high-level topic representations in form of the top K words in each topic.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words to return for each topic.</p> <code>10</code> <p>Returns:</p> Type Description <code>list[list[tuple]]</code> <p>List of topics over each time slice in the dynamic model. Each time slice is a list of topics. Each topic is a tuple of topic ID and the top k words. Top k words are a list of (word, word_importance) pairs.</p> Source code in <code>turftopic/container.py</code> <pre><code>def get_topics_over_time(\n    self, top_k: int = 10\n) -&gt; list[list[tuple[Any, list[tuple[str, float]]]]]:\n    \"\"\"Returns high-level topic representations in form of the top K words\n    in each topic.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of top words to return for each topic.\n\n    Returns\n    -------\n    list[list[tuple]]\n        List of topics over each time slice in the dynamic model.\n        Each time slice is a list of topics.\n        Each topic is a tuple of topic ID and the top k words.\n        Top k words are a list of (word, word_importance) pairs.\n    \"\"\"\n    temporal_components = getattr(self, \"temporal_components_\", None)\n    if temporal_components is None:\n        raise AttributeError(\n            \"Topic model is not dynamic, temporal_components_ attribute is missing.\"\n        )\n    n_topics = temporal_components.shape[1]\n    try:\n        classes = self.classes_\n    except AttributeError:\n        classes = list(range(n_topics))\n    res = []\n    for components in temporal_components:\n        highest = np.argpartition(-components, top_k)[:, :top_k]\n        vocab = self.get_vocab()\n        top = []\n        score = []\n        for component, high in zip(components, highest):\n            importance = component[high]\n            high = high[np.argsort(-importance)]\n            score.append(component[high])\n            top.append(vocab[high])\n        topics = []\n        for topic, words, scores in zip(classes, top, score):\n            topic_data = (topic, list(zip(words, scores)))\n            topics.append(topic_data)\n        res.append(topics)\n    return res\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.plot_multimodal_topics","title":"<code>plot_multimodal_topics(top_k=10, grid_size=4, raw_documents=None, document_topic_matrix=None)</code>","text":"<p>Plots all multimodal topics in a model along with top documents individually, and provides a slider to switch between them.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words and documents to display.</p> <code>10</code> <code>grid_size</code> <code>int</code> <p>The square root of the number of images you want to display for a given topic. For instance if grid_size==4, all topics will have 16 images displayed, since the joint image will have 4 columns and 4 rows.</p> <code>4</code> <code>raw_documents</code> <p>List of documents to consider.</p> <code>None</code> <code>document_topic_matrix</code> <p>Document topic matrix to use. This is useful for transductive methods, as they cannot infer topics from text.</p> <code>None</code> Source code in <code>turftopic/container.py</code> <pre><code>def plot_multimodal_topics(\n    self,\n    top_k: int = 10,\n    grid_size: int = 4,\n    raw_documents=None,\n    document_topic_matrix=None,\n):\n    \"\"\"Plots all multimodal topics in a model along with top documents individually,\n    and provides a slider to switch between them.\n\n    Parameters\n    ----------\n    top_k: int = 10\n        Number of top words and documents to display.\n    grid_size: int, default 4\n        The square root of the number of images you want to display for a given topic.\n        For instance if grid_size==4, all topics will have 16 images displayed,\n        since the joint image will have 4 columns and 4 rows.\n    raw_documents: list of str, optional\n        List of documents to consider.\n    document_topic_matrix: ndarray of shape (n_documents, n_topics), optional\n        Document topic matrix to use. This is useful for transductive methods,\n        as they cannot infer topics from text.\n\n    \"\"\"\n    if not hasattr(self, \"top_images\"):\n        raise ValueError(\n            \"Model either has not been fit or was fit without images. top_images property missing.\"\n        )\n    try:\n        import plotly.express as px\n        import plotly.graph_objects as go\n        from plotly.subplots import make_subplots\n    except (ImportError, ModuleNotFoundError) as e:\n        raise ModuleNotFoundError(\n            \"Please install plotly if you intend to use plots in Turftopic.\"\n        ) from e\n    negative_images = getattr(self, \"negative_images\", None)\n    negative_topics = (\n        self.get_top_words(top_k=top_k, positive=False)\n        if negative_images is not None\n        else None\n    )\n    specs = [{\"type\": \"image\"}, {\"type\": \"table\"}]\n    if negative_images is not None:\n        specs.append({\"type\": \"image\"})\n    fig = make_subplots(\n        rows=1,\n        cols=2 if negative_images is None else 3,\n        specs=[specs],\n        shared_yaxes=True,\n        shared_xaxes=True,\n    )\n    width, height = 1200, 1200\n    topics = self.get_top_words(top_k=top_k)\n    n_topics = len(topics)\n    annotations = []\n    for i, topic in enumerate(topics):\n        images = self.top_images[i]\n        image = TopicContainer._image_grid(\n            images, (width, height), grid_size=(grid_size, grid_size)\n        )\n        trace = px.imshow(image).data[0]\n        trace.visible = False\n        fig.add_trace(trace, col=1, row=1)\n        annt = dict(\n            x=width / 2,\n            y=height / 2,\n            text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(topic),\n            font=dict(\n                size=16,\n                family=\"Roboto Mono\",\n                color=\"white\",\n            ),\n            bgcolor=\"rgba(0,0,255, 0.5)\",\n            xref=\"x\",\n            yref=\"y\",\n        )\n        annotations.append(annt)\n    if negative_topics is not None:\n        for i, negative_topic in enumerate(negative_topics):\n            images = negative_images[i]\n            image = TopicContainer._image_grid(\n                images, (width, height), grid_size=(grid_size, grid_size)\n            )\n            trace = px.imshow(image).data[0]\n            trace.visible = False\n            fig.add_trace(trace, col=3, row=1)\n            annotations.append(\n                dict(\n                    x=width / 2,\n                    y=height / 2,\n                    text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(negative_topic),\n                    font=dict(\n                        size=16,\n                        family=\"Roboto Mono\",\n                        color=\"white\",\n                    ),\n                    bgcolor=\"rgba(255,0,0, 0.5)\",\n                    xref=\"x2\",\n                    yref=\"y2\",\n                )\n            )\n    fig = fig.add_annotation(**annotations[0])\n    if negative_images is not None:\n        fig.add_annotation(**annotations[n_topics])\n    classes = getattr(self, \"classes_\", np.arange(n_topics))\n    for i, topic_id in enumerate(classes):\n        header, *cells = self._representative_docs(\n            topic_id,\n            raw_documents=raw_documents,\n            document_topic_matrix=document_topic_matrix,\n            top_k=top_k,\n            show_negative=negative_images is not None,\n        )\n        # Transposing cells\n        cells = [list(column) for column in zip(*cells)]\n        fig.add_trace(\n            go.Table(\n                columnorder=[1, 2],\n                columnwidth=[400, 80],\n                header=dict(\n                    values=header,\n                    fill_color=\"white\",\n                    line=dict(color=\"black\", width=4),\n                    font=dict(\n                        family=\"Roboto Mono\", color=\"black\", size=20\n                    ),\n                ),\n                cells=dict(\n                    values=cells,\n                    fill_color=\"white\",\n                    align=\"left\",\n                    line=dict(color=\"black\", width=2),\n                    font=dict(\n                        family=\"Roboto Mono\", color=\"black\", size=16\n                    ),\n                    height=40,\n                ),\n                visible=False,\n            ),\n            col=2,\n            row=1,\n        )\n    fig.data[0].visible = True\n    fig.data[n_topics].visible = True\n    if negative_images is not None:\n        fig.data[n_topics * 2].visible = True\n    fig = fig.update_layout(\n        margin={\"l\": 0, \"r\": 0, \"t\": 40, \"b\": 20},\n        template=\"plotly_white\",\n        font=dict(family=\"Roboto Mono\"),\n    )\n    fig = fig.update_xaxes(visible=False)\n    fig = fig.update_yaxes(visible=False)\n    steps = []\n    n_traces = n_topics * 2 if negative_images is None else n_topics * 3\n    for i, name in enumerate(self.topic_names):\n        _annt = [annotations[i]]\n        if negative_topics is not None:\n            _annt.append(annotations[n_topics + i])\n        step = dict(\n            method=\"update\",\n            label=name,\n            args=[\n                {\"visible\": [False] * n_traces},\n                {\n                    \"title\": \"Topic: \" + name,\n                    \"annotations\": _annt,\n                },\n            ],\n        )\n        step[\"args\"][0][\"visible\"][i] = True\n        step[\"args\"][0][\"visible\"][n_topics + i] = True\n        if negative_images is not None:\n            step[\"args\"][0][\"visible\"][n_topics * 2 + i] = True\n        steps.append(step)\n    sliders = [\n        dict(\n            active=0,\n            currentvalue={\"prefix\": \"Topic: \"},\n            pad={\"t\": 50, \"b\": 20, \"r\": 40, \"l\": 40},\n            steps=steps,\n        )\n    ]\n    fig = fig.update_layout(sliders=sliders)\n    return fig\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.plot_topics_over_time","title":"<code>plot_topics_over_time(top_k=6, color_discrete_sequence=None, color_discrete_map=None)</code>","text":"<p>Displays topics over time in the fitted dynamic model on a dynamic HTML figure.</p> <p>You will need to <code>pip install plotly</code> to use this method.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words per topic to display on the figure.</p> <code>6</code> <code>color_discrete_sequence</code> <code>Optional[Iterable[str]]</code> <p>Color palette to use in the plot. Example:</p> <pre><code>import plotly.express as px\nmodel.plot_topics_over_time(color_discrete_sequence=px.colors.qualitative.Light24)\n</code></pre> <code>None</code> <code>color_discrete_map</code> <code>Optional[dict[str, str]]</code> <p>Topic names mapped to the colors that should be associated with them.</p> <code>None</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly graph objects Figure, that can be displayed or exported as HTML or static image.</p> Source code in <code>turftopic/container.py</code> <pre><code>def plot_topics_over_time(\n    self,\n    top_k: int = 6,\n    color_discrete_sequence: Optional[Iterable[str]] = None,\n    color_discrete_map: Optional[dict[str, str]] = None,\n):\n    \"\"\"Displays topics over time in the fitted dynamic model on a dynamic HTML figure.\n\n    &gt; You will need to `pip install plotly` to use this method.\n\n    Parameters\n    ----------\n    top_k: int, default 6\n        Number of top words per topic to display on the figure.\n    color_discrete_sequence: Iterable[str], default None\n        Color palette to use in the plot.\n        Example:\n\n        ```python\n        import plotly.express as px\n        model.plot_topics_over_time(color_discrete_sequence=px.colors.qualitative.Light24)\n        ```\n\n    color_discrete_map: dict[str, str], default None\n        Topic names mapped to the colors that should\n        be associated with them.\n\n    Returns\n    -------\n    go.Figure\n        Plotly graph objects Figure, that can be displayed or exported as\n        HTML or static image.\n    \"\"\"\n    try:\n        import plotly.express as px\n        import plotly.graph_objects as go\n    except (ImportError, ModuleNotFoundError) as e:\n        raise ModuleNotFoundError(\n            \"Please install plotly if you intend to use plots in Turftopic.\"\n        ) from e\n    temporal_components = getattr(self, \"temporal_components_\", None)\n    if temporal_components is None:\n        raise AttributeError(\n            \"Topic model is not dynamic, temporal_components_ attribute is missing.\"\n        )\n    temporal_importance = getattr(self, \"temporal_importance_\", None)\n    if temporal_components is None:\n        raise AttributeError(\n            \"Topic model is not dynamic, temporal_importance_ attribute is missing.\"\n        )\n    if color_discrete_sequence is not None:\n        topic_colors = itertools.cycle(color_discrete_sequence)\n    elif color_discrete_map is not None:\n        topic_colors = [\n            color_discrete_map[topic_name]\n            for topic_name in self.topic_names\n        ]\n    else:\n        topic_colors = px.colors.qualitative.Dark24\n    fig = go.Figure()\n    vocab = self.get_vocab()\n    n_topics = temporal_components.shape[1]\n    try:\n        topic_names = self.topic_names\n    except AttributeError:\n        topic_names = [f\"Topic {i}\" for i in range(n_topics)]\n    for trace_color, (i_topic, topic_imp_t) in zip(\n        itertools.cycle(topic_colors), enumerate(temporal_importance.T)\n    ):\n        component_over_time = temporal_components[:, i_topic, :]\n        name_over_time = []\n        for component, importance in zip(component_over_time, topic_imp_t):\n            if importance &lt; 0:\n                component = -component\n            top = np.argpartition(-component, top_k)[:top_k]\n            values = component[top]\n            if np.all(values == 0) or np.all(np.isnan(values)):\n                name_over_time.append(\"&lt;not present&gt;\")\n                continue\n            top = top[np.argsort(-values)]\n            name_over_time.append(\", \".join(vocab[top]))\n        times = self.time_bin_edges[:-1]\n        fig.add_trace(\n            go.Scatter(\n                x=times,\n                y=topic_imp_t,\n                mode=\"markers+lines\",\n                text=name_over_time,\n                name=topic_names[i_topic],\n                hovertemplate=\"&lt;b&gt;%{text}&lt;/b&gt;\",\n                marker=dict(\n                    line=dict(width=2, color=\"black\"),\n                    size=14,\n                    color=trace_color,\n                ),\n                line=dict(width=3),\n            )\n        )\n    fig.update_layout(\n        template=\"plotly_white\",\n        hoverlabel=dict(font_size=16, bgcolor=\"white\"),\n        hovermode=\"x\",\n        font=dict(family=\"Roboto Mono\"),\n    )\n    fig.add_hline(y=0, line_dash=\"dash\", opacity=0.5)\n    fig.update_xaxes(title=\"Time Slice Start\")\n    fig.update_yaxes(title=\"Topic Importance\")\n    return fig\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.plot_topics_with_images","title":"<code>plot_topics_with_images(n_cols=3, grid_size=4)</code>","text":"<p>Plots the most important images for each topic, along with keywords.</p> <p>Note that you will need to <code>pip install plotly</code> to use plots in Turftopic.</p> <p>Parameters:</p> Name Type Description Default <code>n_cols</code> <code>int</code> <p>Number of columns you want to have in the grid of topics.</p> <code>3</code> <code>grid_size</code> <code>int</code> <p>The square root of the number of images you want to display for a given topic. For instance if grid_size==4, all topics will have 16 images displayed, since the joint image will have 4 columns and 4 rows.</p> <code>4</code> <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly figure containing top images and keywords for topics.</p> Source code in <code>turftopic/container.py</code> <pre><code>def plot_topics_with_images(self, n_cols: int = 3, grid_size: int = 4):\n    \"\"\"Plots the most important images for each topic, along with keywords.\n\n    Note that you will need to `pip install plotly` to use plots in Turftopic.\n\n    Parameters\n    ----------\n    n_cols: int, default 3\n        Number of columns you want to have in the grid of topics.\n    grid_size: int, default 4\n        The square root of the number of images you want to display for a given topic.\n        For instance if grid_size==4, all topics will have 16 images displayed,\n        since the joint image will have 4 columns and 4 rows.\n\n    Returns\n    -------\n    go.Figure\n        Plotly figure containing top images and keywords for topics.\n    \"\"\"\n    if not hasattr(self, \"top_images\"):\n        raise ValueError(\n            \"Model either has not been fit or was fit without images. top_images property missing.\"\n        )\n    try:\n        import plotly.graph_objects as go\n    except (ImportError, ModuleNotFoundError) as e:\n        raise ModuleNotFoundError(\n            \"Please install plotly if you intend to use plots in Turftopic.\"\n        ) from e\n    negative_images = getattr(self, \"negative_images\", None)\n    if negative_images is not None:\n        # If the model has negative images, it should display them side by side with the positive ones.\n        n_components = self.components_.shape[0]\n        fig = go.Figure()\n        width, height = 1200, 1200\n        scale_factor = 0.25\n        w, h = width * scale_factor, height * scale_factor\n        padding = 10\n        figure_height = (h + padding) * n_components\n        figure_width = (w + padding) * 2\n        fig = fig.add_trace(\n            go.Scatter(\n                x=[0, figure_width],\n                y=[0, figure_height],\n                mode=\"markers\",\n                marker_opacity=0,\n            )\n        )\n        vocab = self.get_vocab()\n        for i, component in enumerate(self.components_):\n            positive = vocab[np.argsort(-component)[:7]]\n            negative = vocab[np.argsort(component)[:7]]\n            pos_image = self._image_grid(\n                self.top_images[i],\n                (width, height),\n                grid_size=(grid_size, grid_size),\n            )\n            neg_image = self._image_grid(\n                self.negative_images[i],\n                (width, height),\n                grid_size=(grid_size, grid_size),\n            )\n            x0 = 0\n            y0 = (h + padding) * (n_components - i)\n            fig = fig.add_layout_image(\n                dict(\n                    x=x0,\n                    sizex=w,\n                    y=y0,\n                    sizey=h,\n                    xref=\"x\",\n                    yref=\"y\",\n                    opacity=1.0,\n                    layer=\"below\",\n                    sizing=\"stretch\",\n                    source=pos_image,\n                ),\n            )\n            fig.add_annotation(\n                x=(w / 2),\n                y=(h + padding) * (n_components - i) - (h / 2),\n                text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(positive),\n                font=dict(\n                    size=16,\n                    family=\"Roboto Mono\",\n                    color=\"white\",\n                ),\n                bgcolor=\"rgba(0,0,255, 0.5)\",\n            )\n            x0 = (w + padding) * 1\n            fig = fig.add_layout_image(\n                dict(\n                    x=x0,\n                    sizex=w,\n                    y=y0,\n                    sizey=h,\n                    xref=\"x\",\n                    yref=\"y\",\n                    opacity=1.0,\n                    layer=\"below\",\n                    sizing=\"stretch\",\n                    source=neg_image,\n                ),\n            )\n            fig.add_annotation(\n                x=(w + padding) + (w / 2),\n                y=(h + padding) * (n_components - i) - (h / 2),\n                text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(negative),\n                font=dict(\n                    size=16,\n                    family=\"Times New Roman\",\n                    color=\"white\",\n                ),\n                bgcolor=\"rgba(255,0,0, 0.5)\",\n            )\n        fig = fig.update_xaxes(visible=False, range=[0, figure_width])\n        fig = fig.update_yaxes(\n            visible=False,\n            range=[0, figure_height],\n            # the scaleanchor attribute ensures that the aspect ratio stays constant\n            scaleanchor=\"x\",\n        )\n        fig = fig.update_layout(\n            width=figure_width,\n            height=figure_height,\n            margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n        )\n        return fig\n    else:\n        fig = go.Figure()\n        width, height = 1200, 1200\n        scale_factor = 0.25\n        w, h = width * scale_factor, height * scale_factor\n        padding = 10\n        n_components = self.components_.shape[0]\n        n_rows = n_components // n_cols + int(bool(n_components % n_cols))\n        figure_height = (h + padding) * n_rows\n        figure_width = (w + padding) * n_cols\n        fig = fig.add_trace(\n            go.Scatter(\n                x=[0, figure_width],\n                y=[0, figure_height],\n                mode=\"markers\",\n                marker_opacity=0,\n            )\n        )\n        vocab = self.get_vocab()\n        for i, component in enumerate(self.components_):\n            col = i % n_cols\n            row = i // n_cols\n            top_7 = vocab[np.argsort(-component)[:7]]\n            images = self.top_images[i]\n            image = self._image_grid(\n                images, (width, height), grid_size=(grid_size, grid_size)\n            )\n            x0 = (w + padding) * col\n            y0 = (h + padding) * (n_rows - row)\n            fig = fig.add_layout_image(\n                dict(\n                    x=x0,\n                    sizex=w,\n                    y=y0,\n                    sizey=h,\n                    xref=\"x\",\n                    yref=\"y\",\n                    opacity=1.0,\n                    layer=\"below\",\n                    sizing=\"stretch\",\n                    source=image,\n                ),\n            )\n            fig.add_annotation(\n                x=(w + padding) * col + (w / 2),\n                y=(h + padding) * (n_rows - row) - (h / 2),\n                text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(top_7),\n                font=dict(\n                    size=16,\n                    family=\"Times New Roman\",\n                    color=\"white\",\n                ),\n                bgcolor=\"rgba(0,0,0, 0.5)\",\n            )\n        fig = fig.update_xaxes(visible=False, range=[0, figure_width])\n        fig = fig.update_yaxes(\n            visible=False,\n            range=[0, figure_height],\n            # the scaleanchor attribute ensures that the aspect ratio stays constant\n            scaleanchor=\"x\",\n        )\n        fig = fig.update_layout(\n            width=figure_width,\n            height=figure_height,\n            margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n        )\n        return fig\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.print_representative_documents","title":"<code>print_representative_documents(topic_id, raw_documents=None, document_topic_matrix=None, top_k=5, show_negative=None)</code>","text":"<p>Pretty prints the highest ranking documents in a topic.</p> <p>Parameters:</p> Name Type Description Default <code>topic_id</code> <p>ID of the topic to display.</p> required <code>raw_documents</code> <p>List of documents to consider.</p> <code>None</code> <code>document_topic_matrix</code> <p>Document topic matrix to use. This is useful for transductive methods, as they cannot infer topics from text.</p> <code>None</code> <code>top_k</code> <p>Top K documents to show.</p> <code>5</code> <code>show_negative</code> <code>Optional[bool]</code> <p>Indicates whether lowest ranking documents should also be shown.</p> <code>None</code> Source code in <code>turftopic/container.py</code> <pre><code>def print_representative_documents(\n    self,\n    topic_id,\n    raw_documents=None,\n    document_topic_matrix=None,\n    top_k=5,\n    show_negative: Optional[bool] = None,\n):\n    \"\"\"Pretty prints the highest ranking documents in a topic.\n\n    Parameters\n    ----------\n    topic_id: int\n        ID of the topic to display.\n    raw_documents: list of str\n        List of documents to consider.\n    document_topic_matrix: ndarray of shape (n_documents, n_topics), optional\n        Document topic matrix to use. This is useful for transductive methods,\n        as they cannot infer topics from text.\n    top_k: int, default 5\n        Top K documents to show.\n    show_negative: bool, default False\n        Indicates whether lowest ranking documents should also be shown.\n    \"\"\"\n    columns, *rows = self._representative_docs(\n        topic_id,\n        raw_documents,\n        document_topic_matrix,\n        top_k,\n        show_negative,\n    )\n    table = Table(show_lines=True)\n    table.add_column(\n        \"Document\", justify=\"left\", style=\"magenta\", max_width=100\n    )\n    table.add_column(\"Score\", style=\"blue\", justify=\"right\")\n    for row in rows:\n        table.add_row(*row)\n    console = Console()\n    console.print(table)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.print_topic_distribution","title":"<code>print_topic_distribution(text=None, topic_dist=None, top_k=10)</code>","text":"<p>Pretty prints topic distribution in a document.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>Text to infer topic distribution for.</p> <code>None</code> <code>topic_dist</code> <p>Already inferred topic distribution for the text. This is useful for transductive methods, as they cannot infer topics from text.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>Top K topics to show.</p> <code>10</code> Source code in <code>turftopic/container.py</code> <pre><code>def print_topic_distribution(\n    self, text=None, topic_dist=None, top_k: int = 10\n):\n    \"\"\"Pretty prints topic distribution in a document.\n\n    Parameters\n    ----------\n    text: str, optional\n        Text to infer topic distribution for.\n    topic_dist: ndarray of shape (n_topics), optional\n        Already inferred topic distribution for the text.\n        This is useful for transductive methods,\n        as they cannot infer topics from text.\n    top_k: int, default 10\n        Top K topics to show.\n    \"\"\"\n    columns, *rows = self._topic_distribution(text, topic_dist, top_k)\n    table = Table()\n    table.add_column(\"Topic name\", justify=\"left\", style=\"magenta\")\n    table.add_column(\"Score\", justify=\"right\", style=\"blue\")\n    for row in rows:\n        table.add_row(*row)\n    console = Console()\n    console.print(table)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.print_topics","title":"<code>print_topics(top_k=10, show_scores=False, show_negative=None)</code>","text":"<p>Pretty prints topics in the model in a table.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words to return for each topic.</p> <code>10</code> <code>show_scores</code> <code>bool</code> <p>Indicates whether to show importance scores for each word.</p> <code>False</code> <code>show_negative</code> <code>Optional[bool]</code> <p>Indicates whether the most negative terms should also be displayed.</p> <code>None</code> Source code in <code>turftopic/container.py</code> <pre><code>def print_topics(\n    self,\n    top_k: int = 10,\n    show_scores: bool = False,\n    show_negative: Optional[bool] = None,\n):\n    \"\"\"Pretty prints topics in the model in a table.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of top words to return for each topic.\n    show_scores: bool, default False\n        Indicates whether to show importance scores for each word.\n    show_negative: bool, default False\n        Indicates whether the most negative terms should also be displayed.\n    \"\"\"\n    columns, *rows = self._topics_table(top_k, show_scores, show_negative)\n    table = Table(show_lines=True)\n    for column in columns:\n        if column == \"Highest Ranking\":\n            table.add_column(\n                column, justify=\"left\", style=\"magenta\", max_width=100\n            )\n        elif column == \"Lowest Ranking\":\n            table.add_column(\n                column, justify=\"left\", style=\"red\", max_width=100\n            )\n        elif column == \"Topic ID\":\n            table.add_column(column, style=\"blue\", justify=\"right\")\n        else:\n            table.add_column(column)\n    for row in rows:\n        table.add_row(*row)\n    console = Console()\n    console.print(table)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.print_topics_over_time","title":"<code>print_topics_over_time(top_k=5, show_scores=False, date_format='%Y %m %d')</code>","text":"<p>Pretty prints topics in the model in a table.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words to return for each topic.</p> <code>5</code> <code>show_scores</code> <code>bool</code> <p>Indicates whether to show importance scores for each word.</p> <code>False</code> Source code in <code>turftopic/container.py</code> <pre><code>def print_topics_over_time(\n    self,\n    top_k: int = 5,\n    show_scores: bool = False,\n    date_format: str = \"%Y %m %d\",\n):\n    \"\"\"Pretty prints topics in the model in a table.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of top words to return for each topic.\n    show_scores: bool, default False\n        Indicates whether to show importance scores for each word.\n    \"\"\"\n    columns, *rows = self._topics_over_time(\n        top_k, show_scores, date_format\n    )\n    table = Table(show_lines=True)\n    for column in columns:\n        table.add_column(column)\n    for row in rows:\n        table.add_row(*row)\n    console = Console()\n    console.print(table)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.rename_topics","title":"<code>rename_topics(names, use_documents=True)</code>","text":"<p>Rename topics in a model manually or automatically, using a namer.</p> <p>Examples: <pre><code>model.rename_topics([\"Automobiles\", \"Telephones\"])\n# Or:\nmodel.rename_topics({-1: \"Outliers\", 2: \"Christianity\"})\n# Or:\nnamer = OpenAIAnalyzer()\nmodel.rename_topics(namer)\n</code></pre></p> <p>Parameters:</p> Name Type Description Default <code>names</code> <code>Union[list[str], dict[int, str], Analyzer]</code> <p>Should be a list of topic names, or a mapping of topic IDs to names.</p> required <code>use_documents</code> <code>bool</code> <p>Indicates whether documents should be used when naming topics with an analyzer.</p> <code>True</code> Source code in <code>turftopic/container.py</code> <pre><code>def rename_topics(\n    self,\n    names: Union[list[str], dict[int, str], Analyzer],\n    use_documents: bool = True,\n) -&gt; None:\n    \"\"\"Rename topics in a model manually or automatically, using a namer.\n\n    Examples:\n    ```python\n    model.rename_topics([\"Automobiles\", \"Telephones\"])\n    # Or:\n    model.rename_topics({-1: \"Outliers\", 2: \"Christianity\"})\n    # Or:\n    namer = OpenAIAnalyzer()\n    model.rename_topics(namer)\n    ```\n\n    Parameters\n    ----------\n    names: list[str] or dict[int,str]\n        Should be a list of topic names, or a mapping of topic IDs to names.\n    use_documents: bool, default True\n        Indicates whether documents should be used when naming topics with an analyzer.\n    \"\"\"\n    if isinstance(names, Analyzer):\n        self._rename_automatic(names, use_documents=use_documents)\n    elif isinstance(names, dict):\n        topic_names = self.topic_names\n        for topic_id, topic_name in names.items():\n            try:\n                topic_id = list(self.classes_).index(topic_id)\n            except AttributeError:\n                pass\n            topic_names[topic_id] = topic_name\n        self.topic_names_ = topic_names\n    else:\n        names = list(names)\n        n_given = len(names)\n        n_topics = self.components_.shape[0]\n        if n_topics != n_given:\n            raise ValueError(\n                f\"Number of topics ({n_topics}) doesn't match the length of the given topic name list ({n_given}).\"\n            )\n        self.topic_names_ = names\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.representative_documents_df","title":"<code>representative_documents_df(topic_id, raw_documents=None, document_topic_matrix=None, top_k=5, show_negative=None)</code>","text":"<p>Collects highest ranking documents in a topic to a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>topic_id</code> <p>ID of the topic to display.</p> required <code>raw_documents</code> <p>List of documents to consider.</p> <code>None</code> <code>document_topic_matrix</code> <p>Document topic matrix to use. This is useful for transductive methods, as they cannot infer topics from text.</p> <code>None</code> <code>top_k</code> <p>Top K documents to show.</p> <code>5</code> <code>show_negative</code> <code>Optional[bool]</code> <p>Indicates whether lowest ranking documents should also be shown.</p> <code>None</code> Source code in <code>turftopic/container.py</code> <pre><code>def representative_documents_df(\n    self,\n    topic_id,\n    raw_documents=None,\n    document_topic_matrix=None,\n    top_k=5,\n    show_negative: Optional[bool] = None,\n):\n    \"\"\"Collects highest ranking documents in a topic to a dataframe.\n\n    Parameters\n    ----------\n    topic_id: int\n        ID of the topic to display.\n    raw_documents: list of str\n        List of documents to consider.\n    document_topic_matrix: ndarray of shape (n_documents, n_topics), optional\n        Document topic matrix to use. This is useful for transductive methods,\n        as they cannot infer topics from text.\n    top_k: int, default 5\n        Top K documents to show.\n    show_negative: bool, default False\n        Indicates whether lowest ranking documents should also be shown.\n    \"\"\"\n    try:\n        import pandas as pd\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"You need to pip install pandas to be able to use dataframes.\"\n        )\n    if show_negative is None:\n        show_negative = self.has_negative_side\n    raw_documents = raw_documents or getattr(self, \"corpus\", None)\n    if raw_documents is None:\n        raise ValueError(\n            \"No corpus was passed, can't search for representative documents.\"\n        )\n    document_topic_matrix = document_topic_matrix or getattr(\n        self, \"document_topic_matrix\", None\n    )\n    if document_topic_matrix is None:\n        try:\n            document_topic_matrix = self.transform(raw_documents)\n        except AttributeError:\n            raise ValueError(\n                \"Transductive methods cannot \"\n                \"infer topical content in documents.\\n\"\n                \"Please pass a document_topic_matrix.\"\n            )\n    try:\n        topic_id = list(self.classes_).index(topic_id)\n    except AttributeError:\n        pass\n    kth = min(top_k, document_topic_matrix.shape[0] - 1)\n    highest = np.argpartition(-document_topic_matrix[:, topic_id], kth)[\n        :kth\n    ]\n    highest = highest[\n        np.argsort(-document_topic_matrix[highest, topic_id])\n    ]\n    scores = document_topic_matrix[highest, topic_id]\n    columns = [[\"Document\", \"Score\"]]\n    rows = []\n    for document_id, score in zip(highest, scores):\n        doc = raw_documents[document_id]\n        rows.append([doc, score])\n    if show_negative:\n        lowest = np.argpartition(document_topic_matrix[:, topic_id], kth)[\n            :kth\n        ]\n        lowest = lowest[\n            np.argsort(document_topic_matrix[lowest, topic_id])\n        ]\n        lowest = lowest[::-1]\n        scores = document_topic_matrix[lowest, topic_id]\n        for document_id, score in zip(lowest, scores):\n            doc = raw_documents[document_id]\n            rows.append([doc, score])\n    return pd.DataFrame(rows, columns=columns)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.topic_distribution_df","title":"<code>topic_distribution_df(text=None, topic_dist=None, top_k=10)</code>","text":"<p>Extracts topic distribution into a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <p>Text to infer topic distribution for.</p> <code>None</code> <code>topic_dist</code> <p>Already inferred topic distribution for the text. This is useful for transductive methods, as they cannot infer topics from text.</p> <code>None</code> <code>top_k</code> <code>int</code> <p>Top K topics to show.</p> <code>10</code> Source code in <code>turftopic/container.py</code> <pre><code>def topic_distribution_df(\n    self, text=None, topic_dist=None, top_k: int = 10\n):\n    \"\"\"Extracts topic distribution into a dataframe.\n\n    Parameters\n    ----------\n    text: str, optional\n        Text to infer topic distribution for.\n    topic_dist: ndarray of shape (n_topics), optional\n        Already inferred topic distribution for the text.\n        This is useful for transductive methods,\n        as they cannot infer topics from text.\n    top_k: int, default 10\n        Top K topics to show.\n    \"\"\"\n    try:\n        import pandas as pd\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"You need to pip install pandas to be able to use dataframes.\"\n        )\n    if topic_dist is None:\n        if text is None:\n            raise ValueError(\n                \"You should either pass a text or a distribution.\"\n            )\n        try:\n            topic_dist = self.transform([text])\n        except AttributeError:\n            raise ValueError(\n                \"Transductive methods cannot \"\n                \"infer topical content in documents.\\n\"\n                \"Please pass a topic distribution.\"\n            )\n    topic_dist = np.squeeze(np.asarray(topic_dist))\n    highest = np.argsort(-topic_dist)[:top_k]\n    columns = []\n    columns.append(\"Topic name\")\n    columns.append(\"Score\")\n    rows = []\n    for ind in highest:\n        score = topic_dist[ind]\n        rows.append([self.topic_names[ind], score])\n    return pd.DataFrame(rows, columns=columns)\n</code></pre>"},{"location":"model_interpretation/#turftopic.container.TopicContainer.topics_df","title":"<code>topics_df(top_k=10, show_scores=False, show_negative=None)</code>","text":"<p>Extracts topics into a pandas dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>top_k</code> <code>int</code> <p>Number of top words to return for each topic.</p> <code>10</code> <code>show_scores</code> <code>bool</code> <p>Indicates whether to show importance scores for each word.</p> <code>False</code> <code>show_negative</code> <code>Optional[bool]</code> <p>Indicates whether the most negative terms should also be displayed.</p> <code>None</code> Source code in <code>turftopic/container.py</code> <pre><code>def topics_df(\n    self,\n    top_k: int = 10,\n    show_scores: bool = False,\n    show_negative: Optional[bool] = None,\n):\n    \"\"\"Extracts topics into a pandas dataframe.\n\n    Parameters\n    ----------\n    top_k: int, default 10\n        Number of top words to return for each topic.\n    show_scores: bool, default False\n        Indicates whether to show importance scores for each word.\n    show_negative: bool, default False\n        Indicates whether the most negative terms should also be displayed.\n    \"\"\"\n    try:\n        import pandas as pd\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"You need to pip install pandas to be able to use dataframes.\"\n        )\n    columns, *rows = self._topics_table(top_k, show_scores, show_negative)\n    return pd.DataFrame(rows, columns=columns)\n</code></pre>"},{"location":"model_overview/","title":"Model Overview","text":"<p>Turftopic contains implementations of a number of contemporary topic models. Some of these models might be similar to each other in a lot of aspects, but they might be different in others. It is quite important that you choose the right topic model for your use case.</p> <p>Looking for Model Performance?</p> <p>If you are interested in seeing how these models perform on a bunch of datasets, and would like to base your model choice on evaluations, make sure to check out the Model Leaderboard tab:</p> <p> </p> Model Summary Strengths Weaknesses Topeax Density peak detection + Gaussian mixture approximation Cluster quality, Topic quality, Stability, Automatic n-topics Underestimates N topics, Slower, No inference for new documents KeyNMF Keyword importance estimation + matrix factorization Reliability, Topic quality, Scalability to large corpora and long documents Automatic topic number detection, Multilingual performance, Sometimes includes stop words SensTopic(BETA) Regularized Semi-nonnegative matrix factorization in embedding space Very fast, High quality topics and clusters, Can assign multiple soft clusters to documents, GPU support Automatic n-topics is not too good GMM Soft clustering with Gaussian Mixtures and soft-cTF-IDF Reliability, Speed, Cluster quality Manual n-topics, Lower quality keywords, Curse of dimensionality FASTopic Neural topic modelling with Dual Semantic-relation Reconstruction High quality topics and clusters, GPU support Very slow, Memory hungry, Manual n-topics \\(S^3\\) Semantic axis discovery in embedding space Fastest, Human-readable topics Axes can be very unintuitive, Manual n-topics BERTopic and Top2Vec Embed -&gt; Reduce -&gt; Cluster Flexible, Feature rich Slow, Unreliable and unstable, Wildly overestimates number of clusters, Low topic and cluster quality AutoEncodingTopicModel Discover topics by generating BoW with a variational autoencoder GPU-support Slow, Sometimes low quality topics <p>Different models will naturally be good at different things, because they conceptualize topics differently for instance:</p> <ul> <li><code>BERTopic</code>, <code>Top2Vec</code>, <code>GMM</code> and <code>Topeax</code> find clusters of documents and treats those as topics</li> <li><code>KeyNMF</code>, <code>SensTopic</code>, <code>FASTopic</code> and <code>AutoEncodingTopicModel</code> conceptualize topics as latent nonnegative factors that generate the documents.</li> <li><code>SemanticSignalSeparation</code>(\\(S^3\\)) conceptualizes topics as semantic axes, along which topics are distributed.</li> </ul> <p>You can find a detailed overview of how each of these models work in their respective tabs.</p>"},{"location":"model_overview/#model-features","title":"Model Features","text":"<p>Some models are also capable of being used in a dynamic context, some can be fitted online, some can detect the number of topics for you and some can detect topic hierarchies. You can find an overview of these features in the table below.</p> Model  Multiple Topics per Document  Detecting Number of Topics  Dynamic Modeling  Hierarchical Modeling  Inference over New Documents  Cross-Lingual  Online Fitting KeyNMF SensTopic Topeax SemanticSignalSeparation ClusteringTopicModel GMM AutoEncodingTopicModel FASTopic"},{"location":"model_overview/#model-api-reference","title":"Model API Reference","text":""},{"location":"model_overview/#turftopic.base.ContextualModel","title":"<code>turftopic.base.ContextualModel</code>","text":"<p>             Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code>, <code>TopicContainer</code></p> <p>Base class for contextual topic models in Turftopic.</p> Source code in <code>turftopic/base.py</code> <pre><code>class ContextualModel(BaseEstimator, TransformerMixin, TopicContainer):\n    \"\"\"Base class for contextual topic models in Turftopic.\"\"\"\n\n    @property\n    def has_negative_side(self) -&gt; bool:\n        return False\n\n    def encode_documents(self, raw_documents: Iterable[str]) -&gt; np.ndarray:\n        \"\"\"Encodes documents with the sentence encoder of the topic model.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Textual documents to encode.\n\n        Return\n        ------\n        ndarray of shape (n_documents, n_dimensions)\n            Matrix of document embeddings.\n        \"\"\"\n        if not hasattr(self.encoder_, \"encode\"):\n            return self.encoder.get_text_embeddings(list(raw_documents))\n        return self.encoder_.encode(raw_documents)\n\n    @abstractmethod\n    def fit_transform(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Fits model and infers topic importances for each document.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        y: None\n            Ignored, exists for sklearn compatibility.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n\n        Returns\n        -------\n        ndarray of shape (n_documents, n_topics)\n            Document-topic matrix.\n        \"\"\"\n        pass\n\n    def fit(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ):\n        \"\"\"Fits model on the given corpus.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        y: None\n            Ignored, exists for sklearn compatibility.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n        \"\"\"\n        self.fit_transform(raw_documents, y, embeddings)\n        return self\n\n    def get_vocab(self) -&gt; np.ndarray:\n        \"\"\"Get vocabulary of the model.\n\n        Returns\n        -------\n        ndarray of shape (n_vocab)\n            All terms in the vocabulary.\n        \"\"\"\n        return self.vectorizer.get_feature_names_out()\n\n    def get_feature_names_out(self) -&gt; np.ndarray:\n        \"\"\"Get topic ids.\n\n        Returns\n        -------\n        ndarray of shape (n_topics)\n            IDs for each output feature of the model.\n            This is useful, since some models have outlier\n            detection, and this gets -1 as ID, instead of\n            its index.\n        \"\"\"\n        n_topics = self.components_.shape[0]\n        try:\n            classes = self.classes_\n        except AttributeError:\n            classes = list(range(n_topics))\n        return np.asarray(classes)\n\n    def prepare_topic_data(\n        self,\n        corpus: List[str],\n        embeddings: Optional[np.ndarray] = None,\n    ) -&gt; TopicData:\n        \"\"\"Produces topic inference data for a given corpus, that can be then used and reused.\n        Exists to allow visualizations out of the box with topicwizard.\n\n        Parameters\n        ----------\n        corpus: list of str\n            Documents to infer topical content for.\n        embeddings: ndarray of shape (n_documents, n_dimensions)\n            Embeddings of documents.\n\n        Returns\n        -------\n        TopicData\n            Information about topical inference in a dictionary.\n        \"\"\"\n        if embeddings is None:\n            embeddings = self.encode_documents(corpus)\n        try:\n            document_topic_matrix = self.transform(\n                corpus, embeddings=embeddings\n            )\n        except (AttributeError, NotFittedError):\n            document_topic_matrix = self.fit_transform(\n                corpus, embeddings=embeddings\n            )\n        dtm = self.vectorizer.transform(corpus)  # type: ignore\n        try:\n            classes = self.classes_\n        except AttributeError:\n            classes = list(range(self.components_.shape[0]))\n        res = TopicData(\n            corpus=corpus,\n            document_term_matrix=dtm,\n            vocab=self.get_vocab(),\n            document_topic_matrix=document_topic_matrix,\n            document_representation=embeddings,\n            topic_term_matrix=self.components_,  # type: ignore\n            transform=getattr(self, \"transform\", None),\n            topic_names=self.topic_names,\n            classes=classes,\n            has_negative_side=self.has_negative_side,\n            hierarchy=getattr(self, \"hierarchy\", None),\n        )\n        return res\n\n    def to_disk(self, out_dir: Union[Path, str]):\n        \"\"\"Persists model to directory on your machine.\n\n        Parameters\n        ----------\n        out_dir: Path | str\n            Directory to save the model to.\n        \"\"\"\n        out_dir = Path(out_dir)\n        out_dir.mkdir(exist_ok=True)\n        package_versions = get_package_versions()\n        with out_dir.joinpath(\"package_versions.json\").open(\"w\") as ver_file:\n            ver_file.write(json.dumps(package_versions))\n        joblib.dump(self, out_dir.joinpath(\"model.joblib\"))\n\n    def push_to_hub(self, repo_id: str):\n        \"\"\"Uploads model to HuggingFace Hub\n\n        Parameters\n        ----------\n        repo_id: str\n            Repository to upload the model to.\n        \"\"\"\n        api = HfApi()\n        api.create_repo(repo_id, exist_ok=True)\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            readme_path = Path(tmp_dir).joinpath(\"README.md\")\n            with readme_path.open(\"w\") as readme_file:\n                readme_file.write(create_readme(self, repo_id))\n            self.to_disk(tmp_dir)\n            api.upload_folder(\n                folder_path=tmp_dir,\n                repo_id=repo_id,\n                repo_type=\"model\",\n            )\n</code></pre>"},{"location":"model_overview/#turftopic.base.ContextualModel.encode_documents","title":"<code>encode_documents(raw_documents)</code>","text":"<p>Encodes documents with the sentence encoder of the topic model.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <code>Iterable[str]</code> <p>Textual documents to encode.</p> required Return <p>ndarray of shape (n_documents, n_dimensions)     Matrix of document embeddings.</p> Source code in <code>turftopic/base.py</code> <pre><code>def encode_documents(self, raw_documents: Iterable[str]) -&gt; np.ndarray:\n    \"\"\"Encodes documents with the sentence encoder of the topic model.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Textual documents to encode.\n\n    Return\n    ------\n    ndarray of shape (n_documents, n_dimensions)\n        Matrix of document embeddings.\n    \"\"\"\n    if not hasattr(self.encoder_, \"encode\"):\n        return self.encoder.get_text_embeddings(list(raw_documents))\n    return self.encoder_.encode(raw_documents)\n</code></pre>"},{"location":"model_overview/#turftopic.base.ContextualModel.fit","title":"<code>fit(raw_documents, y=None, embeddings=None)</code>","text":"<p>Fits model on the given corpus.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> required <code>y</code> <p>Ignored, exists for sklearn compatibility.</p> <code>None</code> <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> Source code in <code>turftopic/base.py</code> <pre><code>def fit(\n    self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n):\n    \"\"\"Fits model on the given corpus.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    y: None\n        Ignored, exists for sklearn compatibility.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n    \"\"\"\n    self.fit_transform(raw_documents, y, embeddings)\n    return self\n</code></pre>"},{"location":"model_overview/#turftopic.base.ContextualModel.fit_transform","title":"<code>fit_transform(raw_documents, y=None, embeddings=None)</code>  <code>abstractmethod</code>","text":"<p>Fits model and infers topic importances for each document.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> required <code>y</code> <p>Ignored, exists for sklearn compatibility.</p> <code>None</code> <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_documents, n_topics)</code> <p>Document-topic matrix.</p> Source code in <code>turftopic/base.py</code> <pre><code>@abstractmethod\ndef fit_transform(\n    self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n) -&gt; np.ndarray:\n    \"\"\"Fits model and infers topic importances for each document.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    y: None\n        Ignored, exists for sklearn compatibility.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n\n    Returns\n    -------\n    ndarray of shape (n_documents, n_topics)\n        Document-topic matrix.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"model_overview/#turftopic.base.ContextualModel.get_feature_names_out","title":"<code>get_feature_names_out()</code>","text":"<p>Get topic ids.</p> <p>Returns:</p> Type Description <code>ndarray of shape (n_topics)</code> <p>IDs for each output feature of the model. This is useful, since some models have outlier detection, and this gets -1 as ID, instead of its index.</p> Source code in <code>turftopic/base.py</code> <pre><code>def get_feature_names_out(self) -&gt; np.ndarray:\n    \"\"\"Get topic ids.\n\n    Returns\n    -------\n    ndarray of shape (n_topics)\n        IDs for each output feature of the model.\n        This is useful, since some models have outlier\n        detection, and this gets -1 as ID, instead of\n        its index.\n    \"\"\"\n    n_topics = self.components_.shape[0]\n    try:\n        classes = self.classes_\n    except AttributeError:\n        classes = list(range(n_topics))\n    return np.asarray(classes)\n</code></pre>"},{"location":"model_overview/#turftopic.base.ContextualModel.get_vocab","title":"<code>get_vocab()</code>","text":"<p>Get vocabulary of the model.</p> <p>Returns:</p> Type Description <code>ndarray of shape (n_vocab)</code> <p>All terms in the vocabulary.</p> Source code in <code>turftopic/base.py</code> <pre><code>def get_vocab(self) -&gt; np.ndarray:\n    \"\"\"Get vocabulary of the model.\n\n    Returns\n    -------\n    ndarray of shape (n_vocab)\n        All terms in the vocabulary.\n    \"\"\"\n    return self.vectorizer.get_feature_names_out()\n</code></pre>"},{"location":"model_overview/#turftopic.base.ContextualModel.prepare_topic_data","title":"<code>prepare_topic_data(corpus, embeddings=None)</code>","text":"<p>Produces topic inference data for a given corpus, that can be then used and reused. Exists to allow visualizations out of the box with topicwizard.</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>List[str]</code> <p>Documents to infer topical content for.</p> required <code>embeddings</code> <code>Optional[ndarray]</code> <p>Embeddings of documents.</p> <code>None</code> <p>Returns:</p> Type Description <code>TopicData</code> <p>Information about topical inference in a dictionary.</p> Source code in <code>turftopic/base.py</code> <pre><code>def prepare_topic_data(\n    self,\n    corpus: List[str],\n    embeddings: Optional[np.ndarray] = None,\n) -&gt; TopicData:\n    \"\"\"Produces topic inference data for a given corpus, that can be then used and reused.\n    Exists to allow visualizations out of the box with topicwizard.\n\n    Parameters\n    ----------\n    corpus: list of str\n        Documents to infer topical content for.\n    embeddings: ndarray of shape (n_documents, n_dimensions)\n        Embeddings of documents.\n\n    Returns\n    -------\n    TopicData\n        Information about topical inference in a dictionary.\n    \"\"\"\n    if embeddings is None:\n        embeddings = self.encode_documents(corpus)\n    try:\n        document_topic_matrix = self.transform(\n            corpus, embeddings=embeddings\n        )\n    except (AttributeError, NotFittedError):\n        document_topic_matrix = self.fit_transform(\n            corpus, embeddings=embeddings\n        )\n    dtm = self.vectorizer.transform(corpus)  # type: ignore\n    try:\n        classes = self.classes_\n    except AttributeError:\n        classes = list(range(self.components_.shape[0]))\n    res = TopicData(\n        corpus=corpus,\n        document_term_matrix=dtm,\n        vocab=self.get_vocab(),\n        document_topic_matrix=document_topic_matrix,\n        document_representation=embeddings,\n        topic_term_matrix=self.components_,  # type: ignore\n        transform=getattr(self, \"transform\", None),\n        topic_names=self.topic_names,\n        classes=classes,\n        has_negative_side=self.has_negative_side,\n        hierarchy=getattr(self, \"hierarchy\", None),\n    )\n    return res\n</code></pre>"},{"location":"model_overview/#turftopic.base.ContextualModel.push_to_hub","title":"<code>push_to_hub(repo_id)</code>","text":"<p>Uploads model to HuggingFace Hub</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>Repository to upload the model to.</p> required Source code in <code>turftopic/base.py</code> <pre><code>def push_to_hub(self, repo_id: str):\n    \"\"\"Uploads model to HuggingFace Hub\n\n    Parameters\n    ----------\n    repo_id: str\n        Repository to upload the model to.\n    \"\"\"\n    api = HfApi()\n    api.create_repo(repo_id, exist_ok=True)\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        readme_path = Path(tmp_dir).joinpath(\"README.md\")\n        with readme_path.open(\"w\") as readme_file:\n            readme_file.write(create_readme(self, repo_id))\n        self.to_disk(tmp_dir)\n        api.upload_folder(\n            folder_path=tmp_dir,\n            repo_id=repo_id,\n            repo_type=\"model\",\n        )\n</code></pre>"},{"location":"model_overview/#turftopic.base.ContextualModel.to_disk","title":"<code>to_disk(out_dir)</code>","text":"<p>Persists model to directory on your machine.</p> <p>Parameters:</p> Name Type Description Default <code>out_dir</code> <code>Union[Path, str]</code> <p>Directory to save the model to.</p> required Source code in <code>turftopic/base.py</code> <pre><code>def to_disk(self, out_dir: Union[Path, str]):\n    \"\"\"Persists model to directory on your machine.\n\n    Parameters\n    ----------\n    out_dir: Path | str\n        Directory to save the model to.\n    \"\"\"\n    out_dir = Path(out_dir)\n    out_dir.mkdir(exist_ok=True)\n    package_versions = get_package_versions()\n    with out_dir.joinpath(\"package_versions.json\").open(\"w\") as ver_file:\n        ver_file.write(json.dumps(package_versions))\n    joblib.dump(self, out_dir.joinpath(\"model.joblib\"))\n</code></pre>"},{"location":"multimodal/","title":"Multimodal Topic Modelling (BETA)","text":"<p>Note</p> <p>Multimodal modeling is still a BETA feature in Turftopic, and it is likely that we will add more features and change the interface in the near future.</p> <p>Some corpora spread across multiple modalities. A good example of this would be news articles with images attached. Turftopic now supports multimodal modelling with a number of models.</p>"},{"location":"multimodal/#multimodal-encoders","title":"Multimodal Encoders","text":"<p>In order for images to be usable in Turftopic, you will need an embedding model that can both encode texts and images. You can both use models that are supported in SentenceTransformers, or those that support the MTEB multimodal encoder interface.</p> <p>Use a multimodal encoder model </p> SentenceTransformersMTEB/MIEB <pre><code>from turftopic import KeyNMF\n\nmultimodal_keynmf = KeyNMF(10, encoder=\"clip-ViT-B-32\")\n</code></pre> <p>Tip</p> <p>You can find current state-of-the-art embedding models and their capabilities on the Massive Image Embedding Benchmark leaderboard.</p> <pre><code>pip install \"mteb&lt;2.0.0\"\n</code></pre> <pre><code>from turftopic import KeyNMF\nimport mteb\n\nencoder = mteb.get_model(\"kakaobrain/align-base\")\n\nmultimodal_keynmf = KeyNMF(10, encoder=\"clip-ViT-B-32\")\n</code></pre>"},{"location":"multimodal/#corpus-structure","title":"Corpus Structure","text":"<p>Currently all documents have to have an image attached to them, and only one image. This is a limitation, and we will address it in the future. Images can both be represented as file paths or <code>PIL.Image</code> objects.</p> <pre><code>from PIL import Image\n\nimages: list[Image] = [Image.open(\"file_path/something.jpeg\"), ...]\ntexts: list[str] = [...]\n\nlen(images) == len(texts)\n</code></pre>"},{"location":"multimodal/#basic-usage","title":"Basic Usage","text":"<p>All multimodal models have a <code>fit_multimodal()</code>/<code>fit_transform_multimodal()</code> method, that you can use to discover topics in multimodal corpora.</p> <p>Fit a multimodal model on a corpus</p> SemanticSignalSeparationKeyNMFClustering ModelsGMMAutoEncodingTopicModel <pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(12, encoder=\"clip-ViT-B-32\")\nmodel.fit_multimodal(texts, images=images)\nmodel.plot_multimodal_topics()\n</code></pre> <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(12, encoder=\"clip-ViT-B-32\")\nmodel.fit_multimodal(texts, images=images)\nmodel.plot_multimodal_topics()\n</code></pre> <pre><code>from turftopic import ClusteringTopicModel\n\n# BERTopic-style\nmodel = ClusteringTopicModel(encoder=\"clip-ViT-B-32\", feature_importance=\"c-tf-idf\")\n# Top2Vec-style\nmodel = ClusteringTopicModel(encoder=\"clip-ViT-B-32\", feature_importance=\"centroid\")\nmodel.fit_multimodal(texts, images=images)\nmodel.plot_multimodal_topics()\n</code></pre> <pre><code>from turftopic import GMM\n\nmodel = GMM(12, encoder=\"clip-ViT-B-32\")\nmodel.fit_multimodal(texts, images=images)\nmodel.plot_multimodal_topics()\n</code></pre> <pre><code>from turftopic import AutoEncodingTopicModel\n\n# CombinedTM\nmodel = AutoEncodingTopicModel(12, combined=True, encoder=\"clip-ViT-B-32\")\n# ZeroShotTM\nmodel = AutoEncodingTopicModel(12, combined=False, encoder=\"clip-ViT-B-32\")\nmodel.fit_multimodal(texts, images=images)\nmodel.plot_multimodal_topics()\n</code></pre>"},{"location":"multimodal/#api-reference","title":"API reference","text":""},{"location":"multimodal/#turftopic.multimodal.MultimodalModel","title":"<code>turftopic.multimodal.MultimodalModel</code>","text":"<p>Base model for multimodal topic models.</p> Source code in <code>turftopic/multimodal.py</code> <pre><code>class MultimodalModel:\n    \"\"\"Base model for multimodal topic models.\"\"\"\n\n    def encode_multimodal(\n        self,\n        sentences: list[str],\n        images: list[ImageRepr],\n    ) -&gt; dict[str, np.ndarray]:\n        \"\"\"Produce multimodal embeddings of the documents passed to the model.\n\n        Parameters\n        ----------\n        sentences: list[str]\n            Textual documents to encode.\n        images: list[ImageRepr]\n            Corresponding images for each document.\n\n        Returns\n        -------\n        MultimodalEmbeddings\n            Text, image and joint document embeddings.\n\n        \"\"\"\n        return encode_multimodal(self.encoder_, sentences, images)\n\n    @staticmethod\n    def validate_embeddings(embeddings: Optional[MultimodalEmbeddings]):\n        if embeddings is None:\n            return\n        try:\n            document_embeddings = embeddings[\"document_embeddings\"]\n            image_embeddings = embeddings[\"image_embeddings\"]\n        except KeyError as e:\n            raise TypeError(\n                \"embeddings do not contain document and image embeddings, can't be used for multimodal modelling.\"\n            ) from e\n        if document_embeddings.shape != image_embeddings.shape:\n            raise ValueError(\n                f\"Shape mismatch between document_embeddings {document_embeddings.shape} and image_embeddings {image_embeddings.shape}\"\n            )\n\n    def validate_encoder(self):\n        if not hasattr(self.encoder_, \"encode\"):\n            if not all(\n                (\n                    hasattr(self.encoder_, \"get_text_embeddings\"),\n                    hasattr(self.encoder_, \"get_image_embeddings\"),\n                ),\n            ):\n                raise TypeError(\n                    \"An encoder must either have an encode() method or a get_text_embeddings and get_image_embeddings method (optionally get_fused_embeddings)\"\n                )\n\n    @abstractmethod\n    def fit_transform_multimodal(\n        self,\n        raw_documents: list[str],\n        images: list[ImageRepr],\n        y=None,\n        embeddings: Optional[MultimodalEmbeddings] = None,\n    ) -&gt; np.ndarray:\n        \"\"\"Fits topic model in a multimodal context and returns the document-topic matrix.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        images: list[ImageRepr]\n            Images corresponding to each document.\n        y: None\n            Ignored, exists for sklearn compatibility.\n        embeddings: MultimodalEmbeddings\n            Precomputed multimodal embeddings.\n\n        Returns\n        -------\n        ndarray of shape (n_documents, n_topics)\n            Document-topic matrix.\n        \"\"\"\n        pass\n\n    def fit_multimodal(\n        self,\n        raw_documents: list[str],\n        images: list[ImageRepr],\n        y=None,\n        embeddings: Optional[MultimodalEmbeddings] = None,\n    ):\n        \"\"\"Fits topic model on a multimodal corpus.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        images: list[ImageRepr]\n            Images corresponding to each document.\n        y: None\n            Ignored, exists for sklearn compatibility.\n        embeddings: MultimodalEmbeddings\n            Precomputed multimodal embeddings.\n\n        Returns\n        -------\n        Self\n            The fitted topic model\n        \"\"\"\n        self.fit_transform_multimodal(raw_documents, images, y, embeddings)\n        return self\n\n    @staticmethod\n    def collect_top_images(\n        images: list[Image.Image],\n        image_topic_matrix: np.ndarray,\n        n_images: int = 20,\n        negative: bool = False,\n    ) -&gt; list[list[Image.Image]]:\n        top_images: list[list[Image.Image]] = []\n        for image_topic_vector in image_topic_matrix.T:\n            if negative:\n                image_topic_vector = -image_topic_vector\n            top_im_ind = np.argsort(-image_topic_vector)[:20]\n            top_im = [images[int(i)] for i in top_im_ind]\n            top_images.append(top_im)\n        return top_images\n\n    def prepare_multimodal_topic_data(\n        self,\n        corpus: list[str],\n        images: list[ImageRepr],\n        embeddings: Optional[MultimodalEmbeddings] = None,\n    ) -&gt; TopicData:\n        \"\"\"Produces multimodal topic inference data for a given corpus, that can be then used and reused.\n        Exists to allow visualizations out of the box with topicwizard.\n\n        Parameters\n        ----------\n        corpus: list[str]\n            Documents to infer topical content for.\n        images: list[ImageRepr]\n            Images belonging to the documents.\n        embeddings: MultimodalEmbeddings\n            Embeddings of documents.\n\n        Returns\n        -------\n        TopicData\n            Information about topical inference in a dictionary.\n        \"\"\"\n        if embeddings is None:\n            embeddings = self.encode_multimodal(corpus, images)\n        document_topic_matrix = self.fit_transform_multimodal(\n            corpus, images=images, embeddings=embeddings\n        )\n        dtm = self.vectorizer.transform(corpus)  # type: ignore\n        try:\n            classes = self.classes_\n        except AttributeError:\n            classes = list(range(self.components_.shape[0]))\n        res = TopicData(\n            corpus=corpus,\n            document_term_matrix=dtm,\n            vocab=self.get_vocab(),\n            document_topic_matrix=document_topic_matrix,\n            document_representation=embeddings[\"document_embeddings\"],\n            topic_term_matrix=self.components_,  # type: ignore\n            transform=getattr(self, \"transform\", None),\n            topic_names=self.topic_names,\n            classes=classes,\n            has_negative_side=self.has_negative_side,\n            hierarchy=getattr(self, \"hierarchy\", None),\n            images=images,\n            top_images=self.top_images,\n            negative_images=getattr(self, \"negative_images\", None),\n        )\n        return res\n</code></pre>"},{"location":"multimodal/#turftopic.multimodal.MultimodalModel.encode_multimodal","title":"<code>encode_multimodal(sentences, images)</code>","text":"<p>Produce multimodal embeddings of the documents passed to the model.</p> <p>Parameters:</p> Name Type Description Default <code>sentences</code> <code>list[str]</code> <p>Textual documents to encode.</p> required <code>images</code> <code>list[ImageRepr]</code> <p>Corresponding images for each document.</p> required <p>Returns:</p> Type Description <code>MultimodalEmbeddings</code> <p>Text, image and joint document embeddings.</p> Source code in <code>turftopic/multimodal.py</code> <pre><code>def encode_multimodal(\n    self,\n    sentences: list[str],\n    images: list[ImageRepr],\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Produce multimodal embeddings of the documents passed to the model.\n\n    Parameters\n    ----------\n    sentences: list[str]\n        Textual documents to encode.\n    images: list[ImageRepr]\n        Corresponding images for each document.\n\n    Returns\n    -------\n    MultimodalEmbeddings\n        Text, image and joint document embeddings.\n\n    \"\"\"\n    return encode_multimodal(self.encoder_, sentences, images)\n</code></pre>"},{"location":"multimodal/#turftopic.multimodal.MultimodalModel.fit_multimodal","title":"<code>fit_multimodal(raw_documents, images, y=None, embeddings=None)</code>","text":"<p>Fits topic model on a multimodal corpus.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <code>list[str]</code> <p>Documents to fit the model on.</p> required <code>images</code> <code>list[ImageRepr]</code> <p>Images corresponding to each document.</p> required <code>y</code> <p>Ignored, exists for sklearn compatibility.</p> <code>None</code> <code>embeddings</code> <code>Optional[MultimodalEmbeddings]</code> <p>Precomputed multimodal embeddings.</p> <code>None</code> <p>Returns:</p> Type Description <code>Self</code> <p>The fitted topic model</p> Source code in <code>turftopic/multimodal.py</code> <pre><code>def fit_multimodal(\n    self,\n    raw_documents: list[str],\n    images: list[ImageRepr],\n    y=None,\n    embeddings: Optional[MultimodalEmbeddings] = None,\n):\n    \"\"\"Fits topic model on a multimodal corpus.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    images: list[ImageRepr]\n        Images corresponding to each document.\n    y: None\n        Ignored, exists for sklearn compatibility.\n    embeddings: MultimodalEmbeddings\n        Precomputed multimodal embeddings.\n\n    Returns\n    -------\n    Self\n        The fitted topic model\n    \"\"\"\n    self.fit_transform_multimodal(raw_documents, images, y, embeddings)\n    return self\n</code></pre>"},{"location":"multimodal/#turftopic.multimodal.MultimodalModel.fit_transform_multimodal","title":"<code>fit_transform_multimodal(raw_documents, images, y=None, embeddings=None)</code>  <code>abstractmethod</code>","text":"<p>Fits topic model in a multimodal context and returns the document-topic matrix.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <code>list[str]</code> <p>Documents to fit the model on.</p> required <code>images</code> <code>list[ImageRepr]</code> <p>Images corresponding to each document.</p> required <code>y</code> <p>Ignored, exists for sklearn compatibility.</p> <code>None</code> <code>embeddings</code> <code>Optional[MultimodalEmbeddings]</code> <p>Precomputed multimodal embeddings.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_documents, n_topics)</code> <p>Document-topic matrix.</p> Source code in <code>turftopic/multimodal.py</code> <pre><code>@abstractmethod\ndef fit_transform_multimodal(\n    self,\n    raw_documents: list[str],\n    images: list[ImageRepr],\n    y=None,\n    embeddings: Optional[MultimodalEmbeddings] = None,\n) -&gt; np.ndarray:\n    \"\"\"Fits topic model in a multimodal context and returns the document-topic matrix.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    images: list[ImageRepr]\n        Images corresponding to each document.\n    y: None\n        Ignored, exists for sklearn compatibility.\n    embeddings: MultimodalEmbeddings\n        Precomputed multimodal embeddings.\n\n    Returns\n    -------\n    ndarray of shape (n_documents, n_topics)\n        Document-topic matrix.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"multimodal/#turftopic.multimodal.MultimodalModel.prepare_multimodal_topic_data","title":"<code>prepare_multimodal_topic_data(corpus, images, embeddings=None)</code>","text":"<p>Produces multimodal topic inference data for a given corpus, that can be then used and reused. Exists to allow visualizations out of the box with topicwizard.</p> <p>Parameters:</p> Name Type Description Default <code>corpus</code> <code>list[str]</code> <p>Documents to infer topical content for.</p> required <code>images</code> <code>list[ImageRepr]</code> <p>Images belonging to the documents.</p> required <code>embeddings</code> <code>Optional[MultimodalEmbeddings]</code> <p>Embeddings of documents.</p> <code>None</code> <p>Returns:</p> Type Description <code>TopicData</code> <p>Information about topical inference in a dictionary.</p> Source code in <code>turftopic/multimodal.py</code> <pre><code>def prepare_multimodal_topic_data(\n    self,\n    corpus: list[str],\n    images: list[ImageRepr],\n    embeddings: Optional[MultimodalEmbeddings] = None,\n) -&gt; TopicData:\n    \"\"\"Produces multimodal topic inference data for a given corpus, that can be then used and reused.\n    Exists to allow visualizations out of the box with topicwizard.\n\n    Parameters\n    ----------\n    corpus: list[str]\n        Documents to infer topical content for.\n    images: list[ImageRepr]\n        Images belonging to the documents.\n    embeddings: MultimodalEmbeddings\n        Embeddings of documents.\n\n    Returns\n    -------\n    TopicData\n        Information about topical inference in a dictionary.\n    \"\"\"\n    if embeddings is None:\n        embeddings = self.encode_multimodal(corpus, images)\n    document_topic_matrix = self.fit_transform_multimodal(\n        corpus, images=images, embeddings=embeddings\n    )\n    dtm = self.vectorizer.transform(corpus)  # type: ignore\n    try:\n        classes = self.classes_\n    except AttributeError:\n        classes = list(range(self.components_.shape[0]))\n    res = TopicData(\n        corpus=corpus,\n        document_term_matrix=dtm,\n        vocab=self.get_vocab(),\n        document_topic_matrix=document_topic_matrix,\n        document_representation=embeddings[\"document_embeddings\"],\n        topic_term_matrix=self.components_,  # type: ignore\n        transform=getattr(self, \"transform\", None),\n        topic_names=self.topic_names,\n        classes=classes,\n        has_negative_side=self.has_negative_side,\n        hierarchy=getattr(self, \"hierarchy\", None),\n        images=images,\n        top_images=self.top_images,\n        negative_images=getattr(self, \"negative_images\", None),\n    )\n    return res\n</code></pre>"},{"location":"multimodal/#turftopic.encoders.multimodal.MultimodalEncoder","title":"<code>turftopic.encoders.multimodal.MultimodalEncoder</code>","text":"<p>             Bases: <code>Protocol</code></p> <p>Base class for external encoder models.</p> Source code in <code>turftopic/encoders/multimodal.py</code> <pre><code>class MultimodalEncoder(Protocol):\n    \"\"\"Base class for external encoder models.\"\"\"\n\n    def get_text_embeddings(\n        self,\n        texts: list[str],\n        *,\n        batch_size: int = 8,\n        **kwargs,\n    ): ...\n\n    def get_image_embeddings(\n        self,\n        images: list[Image.Image],\n        *,\n        batch_size: int = 8,\n        **kwargs,\n    ): ...\n\n    def get_fused_embeddings(\n        self,\n        texts: list[str] = None,\n        images: list[Image.Image] = None,\n        batch_size: int = 8,\n        **kwargs,\n    ): ...\n</code></pre>"},{"location":"online/","title":"Online Topic Modeling","text":"<p>Some models in Turftopic can be fitted in an online manner (currently this only includes KeyNMF). These models can be fitted in minibatches instead of the entire corpus at the same time.</p>"},{"location":"online/#use-cases","title":"Use Cases:","text":"<ol> <li>You can use online fitting when you have very large corpora at hand, and it would be impractical to fit a model on it at once.</li> <li>You have new data flowing in constantly, and need a model that can morph the topics based on the incoming data. You can also do this in a dynamic fashion.</li> <li>You need to finetune an already fitted topic model to novel data.</li> </ol>"},{"location":"online/#batch-fitting","title":"Batch Fitting","text":"<p>We will use the batching function from the itertools recipes to produce batches.</p> <p>In newer versions of Python (&gt;=3.12) you can just <code>from itertools import batched</code></p> <pre><code>def batched(iterable, n: int):\n    \"Batch data into lists of length n. The last batch may be shorter.\"\n    if n &lt; 1:\n        raise ValueError(\"n must be at least one\")\n    it = iter(iterable)\n    while batch := tuple(itertools.islice(it, n)):\n        yield batch\n</code></pre> <p>You can fit a model to a very large corpus in batches like so:</p> <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(10, top_n=5)\n\ncorpus = [\"some string\", \"etc\", ...]\nfor batch in batched(corpus, 200):\n    batch = list(batch)\n    model.partial_fit(batch)\n</code></pre> <p>You might want to train in epochs, so that the model sees the same documents multiple times, this might be useful in numerous settings:</p> <pre><code>for epoch in range(5):\n  for batch in batched(corpus, 200):\n      batch = list(batch)\n      model.partial_fit(batch)\n</code></pre>"},{"location":"online/#finetuning-a-model","title":"Finetuning a Model","text":"<p>You can pretrain a topic model on a large corpus and then finetune it on a novel corpus the model has not seen before. This will morph the model's topics to the corpus at hand.</p> <p>In this example I will load a pretrained KeyNMF model from disk. (see Model Loading and Saving)</p> <pre><code>from turftopic import load_model\n\nmodel = load_model(\"pretrained_keynmf_model\")\n\nnew_corpus: list[str] = [...]\n# Finetune the model to the new corpus\nmodel.partial_fit(new_corpus)\n\nmodel.to_disk(\"finetuned_model/\")\n</code></pre>"},{"location":"online/#precomputed-embeddings","title":"Precomputed Embeddings","text":"<p>In the case of very large corpora it is common to precompute embeddings before fitting the model. You can still do this with <code>partial_fit()</code>, you just have to be careful to correctly match the embedding indices with the corpus indices.</p> <p>We provide an example of correct usage here.</p> <p>You might have a <code>utils.py</code> file with a function to load your corpus: <pre><code>def load_corpus() -&gt; list[str]:\n    \"\"\"Function that loads the corpus from some source.\"\"\"\n    ...\n</code></pre></p> <p>Then you have a file which computes the embeddings and saves them to disk: <pre><code>import numpy as np\nfrom sentence_transformers import SentenceTransformers\n\nfrom utils import load_corpus\n\ncorpus = load_corpus()\n\ntrf = SentenceTransformers(\"all-MiniLM-L6-v2\")\nembeddings = trf.encode(corpus)\n\nnp.save(\"embeddings.npy\")\n</code></pre></p> <p>This file then trains the model on the precomputed embeddings: <pre><code>import numpy as np\nfrom turftopic import KeyNMF\n\nfrom utils import load_corpus\n\ncorpus = load_corpus()\nembeddings = np.load(\"embeddings.npy\")\n\nmodel = KeyNMF(10, encoder=\"all-MiniLM-L6-v2\")\nfor batch in batched(zip(corpus, embeddings), 200):\n    text_batch, embedding_batch = zip(*batch)\n    text_batch = list(text_batch)\n    embedding_batch = np.stack(embedding_batch)\n    model.partial_fit(text_batch, embeddings=embedding_batch)\n</code></pre></p>"},{"location":"persistence/","title":"Saving and loading","text":""},{"location":"persistence/#model-persistence","title":"Model persistence","text":"<p>All models in Turftopic can be serialized and saved to disk, or published to the HuggingFace Hub.</p>"},{"location":"persistence/#saving-locally","title":"Saving locally","text":"<p>Turftopic models can now be saved to disk using the <code>to_disk()</code> method of models:</p> <pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(10).fit(corpus)\nmodel.to_disk(\"./local_directory/\")\n</code></pre>"},{"location":"persistence/#publishing-models","title":"Publishing models","text":"<p>Models can also be pushed to HuggingFace repositories. This way, others can also easily access and modify topic models you've trained.</p> <pre><code># The repository name is, of course, arbitrary but descriptive\nmodel.push_to_hub(\"your_user/s3_20-newsgroups_10-topics\")\n</code></pre>"},{"location":"persistence/#loading-models","title":"Loading models","text":"<p>You can load models from either the Hub or disk using the <code>load_model()</code> function:</p> <pre><code>from turftopic import load_model\n\nmodel = load_model(\"./local_directory/\")\n# or from hub\nmodel = load_model(\"your_user/s3_20-newsgroups_10-topics\")\n</code></pre>"},{"location":"persistence/#topicdata-persistence","title":"<code>TopicData</code> persistence","text":"<p>You can also save and load <code>TopicData</code> objects with Turftopic. These are saved using <code>joblib</code> and therefore we recommend that you give a <code>.joblib</code> file extension to all <code>TopicData</code> files:</p> <p>Note on compatibility</p> <p>For backwards compatibility, <code>TopicData</code> objects are saved using <code>joblib</code> as simple <code>dict</code> objects. If you simply load a saved <code>TopicData</code> object with joblib without using <code>from_disk()</code>, it will load as a <code>dict</code>.</p> SaveLoad <pre><code>topic_data = model.prepare_topic_data(corpus)\ntopic_data.to_disk(\"topic_data.joblib\")\n</code></pre> <pre><code>from turftopic.data import TopicData\n\ntopic_data = TopicData.from_disk(\"topic_data.joblib\")\n</code></pre>"},{"location":"s3/","title":"Semantic Signal Separation (\\(S^3\\))","text":"<p>Semantic Signal Separation tries to recover dimensions/axes along which most of the semantic variations can be explained. A topic in \\(S^3\\) is an axis of semantics in the corpus. This makes the model able to recover more nuanced topical content in documents, but is not optimal when you expect topics to be groupings of documents.</p>  Schematic overview of S\u00b3   <p>\\(S^3\\) is one of the fastest topic models out there, even rivalling vanilla NMF, when not accounting for embedding time. It also typically produces very high quality topics, and our evaluations indicate that it performs significantly better when no preprocessing is applied to texts.</p>"},{"location":"s3/#how-does-s3-work","title":"How does \\(S^3\\) work?","text":""},{"location":"s3/#step-1-document-embedding-decomposition","title":"Step 1: Document-embedding Decomposition","text":"<p>The first step is to decompose the embedding matrix using ICA, this step discovers the underlying semantics axes as latent independent components in the embeddings.</p> See formula <ul> <li>Let the encodings of documents in the corpus be \\(X\\).</li> <li>Decompose \\(X\\) using FastICA: \\(X = AS\\), where \\(A\\) is the mixing matrix and \\(S\\) is the document-topic-matrix.</li> </ul>"},{"location":"s3/#step-2-term-importance-estimation","title":"Step 2: Term Importance Estimation","text":"<p>Term importances for each topic are calculated by encoding the entire vocabulary of the corpus using the same embedding model, then recovering the strength of each latent component in the word embedding matrix. The strength of the components in the words will be interpreted as the words' importance in a given topic.</p>  Visual representation of term importance approaches in S\u00b3   See formula <ul> <li>Let the matrix of word encodings be \\(V\\).</li> <li>Calculate the pseudo-inverse of the mixing matrix \\(C = A^{+}\\), where \\(C\\) is the unmixing matrix.</li> <li>Project word embeddings onto the semantic axes by multiplying them with unmixing matrix: \\(W = VC^T\\). \\(W^T\\) is then the topic-term matrix (<code>model.components_</code>).</li> </ul> <p>There are three distinct methods to calculate term importances from word projections:</p> <p>Choose a word importance method</p> AxialAngularCombined (default) <p><pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(n_components=10, feature_importance=\"axial\")\n</code></pre> Axial word importances are defined as the words' positions on the semantic axes. This approach selects highly relevant words for topic descriptions, but topic descriptions might share words if a word scores high on multiple axes.</p> <p>The importance of word \\(j\\) for topic \\(t\\) is: \\(\\beta_{tj} = W_{jt}\\)</p> <p><pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(n_components=10, feature_importance=\"angular\")\n</code></pre> Angular topics can be calculated by taking the cosine of the angle between projected word vectors and semantic axes. This allows the approach axis descriptions to be very distinct and specific to the given axis, but might include words that are not as relevant in the corpus.</p> <p>\\(\\beta_{tj} = cos(\\Theta) = \\frac{W_{jt}}{||W_j||}\\)</p> <p><pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(n_components=10, feature_importance=\"combined\")\n</code></pre> Combined  word importance is a combination of axial and andular term importance, and is recommended as it balances the two approaches' strengths and weaknesses.</p> <p>\\(\\beta_{tj} = \\frac{(W_{jt})^3}{||W_j||}\\)</p>"},{"location":"s3/#dynamic-topic-modeling","title":"Dynamic Topic Modeling","text":"<p>\\(S^3\\) can also be used as a dynamic topic model. Temporally changing components are found using the following steps:</p> <ol> <li>Fit a global \\(S^3\\) model over the whole corpus.</li> <li>Estimate unmixing matrix for each time-slice by fitting a linear regression from the embeddings in the time slice to the document-topic-matrix for the time slice estimated by the global model.</li> <li>Estimate term importances for each time slice the same way as the global model.</li> </ol> <pre><code>from datetime import datetime\nfrom turftopic import SemanticSignalSeparation\n\nts: list[datetime] = [datetime(year=2018, month=2, day=12), ...]\ncorpus: list[str] = [\"First document\", ...]\n\nmodel = SemanticSignalSeparation(10).fit_dynamic(corpus, timestamps=ts, bins=10)\nmodel.plot_topics_over_time()\n</code></pre> <p>Info</p> <p>Topics over time in \\(S^3\\) are treated slightly differently to most other models. This is because topics are not proportional in \\(S^3\\), and can tip below zero. In the timeslices where a topic is below zero, its negative definition is displayed.</p>  Topics over time in a dynamic Semantic Signal Separation model."},{"location":"s3/#model-refitting","title":"Model Refitting","text":"<p>Unlike most other models in Turftopic, \\(S^3\\) can be refit using different parameters and random seeds without needing to initialize the model from scratch. This makes \\(S^3\\) incredibly convenient for exploring different numbers of topics, or adjusting the number of iterations.</p> <p>Refitting the model takes a fraction of the time of initializing a new one and fitting it, as the vocabulary doesn't have to be learned or encoded by the model again.</p> <pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(5, random_state=42)\nmodel.fit(corpus)\n\nprint(len(model.topic_names))\n# 5\n\nmodel.refit(corpus, n_components=10, random_state=30)\nprint(len(model.topic_names))\n# 10\n</code></pre>"},{"location":"s3/#interpretation","title":"Interpretation","text":""},{"location":"s3/#negative-terms","title":"Negative terms","text":"<p>Terms, which rank lowest on a topic have meaning in \\(S^3\\). Whenever interpreting semantic axes, you should probably consider both ends of the axis. As such, when you print or export topics from \\(S^3\\), the lowest ranking terms will also be shown along with the highest ranking ones.</p> <p>Here's an example on ArXiv ML papers:</p> <pre><code>from turftopic import SemanticSignalSeparation\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nmodel = SemanticSignalSeparation(5, vectorizer=CountVectorizer(), random_state=42)\nmodel.fit(corpus)\n\nmodel.print_topics(top_k=5)\n</code></pre> Positive Negative 0 clustering, histograms, clusterings, histogram, classifying reinforcement, exploration, planning, tactics, reinforce 1 textual, pagerank, litigants, marginalizing, entailment matlab, waveforms, microcontroller, accelerometers, microcontrollers 2 sparsestmax, denoiseing, denoising, minimizers, minimizes automation, affective, chatbots, questionnaire, attitudes 3 rebmigraph, subgraph, subgraphs, graphsage, graph adversarial, adversarially, adversarialization, adversary, security 4 clustering, estimations, algorithm, dbscan, estimation cnn, deepmind, deeplabv3, convnet, deepseenet"},{"location":"s3/#concept-compass","title":"Concept Compass","text":"<p>If you want to gain a deeper understanding of terms' relation to axes, you can produce a concept compass. This involves plotting terms in a corpus along two semantic axes.</p> <p>In order to use the compass in Turftopic you will need to have <code>plotly</code> installed:</p> <pre><code>pip install plotly\n</code></pre> <p>You can display a compass based on a fitted model like so:</p> <pre><code>fig = model.plot_concept_compass(topic_x=1, topic_y=4)\nfig.show()\n</code></pre>  Concept Compass of ArXiv ML Papers along two semantic axes."},{"location":"s3/#image-compass","title":"Image Compass","text":"<p>In multimodal contexts, you can also plot images along two chosen axes by using <code>plot_image_compass()</code>.</p> <pre><code>model = SemanticSignalSeparation(10)\nmodel.fit_multimodal(corpus, images=images)\n\nfig = model.plot_image_compass(topic_x=0, topic_y=1)\nfig.show()\n</code></pre>  Image Compass of IKEA furnitures along two semantic axes"},{"location":"s3/#api-reference","title":"API Reference","text":""},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation","title":"<code>turftopic.models.decomp.SemanticSignalSeparation</code>","text":"<p>             Bases: <code>ContextualModel</code>, <code>DynamicTopicModel</code>, <code>MultimodalModel</code></p> <p>Separates the embedding matrix into 'semantic signals' with component analysis methods. Topics are assumed to be dimensions of semantics.</p> <pre><code>from turftopic import SemanticSignalSeparation\n\ncorpus: list[str] = [\"some text\", \"more text\", ...]\n\nmodel = SemanticSignalSeparation(10).fit(corpus)\nmodel.print_topics()\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>n_components</code> <code>int</code> <p>Number of topics.</p> <code>10</code> <code>encoder</code> <code>Union[Encoder, str, MultimodalEncoder]</code> <p>Model to encode documents/terms, all-MiniLM-L6-v2 is the default.</p> <code>'sentence-transformers/all-MiniLM-L6-v2'</code> <code>vectorizer</code> <code>Optional[CountVectorizer]</code> <p>Vectorizer used for term extraction. Can be used to prune or filter the vocabulary.</p> <code>None</code> <code>decomposition</code> <code>Optional[TransformerMixin]</code> <p>Custom decomposition method to use. Can be an instance of FastICA or PCA, or basically any dimensionality reduction method. Has to have <code>fit_transform</code> and <code>fit</code> methods. If not specified, FastICA is used.</p> <code>None</code> <code>max_iter</code> <code>int</code> <p>Maximum number of iterations for ICA.</p> <code>200</code> <code>feature_importance</code> <code>Literal['axial', 'angular', 'combined']</code> <p>Defines whether the word's position on an axis ('axial'), it's angle to the axis ('angular') or their combination ('combined') should determine the word's importance for a topic.</p> <code>'combined'</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state to use so that results are exactly reproducible.</p> <code>None</code> Source code in <code>turftopic/models/decomp.py</code> <pre><code>class SemanticSignalSeparation(\n    ContextualModel, DynamicTopicModel, MultimodalModel\n):\n    \"\"\"Separates the embedding matrix into 'semantic signals' with\n    component analysis methods.\n    Topics are assumed to be dimensions of semantics.\n\n    ```python\n    from turftopic import SemanticSignalSeparation\n\n    corpus: list[str] = [\"some text\", \"more text\", ...]\n\n    model = SemanticSignalSeparation(10).fit(corpus)\n    model.print_topics()\n    ```\n\n    Parameters\n    ----------\n    n_components: int, default 10\n        Number of topics.\n    encoder: str or SentenceTransformer\n        Model to encode documents/terms, all-MiniLM-L6-v2 is the default.\n    vectorizer: CountVectorizer, default None\n        Vectorizer used for term extraction.\n        Can be used to prune or filter the vocabulary.\n    decomposition: TransformerMixin, default None\n        Custom decomposition method to use.\n        Can be an instance of FastICA or PCA, or basically any dimensionality\n        reduction method. Has to have `fit_transform` and `fit` methods.\n        If not specified, FastICA is used.\n    max_iter: int, default 200\n        Maximum number of iterations for ICA.\n    feature_importance: \"axial\", \"angular\" or \"combined\", default \"combined\"\n        Defines whether the word's position on an axis ('axial'), it's angle to the axis ('angular')\n        or their combination ('combined') should determine the word's importance for a topic.\n    random_state: int, default None\n        Random state to use so that results are exactly reproducible.\n    \"\"\"\n\n    def __init__(\n        self,\n        n_components: int = 10,\n        encoder: Union[\n            Encoder, str, MultimodalEncoder\n        ] = \"sentence-transformers/all-MiniLM-L6-v2\",\n        vectorizer: Optional[CountVectorizer] = None,\n        decomposition: Optional[TransformerMixin] = None,\n        max_iter: int = 200,\n        feature_importance: Literal[\n            \"axial\", \"angular\", \"combined\"\n        ] = \"combined\",\n        random_state: Optional[int] = None,\n    ):\n        self.n_components = n_components\n        self.encoder = encoder\n        self.feature_importance = feature_importance\n        if isinstance(encoder, str):\n            self.encoder_ = SentenceTransformer(encoder)\n        else:\n            self.encoder_ = encoder\n        self.validate_encoder()\n        if vectorizer is None:\n            self.vectorizer = default_vectorizer()\n        else:\n            self.vectorizer = vectorizer\n        self.max_iter = max_iter\n        self.random_state = random_state\n        if decomposition is None:\n            self.decomposition = FastICA(\n                n_components, max_iter=max_iter, random_state=random_state\n            )\n        else:\n            self.decomposition = decomposition\n\n    def estimate_components(\n        self, feature_importance: Literal[\"axial\", \"angular\", \"combined\"]\n    ) -&gt; np.ndarray:\n        \"\"\"Reestimates components based on the chosen feature_importance method.\"\"\"\n        if feature_importance == \"axial\":\n            self.components_ = self.axial_components_\n        elif feature_importance == \"angular\":\n            self.components_ = self.angular_components_\n        elif feature_importance == \"combined\":\n            self.components_ = (\n                np.square(self.axial_components_) * self.angular_components_\n            )\n        if hasattr(self, \"axial_temporal_components_\"):\n            if feature_importance == \"axial\":\n                self.temporal_components_ = self.axial_temporal_components_\n            elif feature_importance == \"angular\":\n                self.temporal_components_ = self.angular_temporal_components_\n            elif feature_importance == \"combined\":\n                self.temporal_components_ = (\n                    np.square(self.axial_temporal_components_)\n                    * self.angular_temporal_components_\n                )\n        return self.components_\n\n    @property\n    def has_negative_side(self) -&gt; bool:\n        return True\n\n    def fit_transform(\n        self, raw_documents, y=None, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        console = Console()\n        self.embeddings = embeddings\n        with console.status(\"Fitting model\") as status:\n            if self.embeddings is None:\n                status.update(\"Encoding documents\")\n                self.embeddings = self.encoder_.encode(raw_documents)\n                console.log(\"Documents encoded.\")\n            status.update(\"Decomposing embeddings\")\n            if isinstance(self.decomposition, FastICA) and (y is not None):\n                warnings.warn(\n                    \"y is specified but decomposition method is FastICA, which can't use labels. y will be ignored. Use a metric learning method for semi-supervised S^3.\"\n                )\n            doc_topic = self.decomposition.fit_transform(self.embeddings, y=y)\n            console.log(\"Decomposition done.\")\n            status.update(\"Extracting terms.\")\n            vocab = self.vectorizer.fit(raw_documents).get_feature_names_out()\n            console.log(\"Term extraction done.\")\n            status.update(\"Encoding vocabulary\")\n            self.vocab_embeddings = self.encoder_.encode(vocab)\n            if self.vocab_embeddings.shape[1] != self.embeddings.shape[1]:\n                raise ValueError(\n                    NOT_MATCHING_ERROR.format(\n                        n_dims=self.embeddings.shape[1],\n                        n_word_dims=self.vocab_embeddings.shape[1],\n                    )\n                )\n            console.log(\"Vocabulary encoded.\")\n            status.update(\"Estimating term importances\")\n            vocab_topic = self.decomposition.transform(self.vocab_embeddings)\n            self.axial_components_ = vocab_topic.T\n            if self.feature_importance == \"axial\":\n                self.components_ = self.axial_components_\n            elif self.feature_importance == \"angular\":\n                self.components_ = self.angular_components_\n            elif self.feature_importance == \"combined\":\n                self.components_ = (\n                    np.square(self.axial_components_)\n                    * self.angular_components_\n                )\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=doc_topic\n            )\n            self.negative_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=doc_topic, positive=False\n            )\n            self.document_topic_matrix = doc_topic\n            console.log(\"Model fitting done.\")\n        return doc_topic\n\n    def fit_transform_multimodal(\n        self,\n        raw_documents: list[str],\n        images: list[ImageRepr],\n        y=None,\n        embeddings: Optional[MultimodalEmbeddings] = None,\n    ) -&gt; np.ndarray:\n        self.validate_embeddings(embeddings)\n        console = Console()\n        self.images = images\n        self.multimodal_embeddings = embeddings\n        with console.status(\"Fitting model\") as status:\n            if self.multimodal_embeddings is None:\n                status.update(\"Encoding documents\")\n                self.multimodal_embeddings = self.encode_multimodal(\n                    raw_documents, images\n                )\n                console.log(\"Documents encoded.\")\n            self.embeddings = self.multimodal_embeddings[\"document_embeddings\"]\n            status.update(\"Decomposing embeddings\")\n            if isinstance(self.decomposition, FastICA) and (y is not None):\n                warnings.warn(\n                    \"Supervisory signal is specified but decomposition method is FastICA. y will be ignored. Use a metric learning method for supervised S^3.\"\n                )\n            doc_topic = self.decomposition.fit_transform(self.embeddings, y=y)\n            console.log(\"Decomposition done.\")\n            status.update(\"Extracting terms.\")\n            vocab = self.vectorizer.fit(raw_documents).get_feature_names_out()\n            console.log(\"Term extraction done.\")\n            status.update(\"Encoding vocabulary\")\n            self.vocab_embeddings = self.encode_documents(vocab)\n            if self.vocab_embeddings.shape[1] != self.embeddings.shape[1]:\n                raise ValueError(\n                    NOT_MATCHING_ERROR.format(\n                        n_dims=self.embeddings.shape[1],\n                        n_word_dims=self.vocab_embeddings.shape[1],\n                    )\n                )\n            console.log(\"Vocabulary encoded.\")\n            status.update(\"Estimating term importances\")\n            vocab_topic = self.decomposition.transform(self.vocab_embeddings)\n            self.axial_components_ = vocab_topic.T\n            if self.feature_importance == \"axial\":\n                self.components_ = self.axial_components_\n            elif self.feature_importance == \"angular\":\n                self.components_ = self.angular_components_\n            elif self.feature_importance == \"combined\":\n                self.components_ = (\n                    np.square(self.axial_components_)\n                    * self.angular_components_\n                )\n            console.log(\"Model fitting done.\")\n            status.update(\"Transforming images\")\n            self.image_topic_matrix = self.transform(\n                [], embeddings=self.multimodal_embeddings[\"image_embeddings\"]\n            )\n            self.top_images = self.collect_top_images(\n                images, self.image_topic_matrix\n            )\n            self.negative_images = self.collect_top_images(\n                images, self.image_topic_matrix, negative=True\n            )\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=doc_topic\n            )\n            self.negative_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=doc_topic, positive=False\n            )\n            console.log(\"Images transformed\")\n        return doc_topic\n\n    def plot_topics_with_images(self, n_columns: int = 3, grid_size: int = 4):\n        try:\n            import plotly.graph_objects as go\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n        fig = go.Figure()\n        width, height = 1200, 1200\n        scale_factor = 0.25\n        w, h = width * scale_factor, height * scale_factor\n        padding = 10\n        figure_height = (h + padding) * self.n_components\n        figure_width = (w + padding) * 2\n        fig = fig.add_trace(\n            go.Scatter(\n                x=[0, figure_width],\n                y=[0, figure_height],\n                mode=\"markers\",\n                marker_opacity=0,\n            )\n        )\n        vocab = self.get_vocab()\n        for i, component in enumerate(self.components_):\n            positive = vocab[np.argsort(-component)[:7]]\n            negative = vocab[np.argsort(component)[:7]]\n            pos_image = self._image_grid(\n                self.top_images[i],\n                (width, height),\n                grid_size=(grid_size, grid_size),\n            )\n            neg_image = self._image_grid(\n                self.negative_images[i],\n                (width, height),\n                grid_size=(grid_size, grid_size),\n            )\n            x0 = 0\n            y0 = (h + padding) * (self.n_components - i)\n            fig = fig.add_layout_image(\n                dict(\n                    x=x0,\n                    sizex=w,\n                    y=y0,\n                    sizey=h,\n                    xref=\"x\",\n                    yref=\"y\",\n                    opacity=1.0,\n                    layer=\"below\",\n                    sizing=\"stretch\",\n                    source=pos_image,\n                ),\n            )\n            fig.add_annotation(\n                x=(w / 2),\n                y=(h + padding) * (self.n_components - i) - (h / 2),\n                text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(positive),\n                font=dict(\n                    size=16,\n                    family=\"Times New Roman\",\n                    color=\"white\",\n                ),\n                bgcolor=\"rgba(0,0,255, 0.5)\",\n            )\n            x0 = (w + padding) * 1\n            fig = fig.add_layout_image(\n                dict(\n                    x=x0,\n                    sizex=w,\n                    y=y0,\n                    sizey=h,\n                    xref=\"x\",\n                    yref=\"y\",\n                    opacity=1.0,\n                    layer=\"below\",\n                    sizing=\"stretch\",\n                    source=neg_image,\n                ),\n            )\n            fig.add_annotation(\n                x=(w + padding) + (w / 2),\n                y=(h + padding) * (self.n_components - i) - (h / 2),\n                text=\"&lt;b&gt; \" + \"&lt;br&gt; \".join(negative),\n                font=dict(\n                    size=16,\n                    family=\"Times New Roman\",\n                    color=\"white\",\n                ),\n                bgcolor=\"rgba(255,0,0, 0.5)\",\n            )\n        fig = fig.update_xaxes(visible=False, range=[0, figure_width])\n        fig = fig.update_yaxes(\n            visible=False,\n            range=[0, figure_height],\n            # the scaleanchor attribute ensures that the aspect ratio stays constant\n            scaleanchor=\"x\",\n        )\n        fig = fig.update_layout(\n            width=figure_width,\n            height=figure_height,\n            margin={\"l\": 0, \"r\": 0, \"t\": 0, \"b\": 0},\n        )\n        return fig\n\n    def _rename_automatic(self, analyzer: Analyzer) -&gt; list[str]:\n        \"\"\"Names topics with a topic namer in the model.\n\n        Parameters\n        ----------\n        analyzer: Analyzer\n            A Topic namer model to name topics with.\n\n        Returns\n        -------\n        list[str]\n            List of topic names.\n        \"\"\"\n        try:\n            positive_documents = self.get_top_documents()\n            negative_documents = self.get_top_documents(positive=False)\n        except ValueError as e:\n            warnings.warn(\n                f\"Couldn't get top documents, proceeding only with keywords: {e}\"\n            )\n            positive_documents = None\n            negative_documents = None\n        positive_names = analyzer.name_topics(\n            self._top_terms(), documents=positive_documents\n        )\n        negative_names = analyzer.name_topics(\n            self._top_terms(positive=False), documents=negative_documents\n        )\n        names = []\n        for positive, negative in zip(positive_names, negative_names):\n            names.append(f\"{positive}/{negative}\")\n        self.topic_names_ = names\n        return self.topic_names_\n\n    def refit_transform(\n        self,\n        raw_documents,\n        y=None,\n        embeddings: Optional[np.ndarray] = None,\n        n_components: Optional[int] = None,\n        max_iter: Optional[int] = None,\n        random_state: Optional[int] = None,\n    ):\n        \"\"\"Refits model with the given parameters.\n        This is significantly faster than fitting a new model from scratch.\n\n        Parameters\n        ----------\n        raw_documents\n            Corpus on which the model is based.\n        y\n            Ignored, exists for API compatibility.\n        embeddings\n            Ignored, embeddings are already stored, exists for compatibility.\n        n_components: int, default None\n            Number of topics.\n        max_iter: int, default None\n            Maximum number of iterations for ICA.\n        random_state: int, default None\n            Random state to use so that results are exactly reproducible.\n\n        Returns\n        -------\n        ndarray of shape (n_documents, n_topics)\n            Document-topic matrix.\n        \"\"\"\n        self.n_components = n_components\n        self.topic_names_ = None\n        n_components = (\n            n_components if n_components is not None else self.n_components\n        )\n        max_iter = max_iter if max_iter is not None else self.max_iter\n        random_state = (\n            random_state if random_state is not None else self.random_state\n        )\n        self.decomposition = FastICA(\n            n_components, max_iter=max_iter, random_state=random_state\n        )\n        console = Console()\n        with console.status(\"Refitting model\") as status:\n            status.update(\"Decomposing embeddings\")\n            doc_topic = self.decomposition.fit_transform(self.embeddings)\n            console.log(\"Decomposition done.\")\n            status.update(\"Estimating term importances\")\n            vocab_topic = self.decomposition.transform(self.vocab_embeddings)\n            self.axial_components_ = vocab_topic.T\n            if self.feature_importance == \"axial\":\n                self.components_ = self.axial_components_\n            elif self.feature_importance == \"angular\":\n                self.components_ = self.angular_components_\n            elif self.feature_importance == \"combined\":\n                self.components_ = (\n                    np.square(self.axial_components_)\n                    * self.angular_components_\n                )\n            self.top_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=doc_topic\n            )\n            self.negative_documents = self.get_top_documents(\n                raw_documents, document_topic_matrix=doc_topic, positive=False\n            )\n            console.log(\"Model fitting done.\")\n        return doc_topic\n\n    def fit_transform_dynamic(\n        self,\n        raw_documents,\n        timestamps: list[datetime],\n        embeddings: Optional[np.ndarray] = None,\n        bins: Union[int, list[datetime]] = 10,\n    ) -&gt; np.ndarray:\n        document_topic_matrix = self.fit_transform(\n            raw_documents, embeddings=embeddings\n        )\n        time_labels, self.time_bin_edges = self.bin_timestamps(\n            timestamps, bins\n        )\n        n_comp, n_vocab = self.components_.shape\n        n_bins = len(self.time_bin_edges) - 1\n        self.axial_temporal_components_ = np.full(\n            (n_bins, n_comp, n_vocab),\n            np.nan,\n            dtype=self.components_.dtype,\n        )\n        self.temporal_importance_ = np.zeros((n_bins, n_comp))\n        whitened_embeddings = np.copy(self.embeddings)\n        if getattr(self.decomposition, \"whiten\", False):\n            whitened_embeddings -= self.decomposition.mean_\n        # doc_topic = np.dot(X, self.components_.T)\n        for i_timebin in np.unique(time_labels):\n            topic_importances = document_topic_matrix[\n                time_labels == i_timebin\n            ].mean(axis=0)\n            self.temporal_importance_[i_timebin, :] = topic_importances\n            t_doc_topic = document_topic_matrix[time_labels == i_timebin]\n            t_embeddings = whitened_embeddings[time_labels == i_timebin]\n            linreg = LinearRegression().fit(t_embeddings, t_doc_topic)\n            self.axial_temporal_components_[i_timebin, :, :] = np.dot(\n                self.vocab_embeddings, linreg.coef_.T\n            ).T\n        self.estimate_components(self.feature_importance)\n        return document_topic_matrix\n\n    def refit_transform_dynamic(\n        self,\n        raw_documents,\n        timestamps: list[datetime],\n        embeddings=None,\n        bins: Union[int, list[datetime]] = 10,\n        n_components: Optional[int] = None,\n        max_iter: Optional[int] = None,\n        random_state: Optional[int] = None,\n    ):\n        \"\"\"Refits $S^3$ to be a dynamic model.\"\"\"\n        document_topic_matrix = self.refit_transform(\n            raw_documents,\n            embeddings=embeddings,\n            n_components=n_components,\n            max_iter=max_iter,\n            random_state=random_state,\n        )\n        time_labels, self.time_bin_edges = self.bin_timestamps(\n            timestamps, bins\n        )\n        n_comp, n_vocab = self.components_.shape\n        n_bins = len(self.time_bin_edges) - 1\n        self.axial_temporal_components_ = np.full(\n            (n_bins, n_comp, n_vocab),\n            np.nan,\n            dtype=self.components_.dtype,\n        )\n        self.temporal_importance_ = np.zeros((n_bins, n_comp))\n        whitened_embeddings = np.copy(self.embeddings)\n        if getattr(self.decomposition, \"whiten\"):\n            whitened_embeddings -= self.decomposition.mean_\n        # doc_topic = np.dot(X, self.components_.T)\n        for i_timebin in np.unique(time_labels):\n            topic_importances = document_topic_matrix[\n                time_labels == i_timebin\n            ].mean(axis=0)\n            self.temporal_importance_[i_timebin, :] = topic_importances\n            t_doc_topic = document_topic_matrix[time_labels == i_timebin]\n            t_embeddings = whitened_embeddings[time_labels == i_timebin]\n            linreg = LinearRegression().fit(t_embeddings, t_doc_topic)\n            self.axial_temporal_components_[i_timebin, :, :] = np.dot(\n                self.vocab_embeddings, linreg.coef_.T\n            ).T\n        self.estimate_components(self.feature_importance)\n        return document_topic_matrix\n\n    def refit(\n        self,\n        raw_documents,\n        y=None,\n        embeddings=None,\n        n_components: Optional[int] = None,\n        max_iter: Optional[int] = None,\n        random_state: Optional[int] = None,\n    ):\n        \"\"\"Refits model with the given parameters.\n        This is significantly faster than fitting a new model from scratch.\n\n        Parameters\n        ----------\n        raw_documents\n            Corpus on which the model is based.\n        y\n            Ignored, exists for API compatibility.\n        embeddings\n            Ignored, embeddings are already stored, exists for compatibility.\n        n_components: int, default None\n            Number of topics.\n        max_iter: int, default None\n            Maximum number of iterations for ICA.\n        random_state: int, default None\n            Random state to use so that results are exactly reproducible.\n\n        Returns\n        -------\n        Refitted model.\n        \"\"\"\n        self.refit_transform(\n            raw_documents,\n            y=y,\n            embeddings=embeddings,\n            n_components=n_components,\n            max_iter=max_iter,\n            random_state=random_state,\n        )\n        return self\n\n    @property\n    def angular_components_(self):\n        \"\"\"Reweights words based on their angle in ICA-space to the axis\n        base vectors.\n        \"\"\"\n        if not hasattr(self, \"axial_components_\"):\n            raise NotFittedError(\"Model has not been fitted yet.\")\n        word_vectors = self.axial_components_.T\n        n_topics = self.axial_components_.shape[0]\n        axis_vectors = np.eye(n_topics)\n        cosine_components = cosine_similarity(axis_vectors, word_vectors)\n        return cosine_components\n\n    @property\n    def angular_temporal_components_(self):\n        \"\"\"Reweights words based on their angle in ICA-space to the axis\n        base vectors in a dynamic model.\n        \"\"\"\n        if not hasattr(self, \"axial_temporal_components_\"):\n            raise NotFittedError(\"Model has not been fitted dynamically.\")\n        components = []\n        for axial_components in self.axial_temporal_components_:\n            word_vectors = axial_components.T\n            n_topics = axial_components.shape[0]\n            axis_vectors = np.eye(n_topics)\n            cosine_components = cosine_similarity(axis_vectors, word_vectors)\n            components.append(cosine_components)\n        return np.stack(components)\n\n    def transform(\n        self, raw_documents, embeddings: Optional[np.ndarray] = None\n    ) -&gt; np.ndarray:\n        \"\"\"Infers topic importances for new documents based on a fitted model.\n\n        Parameters\n        ----------\n        raw_documents: iterable of str\n            Documents to fit the model on.\n        embeddings: ndarray of shape (n_documents, n_dimensions), optional\n            Precomputed document encodings.\n\n        Returns\n        -------\n        ndarray of shape (n_dimensions, n_topics)\n            Document-topic matrix.\n        \"\"\"\n        if embeddings is None:\n            embeddings = self.encoder_.encode(raw_documents)\n        return self.decomposition.transform(embeddings)\n\n    def print_topics(\n        self,\n        top_k: int = 10,\n        show_scores: bool = False,\n        show_negative: bool = False,\n    ):\n        show_negative = self.has_negative_side\n        super().print_topics(top_k, show_scores, show_negative)\n\n    def export_topics(\n        self,\n        top_k: int = 10,\n        show_scores: bool = False,\n        show_negative: bool = True,\n        format: str = \"csv\",\n    ) -&gt; str:\n        show_negative = self.has_negative_side\n        return super().export_topics(top_k, show_scores, show_negative, format)\n\n    def print_representative_documents(\n        self,\n        topic_id,\n        raw_documents,\n        document_topic_matrix=None,\n        top_k=10,\n        show_negative: bool = False,\n    ):\n        show_negative = self.has_negative_side\n        super().print_representative_documents(\n            topic_id,\n            raw_documents,\n            document_topic_matrix,\n            top_k,\n            show_negative,\n        )\n\n    def export_representative_documents(\n        self,\n        topic_id,\n        raw_documents,\n        document_topic_matrix=None,\n        top_k=10,\n        show_negative: bool = False,\n        format: str = \"csv\",\n    ):\n        show_negative = self.has_negative_side\n        return super().export_representative_documents(\n            topic_id,\n            raw_documents,\n            document_topic_matrix,\n            top_k,\n            show_negative,\n            format,\n        )\n\n    def concept_compass(\n        self, topic_x: Union[int, str], topic_y: Union[str, int]\n    ):\n        \"\"\"[DEPRECATED] will be removed in version 1.0.0.\n        See plot_concept_compass().\n        \"\"\"\n        warnings.warn(\n            \"concept_compass() is deprecated and will be removed in version 1.0.0. Use plot_concept_compass() instead.\"\n        )\n        return self.plot_concept_compass(topic_x, topic_y)\n\n    def plot_concept_compass(\n        self, topic_x: Union[int, str], topic_y: Union[str, int]\n    ):\n        \"\"\"Display a compass of concepts along two semantic axes.\n        In order for the plot to be concise and readable, terms are randomly selected on\n        a grid of the two topics.\n\n        Parameters\n        ----------\n        topic_x: int or str\n            Index or name of the topic to display on the X axis.\n        topic_y: int or str\n            Index or name of the topic to display on the Y axis.\n\n        Returns\n        -------\n        go.Figure\n            Plotly interactive plot of the concept compass.\n        \"\"\"\n        try:\n            import plotly.express as px\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n        if isinstance(topic_x, str):\n            try:\n                topic_x = list(self.topic_names).index(topic_x)\n            except ValueError as e:\n                raise ValueError(\n                    f\"{topic_x} is not a valid topic name or index.\"\n                ) from e\n        if isinstance(topic_y, str):\n            try:\n                topic_y = list(self.topic_names).index(topic_y)\n            except ValueError as e:\n                raise ValueError(\n                    f\"{topic_y} is not a valid topic name or index.\"\n                ) from e\n        x = self.axial_components_[topic_x]\n        y = self.axial_components_[topic_y]\n        vocab = self.get_vocab()\n        points = np.array(list(zip(x, y)))\n        xx, yy = np.meshgrid(\n            np.linspace(np.min(x), np.max(x), 20),\n            np.linspace(np.min(y), np.max(y), 20),\n        )\n        coords = np.array(list(zip(np.ravel(xx), np.ravel(yy))))\n        coords = coords + np.random.default_rng(0).normal(\n            [0, 0], [0.1, 0.1], size=coords.shape\n        )\n        dist = euclidean_distances(coords, points)\n        idxs = np.argmin(dist, axis=1)\n        fig = px.scatter(\n            x=x[idxs],\n            y=y[idxs],\n            text=vocab[idxs],\n            template=\"plotly_white\",\n        )\n        fig = fig.update_traces(\n            mode=\"text\", textfont_color=\"black\", marker=dict(color=\"black\")\n        ).update_layout(\n            xaxis_title=f\"{self.topic_names[topic_x]}\",\n            yaxis_title=f\"{self.topic_names[topic_y]}\",\n            font=dict(family=\"Roboto Mono\"),\n        )\n        fig = fig.update_layout(\n            font=dict(family=\"Roboto Mono\", color=\"black\", size=21),\n            margin=dict(l=5, r=5, t=5, b=5),\n        )\n        fig = fig.add_hline(y=0, line_color=\"black\", line_width=4)\n        fig = fig.add_vline(x=0, line_color=\"black\", line_width=4)\n        return fig\n\n    def plot_image_compass(\n        self, topic_x: Union[str, int], topic_y: Union[str, int]\n    ):\n        try:\n            import plotly.express as px\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n        top_images = getattr(self, \"top_images\", None)\n        if top_images is None:\n            raise ValueError(\n                \"Topic model is not multimodal. Can't plot image compass.\"\n            )\n        if isinstance(topic_x, str):\n            try:\n                topic_x = list(self.topic_names).index(topic_x)\n            except ValueError as e:\n                raise ValueError(\n                    f\"{topic_x} is not a valid topic name or index.\"\n                ) from e\n        if isinstance(topic_y, str):\n            try:\n                topic_y = list(self.topic_names).index(topic_y)\n            except ValueError as e:\n                raise ValueError(\n                    f\"{topic_y} is not a valid topic name or index.\"\n                ) from e\n        x = self.image_topic_matrix[:, topic_x]\n        y = self.image_topic_matrix[:, topic_y]\n        points = np.array(list(zip(x, y)))\n        xx, yy = np.meshgrid(\n            np.linspace(np.min(x), np.max(x), 8),\n            np.linspace(np.min(y), np.max(y), 8),\n        )\n        coords = np.array(list(zip(np.ravel(xx), np.ravel(yy))))\n        dist = euclidean_distances(coords, points)\n        idxs = np.argmin(dist, axis=1)\n        fig = px.scatter(\n            x=x[idxs],\n            y=y[idxs],\n            template=\"plotly_white\",\n        )\n        sizex = (max(x) - min(x)) / 10\n        sizey = (max(y) - min(y)) / 10\n        for i in np.unique(idxs):\n            fig.add_layout_image(\n                dict(\n                    name=f\"image{i}\",\n                    source=self.images[i],\n                    x=x[i],\n                    y=y[i],\n                    xref=\"x\",\n                    yref=\"y\",\n                    xanchor=\"right\",\n                    yanchor=\"top\",\n                    layer=\"above\",\n                    sizex=sizex,\n                    sizey=sizey,\n                )\n            )\n        fig = fig.update_traces(\n            mode=\"markers\", textfont_color=\"black\", marker=dict(opacity=0)\n        ).update_layout(\n            xaxis_title=f\"{self.topic_names[topic_x]}\",\n            yaxis_title=f\"{self.topic_names[topic_y]}\",\n            font=dict(family=\"Roboto Mono\"),\n        )\n        fig = fig.update_layout(\n            font=dict(family=\"Roboto Mono\", color=\"black\", size=21),\n            margin=dict(l=5, r=5, t=5, b=5),\n        )\n        fig = fig.update_xaxes(range=[min(x), max(x)])\n        fig = fig.update_yaxes(range=[min(y), max(y)])\n        return fig\n\n    def plot_topics_over_time(self, top_k: int = 6):\n        try:\n            import plotly.graph_objects as go\n        except (ImportError, ModuleNotFoundError) as e:\n            raise ModuleNotFoundError(\n                \"Please install plotly if you intend to use plots in Turftopic.\"\n            ) from e\n        fig = go.Figure()\n        vocab = self.get_vocab()\n        n_topics = self.temporal_components_.shape[1]\n        try:\n            topic_names = self.topic_names\n        except AttributeError:\n            topic_names = [f\"Topic {i}\" for i in range(n_topics)]\n        for i_topic, topic_imp_t in enumerate(self.temporal_importance_.T):\n            component_over_time = self.temporal_components_[:, i_topic, :]\n            name_over_time = []\n            for component, importance in zip(component_over_time, topic_imp_t):\n                if importance &lt; 0:\n                    component = -component\n                top = np.argpartition(-component, top_k)[:top_k]\n                values = component[top]\n                if np.all(values == 0) or np.all(np.isnan(values)):\n                    name_over_time.append(\"&lt;not present&gt;\")\n                    continue\n                top = top[np.argsort(-values)]\n                name_over_time.append(\", \".join(vocab[top]))\n            times = self.time_bin_edges[:-1]\n            fig.add_trace(\n                go.Scatter(\n                    x=times,\n                    y=topic_imp_t,\n                    mode=\"markers+lines\",\n                    text=name_over_time,\n                    name=topic_names[i_topic],\n                    hovertemplate=\"&lt;b&gt;%{text}&lt;/b&gt;\",\n                    marker=dict(\n                        line=dict(width=2, color=\"black\"),\n                        size=14,\n                    ),\n                    line=dict(width=3),\n                )\n            )\n        fig.add_hline(y=0, line_dash=\"dash\", opacity=0.5)\n        fig.update_layout(\n            template=\"plotly_white\",\n            hoverlabel=dict(font_size=16, bgcolor=\"white\"),\n            hovermode=\"x\",\n            font=dict(family=\"Roboto Mono\"),\n        )\n        fig.update_xaxes(title=\"Time Slice Start\")\n        fig.update_yaxes(title=\"Topic Importance\")\n        return fig\n\n    def _topics_over_time(\n        self,\n        top_k: int = 5,\n        show_scores: bool = False,\n        date_format: str = \"%Y %m %d\",\n    ) -&gt; list[list[str]]:\n        temporal_components = self.temporal_components_\n        slices = self.get_time_slices()\n        slice_names = []\n        for start_dt, end_dt in slices:\n            start_str = start_dt.strftime(date_format)\n            end_str = end_dt.strftime(date_format)\n            slice_names.append(f\"{start_str} - {end_str}\")\n        n_topics = self.temporal_components_.shape[1]\n        try:\n            topic_names = self.topic_names\n        except AttributeError:\n            topic_names = [f\"Topic {i}\" for i in range(n_topics)]\n        columns = []\n        rows = []\n        columns.append(\"Time Slice\")\n        for topic in topic_names:\n            columns.append(topic)\n        for slice_name, components, weights in zip(\n            slice_names, temporal_components, self.temporal_importance_\n        ):\n            fields = []\n            fields.append(slice_name)\n            vocab = self.get_vocab()\n            for component, weight in zip(components, weights):\n                if np.all(component == 0) or np.all(np.isnan(component)):\n                    fields.append(\"Topic not present.\")\n                    continue\n                if weight &lt; 0:\n                    component = -component\n                top = np.argpartition(-component, top_k)[:top_k]\n                importance = component[top]\n                top = top[np.argsort(-importance)]\n                top = top[importance != 0]\n                scores = component[top]\n                words = vocab[top]\n                if show_scores:\n                    concat_words = \", \".join(\n                        [\n                            f\"{word}({importance:.2f})\"\n                            for word, importance in zip(words, scores)\n                        ]\n                    )\n                else:\n                    concat_words = \", \".join([word for word in words])\n                fields.append(concat_words)\n            rows.append(fields)\n        return [columns, *rows]\n</code></pre>"},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation.angular_components_","title":"<code>angular_components_</code>  <code>property</code>","text":"<p>Reweights words based on their angle in ICA-space to the axis base vectors.</p>"},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation.angular_temporal_components_","title":"<code>angular_temporal_components_</code>  <code>property</code>","text":"<p>Reweights words based on their angle in ICA-space to the axis base vectors in a dynamic model.</p>"},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation.concept_compass","title":"<code>concept_compass(topic_x, topic_y)</code>","text":"<p>[DEPRECATED] will be removed in version 1.0.0. See plot_concept_compass().</p> Source code in <code>turftopic/models/decomp.py</code> <pre><code>def concept_compass(\n    self, topic_x: Union[int, str], topic_y: Union[str, int]\n):\n    \"\"\"[DEPRECATED] will be removed in version 1.0.0.\n    See plot_concept_compass().\n    \"\"\"\n    warnings.warn(\n        \"concept_compass() is deprecated and will be removed in version 1.0.0. Use plot_concept_compass() instead.\"\n    )\n    return self.plot_concept_compass(topic_x, topic_y)\n</code></pre>"},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation.estimate_components","title":"<code>estimate_components(feature_importance)</code>","text":"<p>Reestimates components based on the chosen feature_importance method.</p> Source code in <code>turftopic/models/decomp.py</code> <pre><code>def estimate_components(\n    self, feature_importance: Literal[\"axial\", \"angular\", \"combined\"]\n) -&gt; np.ndarray:\n    \"\"\"Reestimates components based on the chosen feature_importance method.\"\"\"\n    if feature_importance == \"axial\":\n        self.components_ = self.axial_components_\n    elif feature_importance == \"angular\":\n        self.components_ = self.angular_components_\n    elif feature_importance == \"combined\":\n        self.components_ = (\n            np.square(self.axial_components_) * self.angular_components_\n        )\n    if hasattr(self, \"axial_temporal_components_\"):\n        if feature_importance == \"axial\":\n            self.temporal_components_ = self.axial_temporal_components_\n        elif feature_importance == \"angular\":\n            self.temporal_components_ = self.angular_temporal_components_\n        elif feature_importance == \"combined\":\n            self.temporal_components_ = (\n                np.square(self.axial_temporal_components_)\n                * self.angular_temporal_components_\n            )\n    return self.components_\n</code></pre>"},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation.plot_concept_compass","title":"<code>plot_concept_compass(topic_x, topic_y)</code>","text":"<p>Display a compass of concepts along two semantic axes. In order for the plot to be concise and readable, terms are randomly selected on a grid of the two topics.</p> <p>Parameters:</p> Name Type Description Default <code>topic_x</code> <code>Union[int, str]</code> <p>Index or name of the topic to display on the X axis.</p> required <code>topic_y</code> <code>Union[str, int]</code> <p>Index or name of the topic to display on the Y axis.</p> required <p>Returns:</p> Type Description <code>Figure</code> <p>Plotly interactive plot of the concept compass.</p> Source code in <code>turftopic/models/decomp.py</code> <pre><code>def plot_concept_compass(\n    self, topic_x: Union[int, str], topic_y: Union[str, int]\n):\n    \"\"\"Display a compass of concepts along two semantic axes.\n    In order for the plot to be concise and readable, terms are randomly selected on\n    a grid of the two topics.\n\n    Parameters\n    ----------\n    topic_x: int or str\n        Index or name of the topic to display on the X axis.\n    topic_y: int or str\n        Index or name of the topic to display on the Y axis.\n\n    Returns\n    -------\n    go.Figure\n        Plotly interactive plot of the concept compass.\n    \"\"\"\n    try:\n        import plotly.express as px\n    except (ImportError, ModuleNotFoundError) as e:\n        raise ModuleNotFoundError(\n            \"Please install plotly if you intend to use plots in Turftopic.\"\n        ) from e\n    if isinstance(topic_x, str):\n        try:\n            topic_x = list(self.topic_names).index(topic_x)\n        except ValueError as e:\n            raise ValueError(\n                f\"{topic_x} is not a valid topic name or index.\"\n            ) from e\n    if isinstance(topic_y, str):\n        try:\n            topic_y = list(self.topic_names).index(topic_y)\n        except ValueError as e:\n            raise ValueError(\n                f\"{topic_y} is not a valid topic name or index.\"\n            ) from e\n    x = self.axial_components_[topic_x]\n    y = self.axial_components_[topic_y]\n    vocab = self.get_vocab()\n    points = np.array(list(zip(x, y)))\n    xx, yy = np.meshgrid(\n        np.linspace(np.min(x), np.max(x), 20),\n        np.linspace(np.min(y), np.max(y), 20),\n    )\n    coords = np.array(list(zip(np.ravel(xx), np.ravel(yy))))\n    coords = coords + np.random.default_rng(0).normal(\n        [0, 0], [0.1, 0.1], size=coords.shape\n    )\n    dist = euclidean_distances(coords, points)\n    idxs = np.argmin(dist, axis=1)\n    fig = px.scatter(\n        x=x[idxs],\n        y=y[idxs],\n        text=vocab[idxs],\n        template=\"plotly_white\",\n    )\n    fig = fig.update_traces(\n        mode=\"text\", textfont_color=\"black\", marker=dict(color=\"black\")\n    ).update_layout(\n        xaxis_title=f\"{self.topic_names[topic_x]}\",\n        yaxis_title=f\"{self.topic_names[topic_y]}\",\n        font=dict(family=\"Roboto Mono\"),\n    )\n    fig = fig.update_layout(\n        font=dict(family=\"Roboto Mono\", color=\"black\", size=21),\n        margin=dict(l=5, r=5, t=5, b=5),\n    )\n    fig = fig.add_hline(y=0, line_color=\"black\", line_width=4)\n    fig = fig.add_vline(x=0, line_color=\"black\", line_width=4)\n    return fig\n</code></pre>"},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation.refit","title":"<code>refit(raw_documents, y=None, embeddings=None, n_components=None, max_iter=None, random_state=None)</code>","text":"<p>Refits model with the given parameters. This is significantly faster than fitting a new model from scratch.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Corpus on which the model is based.</p> required <code>y</code> <p>Ignored, exists for API compatibility.</p> <code>None</code> <code>embeddings</code> <p>Ignored, embeddings are already stored, exists for compatibility.</p> <code>None</code> <code>n_components</code> <code>Optional[int]</code> <p>Number of topics.</p> <code>None</code> <code>max_iter</code> <code>Optional[int]</code> <p>Maximum number of iterations for ICA.</p> <code>None</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state to use so that results are exactly reproducible.</p> <code>None</code> <p>Returns:</p> Type Description <code>Refitted model.</code> Source code in <code>turftopic/models/decomp.py</code> <pre><code>def refit(\n    self,\n    raw_documents,\n    y=None,\n    embeddings=None,\n    n_components: Optional[int] = None,\n    max_iter: Optional[int] = None,\n    random_state: Optional[int] = None,\n):\n    \"\"\"Refits model with the given parameters.\n    This is significantly faster than fitting a new model from scratch.\n\n    Parameters\n    ----------\n    raw_documents\n        Corpus on which the model is based.\n    y\n        Ignored, exists for API compatibility.\n    embeddings\n        Ignored, embeddings are already stored, exists for compatibility.\n    n_components: int, default None\n        Number of topics.\n    max_iter: int, default None\n        Maximum number of iterations for ICA.\n    random_state: int, default None\n        Random state to use so that results are exactly reproducible.\n\n    Returns\n    -------\n    Refitted model.\n    \"\"\"\n    self.refit_transform(\n        raw_documents,\n        y=y,\n        embeddings=embeddings,\n        n_components=n_components,\n        max_iter=max_iter,\n        random_state=random_state,\n    )\n    return self\n</code></pre>"},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation.refit_transform","title":"<code>refit_transform(raw_documents, y=None, embeddings=None, n_components=None, max_iter=None, random_state=None)</code>","text":"<p>Refits model with the given parameters. This is significantly faster than fitting a new model from scratch.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Corpus on which the model is based.</p> required <code>y</code> <p>Ignored, exists for API compatibility.</p> <code>None</code> <code>embeddings</code> <code>Optional[ndarray]</code> <p>Ignored, embeddings are already stored, exists for compatibility.</p> <code>None</code> <code>n_components</code> <code>Optional[int]</code> <p>Number of topics.</p> <code>None</code> <code>max_iter</code> <code>Optional[int]</code> <p>Maximum number of iterations for ICA.</p> <code>None</code> <code>random_state</code> <code>Optional[int]</code> <p>Random state to use so that results are exactly reproducible.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_documents, n_topics)</code> <p>Document-topic matrix.</p> Source code in <code>turftopic/models/decomp.py</code> <pre><code>def refit_transform(\n    self,\n    raw_documents,\n    y=None,\n    embeddings: Optional[np.ndarray] = None,\n    n_components: Optional[int] = None,\n    max_iter: Optional[int] = None,\n    random_state: Optional[int] = None,\n):\n    \"\"\"Refits model with the given parameters.\n    This is significantly faster than fitting a new model from scratch.\n\n    Parameters\n    ----------\n    raw_documents\n        Corpus on which the model is based.\n    y\n        Ignored, exists for API compatibility.\n    embeddings\n        Ignored, embeddings are already stored, exists for compatibility.\n    n_components: int, default None\n        Number of topics.\n    max_iter: int, default None\n        Maximum number of iterations for ICA.\n    random_state: int, default None\n        Random state to use so that results are exactly reproducible.\n\n    Returns\n    -------\n    ndarray of shape (n_documents, n_topics)\n        Document-topic matrix.\n    \"\"\"\n    self.n_components = n_components\n    self.topic_names_ = None\n    n_components = (\n        n_components if n_components is not None else self.n_components\n    )\n    max_iter = max_iter if max_iter is not None else self.max_iter\n    random_state = (\n        random_state if random_state is not None else self.random_state\n    )\n    self.decomposition = FastICA(\n        n_components, max_iter=max_iter, random_state=random_state\n    )\n    console = Console()\n    with console.status(\"Refitting model\") as status:\n        status.update(\"Decomposing embeddings\")\n        doc_topic = self.decomposition.fit_transform(self.embeddings)\n        console.log(\"Decomposition done.\")\n        status.update(\"Estimating term importances\")\n        vocab_topic = self.decomposition.transform(self.vocab_embeddings)\n        self.axial_components_ = vocab_topic.T\n        if self.feature_importance == \"axial\":\n            self.components_ = self.axial_components_\n        elif self.feature_importance == \"angular\":\n            self.components_ = self.angular_components_\n        elif self.feature_importance == \"combined\":\n            self.components_ = (\n                np.square(self.axial_components_)\n                * self.angular_components_\n            )\n        self.top_documents = self.get_top_documents(\n            raw_documents, document_topic_matrix=doc_topic\n        )\n        self.negative_documents = self.get_top_documents(\n            raw_documents, document_topic_matrix=doc_topic, positive=False\n        )\n        console.log(\"Model fitting done.\")\n    return doc_topic\n</code></pre>"},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation.refit_transform_dynamic","title":"<code>refit_transform_dynamic(raw_documents, timestamps, embeddings=None, bins=10, n_components=None, max_iter=None, random_state=None)</code>","text":"<p>Refits \\(S^3\\) to be a dynamic model.</p> Source code in <code>turftopic/models/decomp.py</code> <pre><code>def refit_transform_dynamic(\n    self,\n    raw_documents,\n    timestamps: list[datetime],\n    embeddings=None,\n    bins: Union[int, list[datetime]] = 10,\n    n_components: Optional[int] = None,\n    max_iter: Optional[int] = None,\n    random_state: Optional[int] = None,\n):\n    \"\"\"Refits $S^3$ to be a dynamic model.\"\"\"\n    document_topic_matrix = self.refit_transform(\n        raw_documents,\n        embeddings=embeddings,\n        n_components=n_components,\n        max_iter=max_iter,\n        random_state=random_state,\n    )\n    time_labels, self.time_bin_edges = self.bin_timestamps(\n        timestamps, bins\n    )\n    n_comp, n_vocab = self.components_.shape\n    n_bins = len(self.time_bin_edges) - 1\n    self.axial_temporal_components_ = np.full(\n        (n_bins, n_comp, n_vocab),\n        np.nan,\n        dtype=self.components_.dtype,\n    )\n    self.temporal_importance_ = np.zeros((n_bins, n_comp))\n    whitened_embeddings = np.copy(self.embeddings)\n    if getattr(self.decomposition, \"whiten\"):\n        whitened_embeddings -= self.decomposition.mean_\n    # doc_topic = np.dot(X, self.components_.T)\n    for i_timebin in np.unique(time_labels):\n        topic_importances = document_topic_matrix[\n            time_labels == i_timebin\n        ].mean(axis=0)\n        self.temporal_importance_[i_timebin, :] = topic_importances\n        t_doc_topic = document_topic_matrix[time_labels == i_timebin]\n        t_embeddings = whitened_embeddings[time_labels == i_timebin]\n        linreg = LinearRegression().fit(t_embeddings, t_doc_topic)\n        self.axial_temporal_components_[i_timebin, :, :] = np.dot(\n            self.vocab_embeddings, linreg.coef_.T\n        ).T\n    self.estimate_components(self.feature_importance)\n    return document_topic_matrix\n</code></pre>"},{"location":"s3/#turftopic.models.decomp.SemanticSignalSeparation.transform","title":"<code>transform(raw_documents, embeddings=None)</code>","text":"<p>Infers topic importances for new documents based on a fitted model.</p> <p>Parameters:</p> Name Type Description Default <code>raw_documents</code> <p>Documents to fit the model on.</p> required <code>embeddings</code> <code>Optional[ndarray]</code> <p>Precomputed document encodings.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray of shape (n_dimensions, n_topics)</code> <p>Document-topic matrix.</p> Source code in <code>turftopic/models/decomp.py</code> <pre><code>def transform(\n    self, raw_documents, embeddings: Optional[np.ndarray] = None\n) -&gt; np.ndarray:\n    \"\"\"Infers topic importances for new documents based on a fitted model.\n\n    Parameters\n    ----------\n    raw_documents: iterable of str\n        Documents to fit the model on.\n    embeddings: ndarray of shape (n_documents, n_dimensions), optional\n        Precomputed document encodings.\n\n    Returns\n    -------\n    ndarray of shape (n_dimensions, n_topics)\n        Document-topic matrix.\n    \"\"\"\n    if embeddings is None:\n        embeddings = self.encoder_.encode(raw_documents)\n    return self.decomposition.transform(embeddings)\n</code></pre>"},{"location":"seeded/","title":"Seeded Topic Modeling","text":"<p>When investigating a set of documents, you might already have an idea about what aspects you would like to explore. Some models are able to account for this by taking seed phrases or words. This is currently only possible with KeyNMF in Turftopic, but will likely be extended in the future.</p> <p>In KeyNMF, you can describe the aspect, from which you want to investigate your corpus, using a free-text seed-phrase, which will then be used to only extract topics, which are relevant to your research question.</p> <p>In this example we investigate the 20Newsgroups corpus from three different aspects:</p> <pre><code>from sklearn.datasets import fetch_20newsgroups\n\nfrom turftopic import KeyNMF\n\ncorpus = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n).data\n\nmodel = KeyNMF(5, seed_phrase=\"&lt;your seed phrase&gt;\")\nmodel.fit(corpus)\n\nmodel.print_topics()\n</code></pre> <code>'Is the death penalty moral?'</code><code>'Evidence for the existence of god'</code><code>'Operating system kernels'</code> Topic ID Highest Ranking 0 morality, moral, immoral, morals, objective, morally, animals, society, species, behavior 1 armenian, armenians, genocide, armenia, turkish, turks, soviet, massacre, azerbaijan, kurdish 2 murder, punishment, death, innocent, penalty, kill, crime, moral, criminals, executed 3 gun, guns, firearms, crime, handgun, firearm, weapons, handguns, law, criminals 4 jews, israeli, israel, god, jewish, christians, sin, christian, palestinians, christianity Topic ID Highest Ranking 0 atheist, atheists, religion, religious, theists, beliefs, christianity, christian, religions, agnostic 1 bible, christians, christian, christianity, church, scripture, religion, jesus, faith, biblical 2 god, existence, exist, exists, universe, creation, argument, creator, believe, life 3 believe, faith, belief, evidence, blindly, believing, gods, believed, beliefs, convince 4 atheism, atheists, agnosticism, belief, arguments, believe, existence, alt, believing, argument Topic ID Highest Ranking 0 windows, dos, os, microsoft, ms, apps, pc, nt, file, shareware 1 ram, motherboard, card, monitor, memory, cpu, vga, mhz, bios, intel 2 unix, os, linux, intel, systems, programming, applications, compiler, software, platform 3 disk, scsi, disks, drive, floppy, drives, dos, controller, cd, boot 4 software, mac, hardware, ibm, graphics, apple, computer, pc, modem, program"},{"location":"topic_data/","title":"<code>TopicData</code>","text":"<p>While Turftopic provides a fully sklearn-compatible interface for training and using topic models, this is not always optimal, especially when you have to visualize models, or save more information about inference then would be practical to have in a <code>model</code> object. We have thus added an abstraction borrowed from topicwizard called <code>TopicData</code>.</p>"},{"location":"topic_data/#producing-topicdata","title":"Producing <code>TopicData</code>","text":"<p>Every model has methods, with which you can produce this object:</p> <p>Prepare <code>TopicData</code> objects</p> <code>prepare_topic_data(corpus, embeddings=None)</code><code>prepare_dynamic_topic_data(corpus, timestamps, embeddings=None, bins=10)</code> <p><pre><code>topic_data = model.prepare_topic_data(corpus)\n# print to see what attributes are available\nprint(topic_data)\n</code></pre> <pre><code>TopicData\n\u251c\u2500\u2500 corpus (1000)\n\u251c\u2500\u2500 vocab (1746,)\n\u251c\u2500\u2500 document_term_matrix (1000, 1746)\n\u251c\u2500\u2500 topic_term_matrix (10, 1746)\n\u251c\u2500\u2500 document_topic_matrix (1000, 10)\n\u251c\u2500\u2500 document_representation (1000, 384)\n\u251c\u2500\u2500 transform\n\u251c\u2500\u2500 topic_names (10)\n\u251c\u2500\u2500 has_negative_side\n\u2514\u2500\u2500 hierarchy\n</code></pre></p> <p>Models that support dynamic topic modeling have this method too, which includes dynamic topics in the resulting <code>TopicData</code> object. <pre><code>import datetime\n\ntimestamps: list[datetime.datetime] = [...] \ntopic_data = model.prepare_dynamic_topic_data(corpus, timestamps=timestamps)\n</code></pre></p>"},{"location":"topic_data/#using-topicdata","title":"Using <code>TopicData</code>","text":"<p><code>TopicData</code> is a dict-like object, and for all intents and purposes can be used as a Python dictionary, but for convenience you can also access its attributes with the dot syntax:</p> <pre><code># They are the same\nassert topic_data[\"document_term_matrix\"].shape == topic_data.document_term_matrix.shape\n</code></pre> <p>Much like models, you can pretty-print information about topic models based on the <code>TopicData</code> object, but, since it contains more information on inference then the model object itself, you sometimes have to pass less parameters than if you called the same method on the model:</p> <pre><code>model.print_representative_documents(0, corpus, document_topic_matrix)\n# This is simpler with TopicData, since you only have to pass the topic ID\ntopic_data.print_representative_documents(0)\n</code></pre> <p>When producing figures, <code>TopicData</code> also gives you shorthands for accessing the topicwizard web app and Figures API:</p> <pre><code>topic_data.figures.topic_map()\n</code></pre> <p>See our guide on Model Interpretation for more info.</p>"},{"location":"topic_data/#api-reference","title":"API Reference","text":""},{"location":"topic_data/#turftopic.data.TopicData","title":"<code>turftopic.data.TopicData</code>","text":"<p>             Bases: <code>Mapping</code>, <code>TopicContainer</code></p> <p>Contains data about topic inference on a corpus. Can be used with multiple convenience and interpretation utilities.</p> <p>Parameters:</p> Name Type Description Default <code>vocab</code> <code>ndarray</code> <p>Array of all words in the vocabulary of the topic model.</p> required <code>document_term_matrix</code> <code>ndarray</code> <p>Bag-of-words document representations. Elements of the matrix are word importances/frequencies for given documents.</p> required <code>document_topic_matrix</code> <code>ndarray</code> <p>Topic importances for each document.</p> required <code>topic_term_matrix</code> <code>ndarray</code> <p>Importances of each term for each topic in a matrix.</p> required <code>document_representation</code> <code>ndarray</code> <p>Embedded representations for documents. Can also be a sparse BoW matrix for classical models.</p> required <code>topic_names</code> <code>Optional[list[str]]</code> <p>Names or topic descriptions inferred for topics by the model.</p> <code>None</code> <code>classes</code> <code>Optional[ndarray]</code> <p>Topic IDs that might be different from 0-n_topics. (For instance if you have an outlier topic, which is labelled -1)</p> <code>None</code> <code>corpus</code> <code>Optional[list[str]]</code> <p>The corpus on which inference was run. Can be None.</p> <code>None</code> <code>transform</code> <code>Optional[Callable]</code> <p>Function that transforms documents to document-topic matrices. Can be None in the case of transductive models.</p> <code>None</code> <code>time_bin_edges</code> <code>Optional[list[datetime]]</code> <p>Edges of the time bins in a dynamic topic model.</p> <code>None</code> <code>temporal_components</code> <code>Optional[ndarray]</code> <p>Topic-term importances over time. Only relevant for dynamic topic models.</p> <code>None</code> <code>temporal_importance</code> <code>Optional[ndarray]</code> <p>Topic strength signal over time. Only relevant for dynamic topic models.</p> <code>None</code> <code>has_negative_side</code> <code>bool</code> <p>Indicates whether the topic model's components are supposed to be interpreted in both directions. e.g. in SemanticSignalSeparation, one is supposed to look at highest, but also lowest ranking words. This is in contrast to KeyNMF for instance, where only positive word importance should be considered.</p> <code>False</code> <code>hierarchy</code> <code>Optional[TopicNode]</code> <p>Optional topic hierarchy for models that support hierarchical topic modeling.</p> <code>None</code> <code>images</code> <code>Optional[list[str | Image]]</code> <p>Images the model has been fit on</p> <code>None</code> <code>top_images</code> <code>Optional[list[list[Image]]]</code> <p>Top images discovered by the topic model.</p> <code>None</code> <code>negative_images</code> <code>Optional[list[list[Image]]]</code> <p>Lowest ranking images discivered by the topic model. (Only relevant with models like S^3)</p> <code>None</code> Source code in <code>turftopic/data.py</code> <pre><code>class TopicData(Mapping, TopicContainer):\n    \"\"\"Contains data about topic inference on a corpus.\n    Can be used with multiple convenience and interpretation utilities.\n\n    Parameters\n    ----------\n    vocab: ndarray of shape (n_vocab,)\n        Array of all words in the vocabulary of the topic model.\n    document_term_matrix: ndarray of shape (n_documents, n_vocab)\n        Bag-of-words document representations.\n        Elements of the matrix are word importances/frequencies for given documents.\n    document_topic_matrix: ndarray of shape (n_documents, n_topics)\n        Topic importances for each document.\n    topic_term_matrix: ndarray of shape (n_topics, n_vocab)\n        Importances of each term for each topic in a matrix.\n    document_representation: ndarray of shape (n_documents, n_dimensions)\n        Embedded representations for documents.\n        Can also be a sparse BoW matrix for classical models.\n    topic_names: list of str, default None\n        Names or topic descriptions inferred for topics by the model.\n    classes: np.ndarray, default None\n        Topic IDs that might be different from 0-n_topics.\n        (For instance if you have an outlier topic, which is labelled -1)\n    corpus: list of str, default None\n        The corpus on which inference was run. Can be None.\n    transform: (list[str]) -&gt; ndarray, default None\n        Function that transforms documents to document-topic matrices.\n        Can be None in the case of transductive models.\n    time_bin_edges: list[datetime], default None\n        Edges of the time bins in a dynamic topic model.\n    temporal_components: np.ndarray (n_slices, n_topics, n_vocab), default None\n        Topic-term importances over time. Only relevant for dynamic topic models.\n    temporal_importance: np.ndarray (n_slices, n_topics), default None\n        Topic strength signal over time. Only relevant for dynamic topic models.\n    has_negative_side: bool, default False\n        Indicates whether the topic model's components are supposed to be interpreted in both directions.\n        e.g. in SemanticSignalSeparation, one is supposed to look at highest, but also lowest ranking words.\n        This is in contrast to KeyNMF for instance, where only positive word importance should be considered.\n    hierarchy: TopicNode, default None\n        Optional topic hierarchy for models that support hierarchical topic modeling.\n    images: list[ImageRepr], default None\n        Images the model has been fit on\n    top_images: list[list[Image]], default None\n        Top images discovered by the topic model.\n    negative_images: list[list[Image]], default None\n        Lowest ranking images discivered by the topic model.\n        (Only relevant with models like S^3)\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        vocab: np.ndarray,\n        document_term_matrix: np.ndarray,\n        document_topic_matrix: np.ndarray,\n        topic_term_matrix: np.ndarray,\n        document_representation: np.ndarray,\n        topic_names: Optional[list[str]] = None,\n        classes: Optional[np.ndarray] = None,\n        corpus: Optional[list[str]] = None,\n        transform: Optional[Callable] = None,\n        time_bin_edges: Optional[list[datetime]] = None,\n        temporal_components: Optional[np.ndarray] = None,\n        temporal_importance: Optional[np.ndarray] = None,\n        has_negative_side: bool = False,\n        hierarchy: Optional[TopicNode] = None,\n        images: Optional[list[str | Image.Image]] = None,\n        top_images: Optional[list[list[Image.Image]]] = None,\n        negative_images: Optional[list[list[Image.Image]]] = None,\n        **kwargs,\n    ):\n        self.corpus = corpus\n        self.vocab = vocab\n        self.document_term_matrix = document_term_matrix\n        self.document_topic_matrix = document_topic_matrix\n        self.topic_term_matrix = topic_term_matrix\n        self.document_representation = document_representation\n        self.transform = transform\n        self.topic_names_ = topic_names\n        self.classes = classes\n        self.time_bin_edges = time_bin_edges\n        self.temporal_components = temporal_components\n        self.temporal_importance = temporal_importance\n        self.hierarchy = hierarchy\n        self._has_negative_side = has_negative_side\n        self.top_images = top_images\n        self.negative_images = negative_images\n        self.images = images\n        for key, value in kwargs:\n            setattr(self, key, value)\n        self._attributes = [\n            \"corpus\",\n            \"vocab\",\n            \"document_term_matrix\",\n            \"topic_term_matrix\",\n            \"document_topic_matrix\",\n            \"document_representation\",\n            \"transform\",\n            \"topic_names\",\n            \"time_bin_edges\",\n            \"temporal_components\",\n            \"temporal_importance\",\n            \"has_negative_side\",\n            \"hierarchy\",\n            *kwargs.keys(),\n        ]\n\n    @property\n    def components_(self) -&gt; np.ndarray:\n        return self.topic_term_matrix\n\n    @property\n    def temporal_components_(self) -&gt; np.ndarray:\n        if self.temporal_components is None:\n            raise AttributeError(\n                \"Topic data does not contain dynamic information.\"\n            )\n        return self.temporal_components\n\n    @property\n    def temporal_importance_(self) -&gt; np.ndarray:\n        if self.temporal_importance is None:\n            raise AttributeError(\n                \"Topic data does not contain dynamic information.\"\n            )\n        return self.temporal_importance\n\n    @property\n    def classes_(self) -&gt; np.ndarray:\n        if self.classes is None:\n            raise AttributeError(\"Topic model does not have classes_\")\n        else:\n            return self.classes\n\n    def __getitem__(self, key):\n        return getattr(self, key)\n\n    def __setitem__(self, key, newvalue):\n        return setattr(self, key, newvalue)\n\n    def __len__(self):\n        return len(self._attributes)\n\n    def __iter__(self):\n        return iter(self._attributes)\n\n    def get_vocab(self) -&gt; np.ndarray:\n        return self.vocab\n\n    def __str__(self):\n        console = Console()\n        with console.capture() as capture:\n            tree = Tree(\"TopicData\")\n            for key, value in self.items():\n                if value is None:\n                    continue\n                if hasattr(value, \"shape\"):\n                    text = f\"{key} {value.shape}\"\n                elif hasattr(value, \"__len__\"):\n                    text = f\"{key} ({len(value)})\"\n                else:\n                    text = key\n                tree.add(text)\n            console.print(tree)\n        return capture.get()\n\n    def __repr__(self):\n        return str(self)\n\n    def visualize_topicwizard(self, **kwargs):\n        \"\"\"Opens the topicwizard web app with which you can interactively investigate your model.\n        See [topicwizard's documentation](https://github.com/x-tabdeveloping/topicwizard) for more detail.\n        \"\"\"\n        try:\n            import topicwizard\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"topicwizard is not installed on your system, you can install it by running pip install turftopic[topic-wizard].\"\n            )\n        return topicwizard.visualize(topic_data=self, **kwargs)\n\n    @property\n    def figures(self):\n        \"\"\"Container object for topicwizard figures that can be generated from this TopicData object.\n        You can use any of the interactive figures from the [Figures API](https://x-tabdeveloping.github.io/topicwizard/figures.html) in topicwizard.\n\n        For instance:\n        ```python\n        topic_data.figures.topic_barcharts()\n        # or\n        topic_data.figures.topic_wordclouds()\n        ```\n        See [topicwizard's documentation](https://github.com/x-tabdeveloping/topicwizard) for more detail.\n        \"\"\"\n        try:\n            import topicwizard.figures\n        except ModuleNotFoundError:\n            raise ModuleNotFoundError(\n                \"topicwizard is not installed on your system, you can install it by running pip install turftopic[topic-wizard].\"\n            )\n\n        # Skip Group figures\n        figure_names = [\n            figure_name\n            for figure_name in topicwizard.figures.__all__\n            if not figure_name.startswith(\"group\")\n        ]\n        module = Figures(figure_names)\n        for figure_name in figure_names:\n            figure_fn = getattr(topicwizard.figures, figure_name)\n            figure_fn = partial(figure_fn, topic_data=self)\n            setattr(\n                module,\n                figure_name,\n                figure_fn,\n            )\n        return module\n\n    @classmethod\n    def from_disk(cls, path: str | Path):\n        \"\"\"Loads TopicData object from disk with Joblib.\n\n        Parameters\n        ----------\n        path: str or Path\n            Path to load the data from, e.g. \"topic_data.joblib\"\n        \"\"\"\n        path = Path(path)\n        data = joblib.load(path)\n        return cls(**data)\n\n    def to_disk(self, path: str | Path):\n        \"\"\"Saves TopicData object to disk.\n\n        Parameters\n        ----------\n        path: str or Path\n            Path to save the data to, e.g. \"topic_data.joblib\"\n        \"\"\"\n        path = Path(path)\n        joblib.dump({**self}, path)\n\n    @property\n    def has_negative_side(self) -&gt; bool:\n        return self._has_negative_side\n</code></pre>"},{"location":"topic_data/#turftopic.data.TopicData.figures","title":"<code>figures</code>  <code>property</code>","text":"<p>Container object for topicwizard figures that can be generated from this TopicData object. You can use any of the interactive figures from the Figures API in topicwizard.</p> <p>For instance: <pre><code>topic_data.figures.topic_barcharts()\n# or\ntopic_data.figures.topic_wordclouds()\n</code></pre> See topicwizard's documentation for more detail.</p>"},{"location":"topic_data/#turftopic.data.TopicData.from_disk","title":"<code>from_disk(path)</code>  <code>classmethod</code>","text":"<p>Loads TopicData object from disk with Joblib.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to load the data from, e.g. \"topic_data.joblib\"</p> required Source code in <code>turftopic/data.py</code> <pre><code>@classmethod\ndef from_disk(cls, path: str | Path):\n    \"\"\"Loads TopicData object from disk with Joblib.\n\n    Parameters\n    ----------\n    path: str or Path\n        Path to load the data from, e.g. \"topic_data.joblib\"\n    \"\"\"\n    path = Path(path)\n    data = joblib.load(path)\n    return cls(**data)\n</code></pre>"},{"location":"topic_data/#turftopic.data.TopicData.to_disk","title":"<code>to_disk(path)</code>","text":"<p>Saves TopicData object to disk.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str | Path</code> <p>Path to save the data to, e.g. \"topic_data.joblib\"</p> required Source code in <code>turftopic/data.py</code> <pre><code>def to_disk(self, path: str | Path):\n    \"\"\"Saves TopicData object to disk.\n\n    Parameters\n    ----------\n    path: str or Path\n        Path to save the data to, e.g. \"topic_data.joblib\"\n    \"\"\"\n    path = Path(path)\n    joblib.dump({**self}, path)\n</code></pre>"},{"location":"topic_data/#turftopic.data.TopicData.visualize_topicwizard","title":"<code>visualize_topicwizard(**kwargs)</code>","text":"<p>Opens the topicwizard web app with which you can interactively investigate your model. See topicwizard's documentation for more detail.</p> Source code in <code>turftopic/data.py</code> <pre><code>def visualize_topicwizard(self, **kwargs):\n    \"\"\"Opens the topicwizard web app with which you can interactively investigate your model.\n    See [topicwizard's documentation](https://github.com/x-tabdeveloping/topicwizard) for more detail.\n    \"\"\"\n    try:\n        import topicwizard\n    except ModuleNotFoundError:\n        raise ModuleNotFoundError(\n            \"topicwizard is not installed on your system, you can install it by running pip install turftopic[topic-wizard].\"\n        )\n    return topicwizard.visualize(topic_data=self, **kwargs)\n</code></pre>"},{"location":"vectorizers/","title":"Vectorizers","text":"<p>One of the most important attributes of a topic model you will have to choose is the vectorizer. A vectorizer is responsible for extracting term-features from text. It determines for which terms word-importance scores will be calculated.</p> <p>By default, Turftopic uses sklearn's CountVectorizer, which naively counts word/n-gram occurrences in text. This usually works quite well, but your use case might require you to use a different or more sophisticated approach. This is why we provide a <code>vectorizers</code> module, where a wide range of useful options is available to you.</p> <p>How is this different from preprocessing?</p> <p>You might think that preprocessing the documents might result in the same effect as some of these vectorizers, but this is not entirely the case. When you remove stop words or lemmatize texts in preprocessing, you remove a lot of valuable information that your topic model then can't use. By defining a custom vectorizer you limit the vocabulary of your model, thereby only learning word importance scores for certain words, but you keep your documents fully intact.</p>"},{"location":"vectorizers/#phrase-vectorizers","title":"Phrase Vectorizers","text":"<p>You might want to get phrases in your topic descriptions instead of individual words. This could prove a very reasonable choice as it's often not words in themselves but phrases made up by them that describe a topic most accurately. Turftopic supports multiple ways of using phrases as fundamental terms.</p>"},{"location":"vectorizers/#n-gram-features-with-countvectorizer","title":"N-gram Features with <code>CountVectorizer</code>","text":"<p><code>CountVectorizer</code> supports n-gram extraction right out of the box. Just define a custom vectorizer with an <code>n_gram_range</code>.</p> <p>Tip</p> <p>While this option is naive, and will likely yield the lowest quality results, it is also incredibly fast in comparison to other phrase vectorization techniques. It might, however be slower, if the topic model encodes its vocabulary when fitting.</p> <pre><code>from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer(ngram_range=(2,3), stop_words=\"english\")\n\nmodel = KeyNMF(10, vectorizer=vectorizer)\nmodel.fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking 0 bronx away sank, blew bronx away, blew bronx, bronx away, sank manhattan, stay blew bronx, manhattan sea, away sank manhattan, said queens stay, queens stay 1 faq alt atheism, alt atheism archive, atheism overview alt, alt atheism resources, atheism faq frequently, archive atheism overview, alt atheism faq, overview alt atheism, titles alt atheism, readers alt atheism 2 theism factor fanatism, theism leads fanatism, fanatism caused theism, theism correlated fanaticism, fanatism point theism, fanatism deletion theism, fanatics tend theism, fanaticism said fanatism, correlated fanaticism belief, strongly correlated fanaticism 3 alt atheism, atheism archive, alt atheism archive, archive atheism, atheism atheism, atheism faq, archive atheism introduction, atheism archive introduction, atheism introduction alt, atheism introduction ..."},{"location":"vectorizers/#noun-phrases-with-nounphrasecountvectorizer","title":"Noun phrases with <code>NounPhraseCountVectorizer</code>","text":"<p>Turftopic can also use noun phrases by utilizing the SpaCy package. For Noun phrase vectorization to work, you will have to install SpaCy.</p> <pre><code>pip install turftopic[spacy]\n</code></pre> <p>You will also need to install a relevant SpaCy pipeline for the language you intend to use. The default pipeline is English, and you should install it before attempting to use <code>NounPhraseCountVectorizer</code>.</p> <p>You can find a model that fits your needs here.</p> <pre><code>python -m spacy download en_core_web_sm\n</code></pre> <p>Using SpaCy pipelines will substantially slow down model fitting, but the results might be more correct and higher quality than with naive n-gram extraction. <pre><code>from turftopic import KeyNMF\nfrom turftopic.vectorizers.spacy import NounPhraseCountVectorizer\n\nmodel = KeyNMF(\n    n_components=10,\n    vectorizer=NounPhraseCountVectorizer(\"en_core_web_sm\"),\n)\nmodel.fit(corpus)\nmodel.print_topics()\n</code></pre></p> Topic ID Highest Ranking 0 atheists, atheism, atheist, belief, beliefs, theists, faith, gods, christians, abortion 1 alt atheism, usenet alt atheism resources, usenet alt atheism introduction, alt atheism faq, newsgroup alt atheism, atheism faq resource txt, alt atheism groups, atheism, atheism faq intro txt, atheist resources 2 religion, christianity, faith, beliefs, religions, christian, belief, science, cult, justification 3 fanaticism, theism, fanatism, all fanatism, theists, strong theism, strong atheism, fanatics, precisely some theists, all theism 4 religion foundation darwin fish bumper stickers, darwin fish, atheism, 3d plastic fish, fish symbol, atheist books, atheist organizations, negative atheism, positive atheism, atheism index ..."},{"location":"vectorizers/#keyphrases-with-keyphrasevectorizers","title":"Keyphrases with KeyphraseVectorizers","text":"<p>You can extract candidate keyphrases from text using KeyphraseVectorizers. KeyphraseVectorizers uses POS tag patterns to identify phrases instead of word dependency graphs, like <code>NounPhraseCountVectorizer</code>. KeyphraseVectorizers can potentially be faster as the dependency parser component is not needed in the SpaCy pipeline. This vectorizer is not part of the Turftopic package, but can be easily used with it out of the box.</p> <pre><code>pip install keyphrase-vectorizers\n</code></pre> <pre><code>from keyphrase_vectorizers import KeyphraseCountVectorizer\n\nvectorizer = KeyphraseCountVectorizer()\nmodel = KeyNMF(10, vectorizer=vectorizer).fit(corpus)\n</code></pre>"},{"location":"vectorizers/#lemmatizing-and-stemming-vectorizers","title":"Lemmatizing and Stemming Vectorizers","text":"<p>Since the same word can appear in multiple forms in a piece of text, one can sometimes obtain higher quality results by stemming or lemmatizing words in a text before processing them.</p> <p>Warning</p> <p>You should NEVER lemmatize or stem texts before passing them to a topic model in Turftopic, but rather, use a vectorizer that limits the model's vocabulary to the terms you are interested in.</p>"},{"location":"vectorizers/#extracting-lemmata-with-lemmacountvectorizer","title":"Extracting lemmata with <code>LemmaCountVectorizer</code>","text":"<p>Similarly to <code>NounPhraseCountVectorizer</code>, <code>LemmaCountVectorizer</code> relies on a SpaCy pipeline for extracting lemmas from a piece of text. This means you will have to install SpaCy and a SpaCy pipeline to be able to use it.</p> <pre><code>pip install turftopic[spacy]\npython -m spacy download en_core_web_sm\n</code></pre> <pre><code>from turftopic import KeyNMF\nfrom turftopic.vectorizers.spacy import LemmaCountVectorizer\n\nmodel = KeyNMF(10, vectorizer=LemmaCountVectorizer(\"en_core_web_sm\"))\nmodel.fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking 0 atheist, theist, belief, christians, agnostic, christian, mythology, asimov, abortion, read 1 morality, moral, immoral, objective, society, animal, natural, societal, murder, morally 2 religion, religious, christianity, belief, christian, faith, cult, church, secular, christians 3 atheism, belief, agnosticism, religious, faq, lack, existence, theism, atheistic, allah 4 islam, muslim, islamic, rushdie, khomeini, bank, imam, bcci, law, secular ..."},{"location":"vectorizers/#stemming-words-with-stemmingcountvectorizer","title":"Stemming words with <code>StemmingCountVectorizer</code>","text":"<p>You might find that lemmatization isn't aggressive enough for your purposes and still many forms of the same word penetrate topic descriptions. In that case you should try stemming! Stemming is available in Turftopic via the Snowball Stemmer, so it has to be installed before using stemming vectorization.</p> <p>Should I choose stemming or lemmatization?</p> <p>In almost all cases you should prefer lemmatizaion over stemming, as it provides higher quality and more correct results. You should only use a stemmer if </p> <ol> <li>You need something fast (lemmatization is slower due to a more involved pipeline)</li> <li>You know what you want and it is definitely stemming.</li> </ol> <pre><code>pip install turftopic[snowball]\n</code></pre> <p>Then you can initialize a topic model with this vectorizer:</p> <pre><code>from turftopic import KeyNMF\nfrom turftopic.vectorizers.snowball import StemmingCountVectorizer\n\nmodel = KeyNMF(10, vectorizer=StemmingCountVectorizer(language=\"english\"))\nmodel.fit(corpus)\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking 0 atheism, belief, alt, theism, agnostic, stalin, lack, sceptic, exist, faith 1 religion, belief, religi, cult, faith, theism, secular, theist, scientist, dogma 2 bronx, manhattan, sank, queen, sea, away, said, com, bob, blew 3 moral, human, instinct, murder, kill, law, behaviour, action, behavior, ethic 4 atheist, theist, belief, asimov, philosoph, mytholog, strong, faq, agnostic, weak"},{"location":"vectorizers/#non-english-vectorization","title":"Non-English Vectorization","text":"<p>You may find that, especially with non-Indo-European languages, <code>CountVectorizer</code> does not perform that well. In these cases we recommend that you use a vectorizer with its own language-specific tokenization rules and stop-word list:</p>"},{"location":"vectorizers/#vectorizing-any-language-with-tokencountvectorizer","title":"Vectorizing Any Language with <code>TokenCountVectorizer</code>","text":"<p>The SpaCy package includes language-specific tokenization and stop-word rules for just about any language. We provide a vectorizer that you can use with the language of your choice.</p> <pre><code>pip install turftopic[spacy]\n</code></pre> <p>Note</p> <p>Note that you do not have to install any SpaCy pipelines for this to work. No pipelines or models will be loaded with <code>TokenCountVectorizer</code> only a language-specific tokenizer.</p> <pre><code>from turftopic import KeyNMF\nfrom turftopic.vectorizers.spacy import TokenCountVectorizer\n\n# CountVectorizer for Arabic\nvectorizer = TokenCountVectorizer(\"ar\", min_df=10)\n\nmodel = KeyNMF(\n    n_components=10,\n    vectorizer=vectorizer,\n    encoder=\"Omartificial-Intelligence-Space/Arabic-MiniLM-L12-v2-all-nli-triplet\"\n)\nmodel.fit(corpus)\n</code></pre>"},{"location":"vectorizers/#extracting-chinese-tokens-with-chinesecountvectorizer","title":"Extracting Chinese Tokens with <code>ChineseCountVectorizer</code>","text":"<p>The Chinese language does not separate tokens by whitespace, unlike most Indo-European languages. You thus need to use special tokenization rules for Chinese. Turftopic provides tools for Chinese tokenization via the Jieba package.</p> <p>Note</p> <p>We recommend that you use Jieba over SpaCy for topic modeling with Chinese.</p> <p>You will need to install the package in order to be able to use our Chinese vectorizer.</p> <pre><code>pip install turftopic[jieba]\n</code></pre> <p>You can then use the <code>ChineseCountVectorizer</code> object, which comes preloaded with the jieba tokenizer along with a Chinese stop word list.</p> <pre><code>from turftopic import KeyNMF\nfrom turftopic.vectorizers.chinese import ChineseCountVectorizer\n\nvectorizer = ChineseCountVectorizer(min_df=10, stop_words=\"chinese\")\n\nmodel = KeyNMF(10, vectorizer=vectorizer, encoder=\"BAAI/bge-small-zh-v1.5\")\nmodel.fit(corpus)\n\nmodel.print_topics()\n</code></pre> Topic ID Highest Ranking 0 \u200b\u6d88\u606f\u200b, \u200b\u65f6\u95f4\u200b, \u200b\u79d1\u6280\u200b, \u200b\u5a92\u4f53\u62a5\u9053\u200b, \u200b\u7f8e\u56fd\u200b, \u200b\u636e\u200b, \u200b\u56fd\u5916\u200b, \u200b\u8baf\u200b, \u200b\u5ba3\u5e03\u200b, \u200b\u79f0\u200b 1 \u200b\u4f53\u80b2\u8baf\u200b, \u200b\u65b0\u6d6a\u200b, \u200b\u7403\u5458\u200b, \u200b\u7403\u961f\u200b, \u200b\u8d5b\u5b63\u200b, \u200b\u706b\u7bad\u200b, nba, \u200b\u5df2\u7ecf\u200b, \u200b\u4e3b\u573a\u200b, \u200b\u65f6\u95f4\u200b 2 \u200b\u8bb0\u8005\u200b, \u200b\u672c\u62a5\u8baf\u200b, \u200b\u6628\u65e5\u200b, \u200b\u83b7\u6089\u200b, \u200b\u65b0\u534e\u7f51\u200b, \u200b\u57fa\u91d1\u200b, \u200b\u901a\u8baf\u5458\u200b, \u200b\u91c7\u8bbf\u200b, \u200b\u7537\u5b50\u200b, \u200b\u6628\u5929\u200b 3 \u200b\u80a1\u200b, \u200b\u4e0b\u8dcc\u200b, \u200b\u4e0a\u6da8\u200b, \u200b\u9707\u8361\u200b, \u200b\u677f\u5757\u200b, \u200b\u5927\u76d8\u200b, \u200b\u80a1\u6307\u200b, \u200b\u6da8\u5e45\u200b, \u200b\u6caa\u200b, \u200b\u53cd\u5f39\u200b ..."},{"location":"vectorizers/#api-reference","title":"API Reference","text":""},{"location":"vectorizers/#turftopic.vectorizers.spacy.NounPhraseCountVectorizer","title":"<code>turftopic.vectorizers.spacy.NounPhraseCountVectorizer</code>","text":"<p>             Bases: <code>CountVectorizer</code></p> <p>Extracts Noun phrases from text using SpaCy.</p> <p>Parameters:</p> Name Type Description Default <code>nlp</code> <code>Union[Language, str]</code> <p>A Spacy pipeline or its name.</p> <code>'en_core_web_sm'</code> Source code in <code>turftopic/vectorizers/spacy.py</code> <pre><code>class NounPhraseCountVectorizer(CountVectorizer):\n    \"\"\"Extracts Noun phrases from text using SpaCy.\n\n    Parameters\n    ----------\n    nlp: spacy.Language or str, default \"en_core_web_sm\"\n        A Spacy pipeline or its name.\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Union[Language, str] = \"en_core_web_sm\",\n        *,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n        analyzer=\"word\",\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.int64,\n    ):\n        self.nlp = nlp\n        if isinstance(nlp, str):\n            self._nlp = spacy.load(nlp)\n        else:\n            self._nlp = nlp\n        super().__init__(\n            input=input,\n            encoding=encoding,\n            decode_error=decode_error,\n            strip_accents=strip_accents,\n            lowercase=lowercase,\n            preprocessor=preprocessor,\n            tokenizer=tokenizer,\n            stop_words=stop_words,\n            token_pattern=token_pattern,\n            ngram_range=ngram_range,\n            analyzer=analyzer,\n            max_df=max_df,\n            min_df=min_df,\n            max_features=max_features,\n            vocabulary=vocabulary,\n            binary=binary,\n            dtype=dtype,\n        )\n\n    def nounphrase_tokenize(self, text: str) -&gt; list[str]:\n        doc = self._nlp(text)\n        tokens = []\n        for chunk in doc.noun_chunks:\n            if chunk[0].is_stop:\n                chunk = chunk[1:]\n            phrase = chunk.text\n            phrase = re.sub(r\"[^\\w\\s]\", \" \", phrase)\n            phrase = \" \".join(phrase.split()).strip()\n            if phrase:\n                tokens.append(phrase)\n        return tokens\n\n    def build_tokenizer(self):\n        return self.nounphrase_tokenize\n</code></pre>"},{"location":"vectorizers/#turftopic.vectorizers.spacy.LemmaCountVectorizer","title":"<code>turftopic.vectorizers.spacy.LemmaCountVectorizer</code>","text":"<p>             Bases: <code>CountVectorizer</code></p> <p>Extracts lemmata from text using SpaCy.</p> <p>Parameters:</p> Name Type Description Default <code>nlp</code> <code>Union[Language, str]</code> <p>A Spacy pipeline or its name.</p> <code>'en_core_web_sm'</code> Source code in <code>turftopic/vectorizers/spacy.py</code> <pre><code>class LemmaCountVectorizer(CountVectorizer):\n    \"\"\"Extracts lemmata from text using SpaCy.\n\n    Parameters\n    ----------\n    nlp: spacy.Language or str, default \"en_core_web_sm\"\n        A Spacy pipeline or its name.\n    \"\"\"\n\n    def __init__(\n        self,\n        nlp: Union[Language, str] = \"en_core_web_sm\",\n        *,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n        analyzer=\"word\",\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.int64,\n    ):\n        self.nlp = nlp\n        if isinstance(nlp, str):\n            self._nlp = spacy.load(nlp)\n        else:\n            self._nlp = nlp\n        super().__init__(\n            input=input,\n            encoding=encoding,\n            decode_error=decode_error,\n            strip_accents=strip_accents,\n            lowercase=lowercase,\n            preprocessor=preprocessor,\n            tokenizer=tokenizer,\n            stop_words=stop_words,\n            token_pattern=token_pattern,\n            ngram_range=ngram_range,\n            analyzer=analyzer,\n            max_df=max_df,\n            min_df=min_df,\n            max_features=max_features,\n            vocabulary=vocabulary,\n            binary=binary,\n            dtype=dtype,\n        )\n\n    def lemma_tokenize(self, text: str) -&gt; list[str]:\n        doc = self._nlp(text)\n        tokens = []\n        for token in doc:\n            if token.is_stop or not token.is_alpha:\n                continue\n            tokens.append(token.lemma_.strip())\n        return tokens\n\n    def build_tokenizer(self):\n        return self.lemma_tokenize\n</code></pre>"},{"location":"vectorizers/#turftopic.vectorizers.spacy.TokenCountVectorizer","title":"<code>turftopic.vectorizers.spacy.TokenCountVectorizer</code>","text":"<p>             Bases: <code>CountVectorizer</code></p> <p>Tokenizes text with SpaCy using its language-specific tokenization rules and stop-word lists</p> <p>Parameters:</p> Name Type Description Default <code>language_code</code> <code>str</code> <p>Language code for the language you intend to use.</p> <code>'en'</code> <code>remove_stop_words</code> <code>bool</code> <p>Indicates whether stop words should be removed.</p> <code>True</code> <code>remove_nonalpha</code> <code>bool</code> <p>Indicates whether only tokens containing alphabetical characters should be kept.</p> <code>True</code> Source code in <code>turftopic/vectorizers/spacy.py</code> <pre><code>class TokenCountVectorizer(CountVectorizer):\n    \"\"\"Tokenizes text with SpaCy using its language-specific tokenization rules and stop-word lists\n\n    Parameters\n    ----------\n    language_code: str, default \"en\"\n        Language code for the language you intend to use.\n    remove_stop_words: bool, default True\n        Indicates whether stop words should be removed.\n    remove_nonalpha: bool, default True\n        Indicates whether only tokens containing alphabetical characters should be kept.\n    \"\"\"\n\n    def __init__(\n        self,\n        language_code: str = \"en\",\n        remove_stop_words: bool = True,\n        remove_nonalpha: bool = True,\n        *,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n        analyzer=\"word\",\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.int64,\n    ):\n        self.language_code = language_code\n        self.remove_stop_words = remove_stop_words\n        self.remove_nonalpha = remove_nonalpha\n        super().__init__(\n            input=input,\n            encoding=encoding,\n            decode_error=decode_error,\n            strip_accents=strip_accents,\n            lowercase=lowercase,\n            preprocessor=preprocessor,\n            tokenizer=tokenizer,\n            stop_words=stop_words,\n            token_pattern=token_pattern,\n            ngram_range=ngram_range,\n            analyzer=analyzer,\n            max_df=max_df,\n            min_df=min_df,\n            max_features=max_features,\n            vocabulary=vocabulary,\n            binary=binary,\n            dtype=dtype,\n        )\n\n    def build_tokenizer(self):\n        nlp = spacy.blank(self.language_code)\n\n        def tokenize(text: str) -&gt; list[str]:\n            doc = nlp(text)\n            result = []\n            for tok in doc:\n                if self.remove_stop_words and tok.is_stop:\n                    continue\n                if self.remove_nonalpha and not tok.is_alpha:\n                    continue\n                result.append(tok.orth_)\n            return result\n\n        return tokenize\n</code></pre>"},{"location":"vectorizers/#turftopic.vectorizers.snowball.StemmingCountVectorizer","title":"<code>turftopic.vectorizers.snowball.StemmingCountVectorizer</code>","text":"<p>             Bases: <code>CountVectorizer</code></p> <p>Extractes stemmed words from documents using Snowball.</p> Source code in <code>turftopic/vectorizers/snowball.py</code> <pre><code>class StemmingCountVectorizer(CountVectorizer):\n    \"\"\"Extractes stemmed words from documents using Snowball.\"\"\"\n\n    def __init__(\n        self,\n        language=\"english\",\n        *,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n        analyzer=\"word\",\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.int64,\n    ):\n        self.language = language\n        self._stemmer = snowballstemmer.stemmer(self.language)\n        super().__init__(\n            input=input,\n            encoding=encoding,\n            decode_error=decode_error,\n            strip_accents=strip_accents,\n            lowercase=lowercase,\n            preprocessor=preprocessor,\n            tokenizer=tokenizer,\n            stop_words=stop_words,\n            token_pattern=token_pattern,\n            ngram_range=ngram_range,\n            analyzer=analyzer,\n            max_df=max_df,\n            min_df=min_df,\n            max_features=max_features,\n            vocabulary=vocabulary,\n            binary=binary,\n            dtype=dtype,\n        )\n\n    def build_tokenizer(self):\n        super_tokenizer = super().build_tokenizer()\n\n        def tokenizer(text):\n            return self._stemmer.stemWords(super_tokenizer(text))\n\n        return tokenizer\n</code></pre>"},{"location":"vectorizers/#turftopic.vectorizers.chinese.ChineseCountVectorizer","title":"<code>turftopic.vectorizers.chinese.ChineseCountVectorizer</code>","text":"<p>             Bases: <code>CountVectorizer</code></p> <p>Chinese count vectorizer. Does word segmentation with Jieba, and includes a chinese stop words list. You have to specify stop_words=\"chinese\" for this to kick into effect.</p> Source code in <code>turftopic/vectorizers/chinese.py</code> <pre><code>class ChineseCountVectorizer(CountVectorizer):\n    \"\"\"Chinese count vectorizer. Does word segmentation with Jieba, and includes a chinese stop words list.\n    You have to specify stop_words=\"chinese\" for this to kick into effect.\n    \"\"\"\n\n    def __init__(\n        self,\n        *,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=tokenize_zh,\n        stop_words=None,\n        token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n        ngram_range=(1, 1),\n        analyzer=\"word\",\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.int64,\n    ):\n        if stop_words == \"chinese\":\n            stop_words = chinese_stop_words\n        super().__init__(\n            input=input,\n            encoding=encoding,\n            decode_error=decode_error,\n            strip_accents=strip_accents,\n            lowercase=lowercase,\n            preprocessor=preprocessor,\n            tokenizer=tokenizer,\n            stop_words=stop_words,\n            token_pattern=token_pattern,\n            ngram_range=ngram_range,\n            analyzer=analyzer,\n            max_df=max_df,\n            min_df=min_df,\n            max_features=max_features,\n            vocabulary=vocabulary,\n            binary=binary,\n            dtype=dtype,\n        )\n</code></pre>"},{"location":"tutorials/arxiv_ml/","title":"Building a Taxonomy of Machine Learning Papers","text":"<p>Topic models can often be used to gain an overall understanding of what kinds of topics are being discussed in a corpus. If one wanted to build a taxonomy of the field of machine learning, a good starting point would be to investigate what sorts of machine learning articles have been put on ArXiv, the largest preprint server.</p> <p>In this tutorial, I will demonstrate how you can use Turftopic to discover themes in machine learning abstracts using a clustering topic model, we will look at:</p> <ul> <li>Building and training a clustering topic model</li> <li>Interpreting the model's output, and</li> <li>Topic-based filtering and retrieval </li> </ul>"},{"location":"tutorials/arxiv_ml/#installation","title":"Installation","text":"<p>For this investigation we will use two additional libraries. <code>datasets</code> to fetch the dataset from HF Hub, as well as <code>plotly</code> for creating interactive plots of our results. We also need to install optional dependencies to be able to use umap-based clustering models, and then create a map using datamapplot.</p> <pre><code>pip install datasets plotly turftopic[umap-learn, datamapplot]\n</code></pre>"},{"location":"tutorials/arxiv_ml/#data-preparation","title":"Data Preparation","text":"<p>We will download the dataset from HuggingFace Hub, using <code>datasets</code>.</p> <p>Note</p> <p>In this example, we will downsample to a subset of the data (10000 examples). We do this to make the tutorial run smoothly. However, often downsampling can be  a great strategy for speeding up topic modelling.</p> <pre><code>from datasets import load_dataset\n\nds = load_dataset(\"CShorten/ML-ArXiv-Papers\", split=\"train\")\n# Subsampling dataset\nds = ds.train_test_split(seed=42, test_size=10_000)[\"test\"]\nabstracts = ds[\"abstract\"]\n</code></pre> <p>Turftopic uses contextual embeddings of documents in order to understand what is in the corpus. We will use the small and fast <code>all-MiniLM-L6-v2</code> sentence transformer to produce embeddings of our data.</p> <p>Tip</p> <p>Sometimes you might want to reuse embeddings in different topic models, or save them to disk. It is thus recommended, but not necessary to precompute them before running a topic model.</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\nembeddings = encoder.encode(abstracts, show_progress_bar=True)\n</code></pre>"},{"location":"tutorials/arxiv_ml/#model-training","title":"Model Training","text":"<p>For building a hierarchy, we will assume that a paper is centered around one central theme, and we would like to categorize our articles into distinct groups. Clustering topic models, such as BERTopic and Top2Vec are well suited for this task, as they assume that a document belongs to a single cluster. They can also be used in settings, where we expect there to be a hierarchy of topics. This is certainly the case for machine learning, where, for instance, auto-encoders could be a sub-topic of unsupervised learning.</p> <p>In this example I am going to use the Top2Vec topic model, which discovers topic using UMAP and HDBSCAN, and produces very clean topic descriptions. Top2Vec learns the number of topics from the data, so we won't have to specify it a-priori.</p> <pre><code>from turftopic import Top2Vec\n\nmodel = Top2Vec(encoder=encoder, random_state=42)\ntopic_data = model.prepare_topic_data(abstracts, embeddings=embeddings)\n</code></pre>"},{"location":"tutorials/arxiv_ml/#interpreting-results","title":"Interpreting Results","text":"<p>Tip</p> <p>For a more detailed discussion, see the Model Interpretation page in the documentation.</p> <p>Let's print the topic in our model in order to see what sorts of topics have been discovered.</p> <pre><code>model.print_topics()\n</code></pre> Topic ID Highest Ranking -1 softmax, supervised, cnn, cnns, autoencoders, learns, classifiers, imagenet, rnns, learning 0 quantum, quantization, qubit, quantized, encoders, backpropagation, learns, disentangling, learning, decoders 1 backpropagation, robustness, lyapunov, robust, adversarial, softmax, controllers, learnable, robustly, controllable 2 fairness, discrimination, bias, unfairness, biases, discriminating, discriminate, adversarially, classifiers, penalized 3 investment, learns, learning, reinforcement, learnt, finance, rnns, trading, memorization, bandit ... <p>We can already see some clear themes emerge, for instance we can see that a group of ML papers are dedicated to quantum computing, and there are also some researchers investigating bias and fairness in machine learning approaches.</p>"},{"location":"tutorials/arxiv_ml/#building-a-topic-hierarchy","title":"Building a Topic Hierarchy","text":"<p>Tip</p> <p>For a more detailed discussion, see the Hierarchical Modelling page in the documentation.</p> <p>Our model has discovered over 100 topics, which can be a bit hard to interpret. Luckily, we can reduce the number of top level topics in clustering models by iteratively merging them until we obtain a desired number. This will also build a cluster hierarchy, which we can then investigate using the model's <code>hierarchy</code> property.</p> <pre><code>model.reduce_topics(n_reduce_to=25)\nprint(model.hierarchy.cut(3))\n</code></pre> <pre><code>Root: \n\u251c\u2500\u2500 -1: softmax, supervised, cnn, cnns, autoencoders, learns, classifiers, imagenet, rnns, learning\n\u251c\u2500\u2500 6: privacy, adversarially, randomization, adversarial, softmax, randomized, distributed, supervised, rnns, regularization\n\u251c\u2500\u2500 50: gans, generative, gan, adversarial, adversarially, autoencoders, inception, cyclegan, autoencoder, imagenet\n\u251c\u2500\u2500 105: minimizers, optimizations, minimizes, optimization, minimizer, optimizers, optimizer, minimization, minimize, optimizing\n\u251c\u2500\u2500 161: learns, reinforcement, learning, learnt, planning, ai, memorization, learnable, rnns, supervised\n\u2502   \u251c\u2500\u2500 8: mobilenet, networking, mobilenetv2, scheduling, bandwidth, 5g, resnets, networks, network, congestion\n\u2502   \u2514\u2500\u2500 129: learns, reinforcement, learning, learnt, planning, ai, memorization, learnable, rnns, supervised\n\u2502       \u251c\u2500\u2500 1: backpropagation, robustness, lyapunov, robust, adversarial, softmax, controllers, learnable, robustly, controllable\n...\n</code></pre> <p>We can also look at a particular part of the hierarchy, we might be interested in. If I would like to gain a more complete picture of what approaches exist for graph learning, I can look at that part of the hierarchy, specifically:</p> <pre><code>fig = model.hierarchy[193].plot_tree()\nfig.show()\n</code></pre>"},{"location":"tutorials/arxiv_ml/#investigating-the-topic-landscape","title":"Investigating the Topic Landscape","text":"<p>We can also gain more detailed insights by looking at the document clusters on an interactive map, that way, not all topics have to be seen at once, but one can zoom in to gain deeper insights about a certain area. We will do this by using <code>datamapplot</code>, and make sure that document names can be seen when we hover over their respective datapoints.</p> <p>This plot will also allow us to see how far or close clusters are to each other, as well as what kinds of paper belong to each cluster.</p> <pre><code># We will reset the hierarchy, so that we can see all topics at once.\nmodel.reset_topics()\nfig = model.plot_clusters_datamapplot(hover_text=ds[\"title\"])\nfig.show()\n</code></pre>"},{"location":"tutorials/arxiv_ml/#topic-based-retrieval-and-filtering","title":"Topic-based Retrieval and Filtering","text":"<p>Suppose that I am a cognitive neuroscientist, and would like to incorporate machine learning methods into my work. I would like to gain an overview of methods, get some paper recommendations, and get an understanding of how prevalent neuroscience is in machine learning literature.</p> <p>First, we need to know which topics are relevant. We can do this by estimating topic importance scores for a phrase that captures what we want to find. Let's use <code>cognitive neuroscience, imaging</code> as a search term.</p> <pre><code>model.print_topic_distribution(\"cognitive neuroscience imaging\")\n``` ```\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Topic name                                             \u2503 Score \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 48_fmri_neural_neuroimaging_cortex                     \u2502  1.64 \u2502\n\u2502 53_imaging_mri_deconvolution_imagenet                  \u2502  1.47 \u2502\n\u2502 47_neuron_neurons_neural_neuronal                      \u2502  1.43 \u2502\n\u2502 25_electroencephalogram_electroencephalography_eeg_bci \u2502  1.41 \u2502\n\u2502 54_tomography_imaging_imagenet_cnn                     \u2502  1.38 \u2502\n\u2502 49_deconvolution_regularization_denoising_compressive  \u2502  1.34 \u2502\n\u2502 51_denoising_deconvolution_cnn_imagenet                \u2502  1.32 \u2502\n\u2502 58_imagenet_cnn_cnns_inception                         \u2502  1.29 \u2502\n\u2502 59_imagenet_cnn_cnns_supervised                        \u2502  1.18 \u2502\n\u2502 74_cnns_convolutions_cnn_imagenet                      \u2502  1.17 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>All of the topics that show up are related to image processing, but only the first 5 seem to revolve around neuroimaging specifically.</p> <p>If we want these documents to belong to one cluster, we can join these clusters together in the model manually into the topic with the smallest ID, then give it a descriptive name.</p> <pre><code>model.join_topics([48, 53, 47, 25, 54], joint_id=25)\nmodel.rename_topics({25: \"Neuroimaging\"})\n</code></pre> <p>We can collect documents that belong to this topic:</p> <p><pre><code>import numpy as np\n\nis_relevant = model.labels_ == 25\n\nrelevant_documents = np.array(abstracts)[is_relevant]\nrelevant_titles = np.array(ds[\"title\"])[is_relevant]\n\n# We calculate document-topic importance scores to retrieve most relevant documents\ndoc_topic_importance = model.transform(relevant_documents, embeddings=embeddings[is_relevant])\n\nprint(len(relevant_documents))\n# 162\n</code></pre> We have now collected 162 papers that are relevant to our inquiry. If we wish to get paper recommendations, we can check, which documents rank highest on this topic:</p> <pre><code>topic_idx = list(model.classes_).index(25)\nmost_relevant = np.argsort(-doc_topic_importance[:, 25])\n\nfor idx in most_relevant[:10]:\n    print(relevant_titles[idx])\n</code></pre>"},{"location":"tutorials/arxiv_ml/#top-10-matches","title":"Top 10 matches:","text":"<ul> <li>neuro2vec: Masked Fourier Spectrum Prediction for Neurophysiological   Representation Learning</li> <li>Synthetic Epileptic Brain Activities Using Generative Adversarial   Networks</li> <li>Reconstructing ERP Signals Using Generative Adversarial Networks for   Mobile Brain-Machine Interface</li> <li>Deep learning approaches for neural decoding: from CNNs to LSTMs and   spikes to fMRI</li> <li>Real-time EEG-based Emotion Recognition using Discrete Wavelet   Transforms on Full and Reduced Channel Signals</li> <li>Evaluation of Preference of Multimedia Content using Deep Neural   Networks for Electroencephalography</li> <li>A Compact and Interpretable Convolutional Neural Network for   Cross-Subject Driver Drowsiness Detection from Single-Channel EEG</li> <li>Personalized Automatic Sleep Staging with Single-Night Data: a Pilot   Study with KL-Divergence Regularization</li> <li>SeizureNet: Multi-Spectral Deep Feature Learning for Seizure Type   Classification</li> <li>Towards physiology-informed data augmentation for EEG-based BCIs</li> </ul>"},{"location":"tutorials/arxiv_ml/#filtering-new-documents","title":"Filtering New Documents","text":"<p>Suppose that, in the future, more and more papers get published, but we are only interested in the ones that have a neuroimaging theme. Since our topic model has learned a good representation of what this means in relation to other machine learning papers, we can use the model to filter documents in the future based on their topical content.</p> <p>This means that we can use topic models to create a classification model without labelling documents or training a classifier.</p> <pre><code>new_documents = [\n    \"We utilized fMRI and unsupervised learning to uncover patterns in the development of schizophrenia in adolescents.\",\n    \"Our approach utilizies fluid dynamics for hyperparameter optimization, and achieves state-of-the-art results on multiple neural network architectures.\"\n]\n\ndoc_topic_importance = model.transform(new_documents)\ntopic_labels = [model.topic_names[topic_idx] for topic_idx in np.argmax(doc_topic_importance, axis=1)]\nprint(topic_labels)\n# ['Neuroimaging', '41_autonomous_driving_planning_vehicles']\n</code></pre>"},{"location":"tutorials/ideologies/","title":"Discovering a Data-driven Political Compass","text":"<p>The Political Compass is a dimensional theory of political ideologies and views. This model posits that political ideology is distributed along a Left-Right and Libertarian-Authoritarian axis.</p> The Political Compass (figure from Political Compass website) <p>While this model enjoys wide public recognition, one potential issue with it is that it is a top-down model, meaning that these dimensions were not discovered from some underlying data, but is based on experts' intuitions. Dimensional analysis of views is also typically conducted using surveys.</p> <p>In this tutorial we are going to look into how one could discover a bottom-up, data-driven Political Compass using the power of topic modelling, we will look at:</p> <ul> <li>How to build and train a Semantic Signal Separation (\\(S^3\\)) model on our corpus</li> <li>How to interpret the semantic axes discovered by our model</li> <li>How to investigate the distribution of political parties along the discovered axes</li> </ul>"},{"location":"tutorials/ideologies/#installation","title":"Installation","text":"<p>We will install Turftopic with Plotly to be able to plot our results, and the <code>datasets</code> library, for fetching data from HF Hub.</p> <pre><code>pip install datasets plotly pandas turftopic\n</code></pre>"},{"location":"tutorials/ideologies/#data-preparation","title":"Data Preparation","text":"<p>For this demonstration, I will be using a synthetic dataset, in which a large language model was tasked with expressing political opinions in free-form text.</p> <pre><code>from datasets import load_dataset\n\nds = load_dataset(\"JyotiNayak/political_ideologies\", split=\"train\")\ntexts = ds[\"statement\"]\n</code></pre> <p>We will be using the <code>paraphrase-MiniLM-L12-v2</code> for embedding our dataset and pre-computing embeddings.</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\"paraphrase-MiniLM-L12-v2\")\nembeddings = encoder.encode(texts, show_progress_bar=True)\n</code></pre>"},{"location":"tutorials/ideologies/#model-training","title":"Model Training","text":"<p>We will use the \\(S^3\\) topic model for our investigations, as it conceptualized topics as independent axes in semantic space, meaning it is built for establishing dimensional theories similar to the Political Compass. For more details, read the documentation page on Semantic Signal Separation.</p> <p>Instead of a 2-dimensional model, similar to the Political Compass, we will opt to discover 3 dimensions.</p> <p>Note</p> <p>You can easily expand this to more dimensions, the only reason we're not doing it here is because it would take more time to interpret them, and the tutorial is more accessible this way.</p> <pre><code>from turftopic import SemanticSignalSeparation\n\nmodel = SemanticSignalSeparation(3, encoder=encoder, random_state=42)\ndoc_topic_matrix = model.fit_transform(texts, embeddings=embeddings)\n</code></pre>"},{"location":"tutorials/ideologies/#model-interpretation","title":"Model Interpretation","text":"<p>First, let us examine the highest and lowest ranking terms on each axis in order to gain an intuition for what the dimensions could be about.</p> <p>Tip</p> <p>For a more detailed discussion, see the Model Interpretation page in the documentation.</p> <pre><code>model.print_topics(top_k=10)\n</code></pre> Topic ID Highest Ranking Lowest Ranking 0 religion, religious, faith, church, religions, faiths, doctrines, freedom, freedoms, beliefs households, labor, household, poverty, socioeconomic, hardworking, income, wage, pay, welfare 1 investments, investment, spending, fiscal, invest, funding, policy, pollution, economic, budget racism, racial, ethnicity, diverse, discrimination, distinct, genders , ethnic, families, adoption 2 warming, carbon, environment, environmental, planet, change, solar, greenhouse, fossil, biodiversity wealth, taxation, prosperity, wealthiest, tax, profit, entrepreneurship, taxes, fiscal, government <p>While this overview already gives us some idea as to what the axes represent, we might lose a lot of information by just looking at the top N words. Luckily, Turftopic comes with utilities for displaying a more complete compass of concepts along two axes at a time.</p> <p>Interpret Political Axes on the Concept Compass</p> Axis 0 and 1Axis 1 and 2Axis 2 and 0 <pre><code>model.plot_concept_compass(0, 1)\n</code></pre> <p> </p> <pre><code>model.plot_concept_compass(1, 2)\n</code></pre> <p> </p> <pre><code>model.plot_concept_compass(2, 0)\n</code></pre> <p> </p> <p>Note</p> <p>Note that these axes seem to differ quite a bit from those proposed by the Political Compass. Survey-based methods usually focus more on differences in views on selected issues, while it seems that we have discovered more of a distribution of issue-importance. Surveys have long been criticized for neglected the salience of issues for individuals, so while this method might not replace them, it could be a very useful for the augmentation of survey results.</p> <p>These plots give us a deeper insight into how concepts are distributed along the discovered axes. A potential interpretation of these could be the following:</p> <pre><code>model.rename_topics({\n    0: \"Religiosity\",\n    1: \"Economic vs Social\",\n    2: \"Environmentalism\",\n})\n</code></pre> <p>As a sanity check we can also try predicting these axes for a new statement that we write:</p> <pre><code>model.print_topic_distribution(\"I am a socialist and I am concerned with the growing inequality in our societies. I'd like to see governments do more to prevent the exploitation of workers.\")\n</code></pre> Topic name Score Economic vs Social 1.01 Religiosity -0.78 Environmentalism -1.10 <p>This makes sense, as the statement above is mostly concerned with an economic issue, is not based in religion or beliefs, and is not about the environment.</p>"},{"location":"tutorials/ideologies/#relating-axes-to-party-affiliation","title":"Relating Axes to Party Affiliation","text":"<p>In this synthetic dataset we also have access to party affiliation labels. As such we can investigate the relation between (hypothetical) political parties and the discovered ideological dimensions.</p> <p>We will do this by organizing all information into a dataframe, then plotting it on a scatterplot matrix.</p> <pre><code>import pandas as pd\nimport plotly.express as px\n\ndf = pd.DataFrame(doc_topic_matrix, columns=model.topic_names)\ndf[\"party\"] = [\"Liberal\" if label == 1 else \"Conservative\" for label in ds[\"label\"]]\n\nfig = px.scatter_matrix(df, dimensions=model.topic_names, color=\"party\", template=\"plotly_white\")\nfig = fig.update_traces(diagonal_visible=False, showupperhalf=False, marker=dict(opacity=0.6))\nfig.show()\n</code></pre> <p>While there doesn't seem to be a clear divide on these issues between these hypothetical liberals and convservatives, some differences can already be seen. For instance environmental issues are discussed more by liberals, while belief-based and religious issues are more prevalent in conservative texts.</p>"},{"location":"tutorials/overview/","title":"Tutorial Overview","text":""},{"location":"tutorials/overview/#case-studies","title":"Case Studies","text":"<p>Topic models can be used in various real-world scenarios in both academia and industry. We provide a number of concrete examples of using Turftopic to gain real insights into the nature of text data.</p> <p> Cluster Analysis of the landscape of machine learning research.</p> <p> Discourse Analysis of internet forums on morality and religion.</p> <p> Dimensional Analysis of political ideologies and issues.</p> <p> Dissatisfaction Analysis of customer reviews.</p>"},{"location":"tutorials/religious/","title":"Analyzing Discourse on Religion and Morality","text":"<p>Topic models are an effective tool for discourse analysis, and widely applied in computational humanities research. One potential research question could be to investigate how groups online discuss the connection between morality and religion.</p> <p>For this task, we are going to utilize KeyNMF, which is a powerful topic model based on keyword/keyphrase extraction using contextual representations, we will look at:</p> <ul> <li>How to construct and train a Seeded KeyNMF model on our corpus</li> <li>How to interpret the topic model's output</li> <li>How topics are distributed across groups</li> </ul>"},{"location":"tutorials/religious/#installation","title":"Installation","text":"<pre><code>pip install turftopic[topic-wizard]\n</code></pre>"},{"location":"tutorials/religious/#data-preparation","title":"Data Preparation","text":"<p>We are going to use a subset of the 20 Newsgroups dataset, which includes three newsgroups oriented at discussing religion and atheism. Luckily this dataset is directly available from scikit-learn's repositories.</p> <pre><code>import numpy as np\nfrom sklearn.datasets import fetch_20newsgroups\n\nds = fetch_20newsgroups(\n    subset=\"all\",\n    remove=(\"headers\", \"footers\", \"quotes\"),\n    categories=[\n        \"alt.atheism\",\n        \"talk.religion.misc\",\n        \"soc.religion.christian\",\n    ],\n)\ncorpus = ds.data\ngroup_labels = np.array(ds.target_names)[ds.target]\n</code></pre> <p>Turftopic uses contextual embeddings of documents in order to understand what is in the corpus. We will use the small and fast <code>all-MiniLM-L6-v2</code> sentence transformer to produce embeddings of our data.</p> <p>Tip</p> <p>Sometimes you might want to reuse embeddings in different topic models, or save them to disk. It is thus recommended, but not necessary to precompute them before running a topic model.</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\"all-MiniLM-L6-v2\")\nembeddings = encoder.encode(corpus, show_progress_bar=True)\n</code></pre>"},{"location":"tutorials/religious/#model-training","title":"Model Training","text":"<p>Since we want to investigate the discourse from the perspective of morality, we would like to include this information in our model. KeyNMF is capable of investigating a corpus from a certain angle based on a seed phrase, this will allow us to focus the topics in the model on morality.</p> <p>Tip</p> <p>For a more detailed discussion, check out our documentation page on Seeded Topic Modelling</p> <pre><code>from turftopic import KeyNMF\n\nmodel = KeyNMF(\n    n_components=15,\n    random_state=42,\n    encoder=encoder,\n    seed_phrase=\"Religion and Morality\"\n)\ntopic_data = model.prepare_topic_data(corpus, embeddings=embeddings)\n</code></pre>"},{"location":"tutorials/religious/#model-interpretation","title":"Model Interpretation","text":"<p>Let us first print the top 10 words for each topic discovered by the model.</p> <p>Tip</p> <p>For a more detailed discussion, see the Model Interpretation page in the documentation.</p> <pre><code>topic_data.print_topics()\n</code></pre> Topic ID Highest Ranking 0 atheist, atheists, theists, religious, abortion, agnostic, theist, just, mythology, argument 1 immoral, morals, morality, moral, morally, behavior, society, religious, meat, societies 2 bible, scripture, biblical, scriptures, faith, gospel, testament, commandments, revelation, interpretation 3 religion, religious, religions, beliefs, cult, science, spiritual, don, scientific, secular 4 god, believe, satan, existence, gods, evil, exist, just, genocide, creator 5 homosexuality, homosexual, homosexuals, heterosexual, sexual, gay, fornication, immoral, sex, sodom 6 morality, objective, subjective, absolute, natural, relativism, societal, objectively, science, correct 7 belief, faith, beliefs, believe, believing, evidence, philosophy, religions, believed, believes 8 atheism, theism, religious, alt, belief, stalin, agnosticism, theists, newsgroup, read 9 christians, religions, jesus, fundamentalist, beliefs, commandments, hell, worship, jewish, christian 10 christianity, christian, christ, gospel, evangelical, cult, book, life, faith, buddhist 11 moral, animals, species, kill, murder, killing, values, ethics, morally, society 12 church, catholic, christian, churches, catholics, religious, orthodox, theology, protestant, marriage 13 sin, sins, punishment, jesus, christ, salvation, sinner, repentance, repent, sinful 14 islam, muslim, islamic, muslims, qur, secular, religions, rushdie, law, khomeini <p>We can already see multiple topics that are highly related to our research question. Topic 2 for instance is concerned with dogmatic morality, topic 4 is focused on the subject of homosexuality, which some religions/religious groups deem immoral, and Topic 11 is seemingly about murder and killing, but also includes \"animals\" and \"species\", so it might be related to veganism.</p> <p>To see whether our intuition is correct, we can investigate the top 10 documents on this topic:</p> <pre><code>topic_data.print_representative_documents(11)\n</code></pre> Document Score See, there you go again, saying that a moral act is only significant if it is \"voluntary.\" Why do you think this? And anyway, humans have the ability to disregard some of their instincts. You are attaching too many things to the term \"moral,\" I think. Let's try this: is it \"good\" that animals of the... 0.20 For example, if it were instinctive not to murder... So, only intelligent beings can be moral, even if the bahavior of other beings mimics theirs? And, how much emphasis do you place on intelligence? Animals of the same species could kill each other arbitarily, but they don't. Are you trying to say ... 0.20 If you force me to do something, am I morally responsible for it? Well, make up your mind. Is it to be \"instinctive not to murder\" or not? It's not even correct. Animals of the same species do kill one another. Sigh. I wonder how many times we have been round this loop. I think that instinctive baha... 0.18 But now you are contradicting yourself in a pretty massive way, and I don't think you've even noticed. In another part of this thread, you've been telling us that the \"goal\" of a natural morality is what animals do to survive. But suppose that your omniscient being told you that the long term surviv... 0.18 Well I agree with you in the sense that they have no \"moral\" right to inflict these rules, but there is one thing I might add: at the very least, almost everybody wants to avoid pain, and if that means sacrific ing some stuff for a herd morality, then so be it. Right, and since they grew up and learn... 0.17 <p>By investigating the documents, we may discover, that this topic is actually about the differences between man and animal, and whether animals can be held morally responsible.</p>"},{"location":"tutorials/religious/#investigating-the-word-landscape","title":"Investigating the Word Landscape","text":"<p>KeyNMF can also be thought of as a soft word clustering method, and we can display a word-map to investigate how and which words are related to each other.</p> <pre><code>fig = topic_data.figures.word_map()\nfig.show()\n</code></pre> <p>Tip</p> <p>You can zoom in on the graph to investigate groups closer. Hover over a word to display it. You can click on the legend to hide topics.</p>"},{"location":"tutorials/religious/#topical-differences-in-groups","title":"Topical Differences in Groups","text":"<p>One interesting aspect to look at when investigating data from social media, is understanding, which topics get mostly discussed by which group. Since we have group labels for each text in 20 Newsgroups, we can investigate how topic distributions are different in the three newsgroups we are analysing.</p> <p>We can do this by summing up all document-topic distributions in the given groups, and then plotting them on a heatmap.</p> <pre><code>import plotly.express as px\nimport pandas as pd\n\ngroups = [\n    \"alt.atheism\",\n    \"talk.religion.misc\",\n    \"soc.religion.christian\",\n]\ngroup_topic_matrix = []\nfor label in groups:\n    group_topic_matrix.append(\n        topic_data.document_topic_matrix[group_labels==label].sum(axis=0)\n    )\ngroup_topic_matrix = np.stack(group_topic_matrix)\n# We turn this into a dataframe, so that Plotly knows what to write on the x and y axes.\ngroup_topic_matrix = pd.DataFrame(group_topic_matrix, columns=model.topic_names, index=groups)\nfig = px.imshow(group_topic_matrix)\nfig.show()\n</code></pre> <p>Here we can see that the distinction between theism and atheism, as well as subjectivity and objectivity are central themes for atheists. Christians, meanwhile, understandably, talk more about the bible, the gospel, differences between denominations and sins. Interestingly, Islam is mostly discussed in the atheism group, but not in the religion ones.</p>"},{"location":"tutorials/reviews/","title":"Customer Dissatisfaction Analysis","text":"<p>When providing some service for customers as a company, it is valuable to have an overview of what customers are dissatisfied with. A good place to start would be to analyze reviews of your product that have low scores.</p> <p>In this tutorial we will use Turftopic to find topics in customer reviews of the Uber app on Google Play. The dataset is openly available on Kaggle, we will:</p> <ul> <li>Build and train a Noun-phrase informed KeyNMF topic model on customer feedback</li> <li>Learn about topic interpretation.</li> <li>Investigate the prevalence of issues identified by customers.</li> </ul>"},{"location":"tutorials/reviews/#installation","title":"Installation","text":"<p>We will be using SpaCy for phrase extraction and Plotly for producing visualizations and pandas for data wrangling. We will also have to install a SpaCy pipeline.</p> <pre><code>pip install turftopic[spacy] plotly pandas\npython -m spacy download en_core_web_sm\n</code></pre>"},{"location":"tutorials/reviews/#data-preparation","title":"Data Preparation","text":"<p>We will load the dataset, and extract the reviews with the lowest scores, to see what the most disappointed users take issue with.</p> <pre><code>import pandas as pd\n\ndf = pd.read_csv(\"uber_reviews_without_reviewid.csv\")\ncorpus = list(df[\"content\"][df[\"score\"] == 1])\n</code></pre> <p>Since the dataset contains reviews in Spanish, we will be using a multilingual encoder model:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nencoder = SentenceTransformer(\"paraphrase-multilingual-MiniLM-L12-v2\")\nembeddings = encoder.encode(corpus, show_progress_bar=True)\n</code></pre>"},{"location":"tutorials/reviews/#model-training","title":"Model Training","text":"<p>We will be using the KeyNMF model for topic discovery, as it is capable of hierarchically expanding topics if we need it.</p> <p>Tip</p> <p>Using noun phrases, as it is done in this tutorial can greatly increase the interpretability, clarity and precision of your topic descriptions, but can make model training substantially slower, as sentences have to be parsed. See the Vectorizers page for more detail.</p> <pre><code>from turftopic import KeyNMF\nfrom turftopic.vectorizers.spacy import NounPhraseCountVectorizer\n\nmodel = KeyNMF(\n    # We train first with 10 topics, and can then expand the hierarchy if needed\n    n_components=10,\n    encoder=encoder,\n    vectorizer=NounPhraseCountVectorizer(\"en_core_web_sm\"),\n    random_state=42,\n)\ndoc_topic_matrix = model.fit_transform(corpus, embeddings=embeddings)\n</code></pre>"},{"location":"tutorials/reviews/#model-interpretation","title":"Model Interpretation","text":"<p>Let us first examine the highest ranking phrases on each topic.</p> <p>Tip</p> <p>For a more detailed discussion, see the Model Interpretation page in the documentation.</p> <pre><code>model.print_topics()\n</code></pre> Topic ID Highest Ranking 0 driver, pickup, cancellation fee, car, money, fee, location, extra money, road, auto driver 1 app, apps, worst application, option, taxi, phone, support, ads, time, customer service 2 drivers, rides, company, time, trips, customer, riders, customers, car, apps 3 ride, time, rider, charge, destination, worst service, rides, emergency no rider, 2 rides, uber app 4 worst app, booking, customer support, waiting time, destination, rides, scam, worst customer support, senseless irresponsible app, traffic useless app 5 cab, cab driver, time, emergency, booking, auto, bad service, worst cab bookinv app, rapido, amount 6 account, money, customer support, refund, card, bank, permission, email, credit card, customers 7 price, prices, pricing, fees, traffic, predatory pricing, destination, higher price, discount, amount 8 trip, cash, amount, car, customer support, service, trips, bad experience, destination, cancelation fee 9 uber, service, payment, taxi, rides, customer service, worst service, uber app, customers, refund <p>It seems that a substantial part of the bad reviews are related to the drivers, either because they are rude, people, or overcharge customers. This indicates that the company should be wary of such behaviour and should implement measures to counteract these issues. Other topics are related to customer support, or the quality of the app itself.</p>"},{"location":"tutorials/reviews/#hierarchical-topic-expansion","title":"Hierarchical Topic Expansion","text":"<p>Let us say that we are mostly interested in issues with customer service since we want to provide insights, about what could be improved. We can investigate the topics related to this in more detail by expanding the model's hierarchy.</p> <p>Tip</p> <p>For a more detailed discussion, see the Hierarchical Modelling page in the documentation.</p> <p>We can for instance investigate what subtopics can be found in Topic 9: <pre><code># Accessing Topic 9 and dividing it to 3 subtopics\nprint(model.hierarchy[9].divide(3))\n</code></pre></p> 9: uber, service, payment, taxi, rides, customer service, worst service, uber app, customers, refund  \u251c\u2500\u2500 0: driver, ride, uber, payment, money, cab, cash, uber app, trip, refund  \u251c\u2500\u2500 1: app, taxi, the app prices journeys, airport, prices, inflated prices, scam, inflated price, customer service, uber  \u2514\u2500\u2500 2: drivers, uber, rides, service, fraud drivers, voucher payment, prices, frauds, uber wallet, payments  <p></p> <p>This tells us that issues with customer service are often related to refunds because of inflated prices or fraud. We can also see that voucher payments and uber wallet are often associated with fraud.</p>"},{"location":"tutorials/reviews/#topic-prevalence","title":"Topic Prevalence","text":"<p>We can check how prevalent each of these issues are by adding the occurrence of these topics up in the corpus, then plotting them.</p> <pre><code>import plotly.express as px\n\ntopic_prevalence = doc_topic_matrix.sum(axis=0)\nprev_df = pd.DataFrame({\"prevalence\": topic_prevalence, \"name\": model.topic_names})\nfig = px.pie(prev_df, values=\"prevalence\", names=\"name\", title=\"Prevalence of topics in negative reviews.\")\nfig.show()\n</code></pre> <p>We can see that most complaints are made about their driver, as topics of this nature dominate most of the documents, while the next most common issue is with the application. Customer support is the least prevalent of issues in our dataset.</p>"}]}